{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy Measurement Harness - Kaggle Edition\n",
    "## For Krishna: Complete Energy + Latency Measurement System\n",
    "\n",
    "**Purpose:** Measure GPU power, energy, and latency for FP32, FP16, INT8 models\n",
    "\n",
    "**Architecture:** 10-layer design from specifications\n",
    "- Zero I/O during measurement\n",
    "- Multi-trial support (5 trials for statistical confidence)\n",
    "- PowerLogger for nvidia-smi\n",
    "- CSV/JSON output\n",
    "\n",
    "**Integration:** Uses Taara's pre-tokenized dataset, merges with accuracy results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install -q transformers datasets\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "print(\"✓ Imports complete\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 0: Config & Experiment Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration for energy measurement experiments.\"\"\"\n",
    "    model_name: str = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    precision: str = \"fp32\"\n",
    "    batch_size: int = 8\n",
    "    seq_len: int = 128\n",
    "    num_loops: int = 200\n",
    "    warmup_loops: int = 50\n",
    "    dataset_path: str = \"/kaggle/working/tokenized_data\"\n",
    "    device: str = \"cuda\"\n",
    "    num_trials: int = 5\n",
    "    poll_interval_ms: int = 100\n",
    "\n",
    "# Create default config\n",
    "config = ExperimentConfig()\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(\"-\" * 60)\n",
    "for key, value in asdict(config).items():\n",
    "    print(f\"  {key:20s}: {value}\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 1: Dataset & Model Loading (Zero I/O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_tokenized_dataset(dataset_path: str, device: str):\n",
    "    \"\"\"Load pre-tokenized dataset - ALL data to GPU at once.\"\"\"\n",
    "    data_path = Path(dataset_path)\n",
    "    \n",
    "    print(f\"\\nLoading dataset from {dataset_path}...\")\n",
    "    input_ids = torch.load(data_path / 'input_ids.pt').to(device)\n",
    "    attention_mask = torch.load(data_path / 'attention_mask.pt').to(device)\n",
    "    labels = torch.load(data_path / 'labels.pt').to(device)\n",
    "    \n",
    "    print(f\"✓ Loaded {len(labels)} samples to {device}\")\n",
    "    print(f\"  - Input shape: {input_ids.shape}\")\n",
    "    print(f\"  - Zero I/O during measurement ✓\")\n",
    "    \n",
    "    return input_ids, attention_mask, labels\n",
    "\n",
    "\n",
    "def batched_iterator(input_ids, attention_mask, batch_size: int):\n",
    "    \"\"\"Infinite batch iterator with wraparound (zero I/O).\"\"\"\n",
    "    N = input_ids.size(0)\n",
    "    idx = 0\n",
    "    \n",
    "    while True:\n",
    "        end_idx = idx + batch_size\n",
    "        \n",
    "        if end_idx <= N:\n",
    "            yield input_ids[idx:end_idx], attention_mask[idx:end_idx]\n",
    "            idx = end_idx\n",
    "        else:\n",
    "            idx = 0\n",
    "\n",
    "\n",
    "def load_model(precision: str, model_name: str, device: str):\n",
    "    \"\"\"Load model with specified precision.\"\"\"\n",
    "    print(f\"\\nLoading {precision.upper()} model...\")\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    if precision == \"fp16\":\n",
    "        model = model.half()\n",
    "        print(\"✓ Converted to FP16\")\n",
    "    elif precision == \"int8\":\n",
    "        print(\"⚠ INT8 quantization not implemented yet\")\n",
    "        print(\"  Using FP32 as placeholder (Thomas will implement)\")\n",
    "    \n",
    "    print(f\"✓ Model loaded on {device}\")\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"  - Parameters: {param_count:,} ({param_count/1e6:.1f}M)\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"✓ Data/model loading functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2: Warmup Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup(model, batch_iter, num_iters: int):\n",
    "    \"\"\"Warmup to stabilize GPU clocks.\"\"\"\n",
    "    print(f\"\\nWarming up with {num_iters} iterations...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_iters):\n",
    "            input_ids, attention_mask = next(batch_iter)\n",
    "            _ = model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  Warmup: {i+1}/{num_iters}\")\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    print(\"✓ Warmup complete - GPU stabilized\")\n",
    "\n",
    "\n",
    "print(\"✓ Warmup function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 3: Timed Inference Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_loop(model, batch_iter, num_loops: int) -> float:\n",
    "    \"\"\"Timed inference loop for latency measurement.\"\"\"\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_loops):\n",
    "            input_ids, attention_mask = next(batch_iter)\n",
    "            _ = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    return end - start\n",
    "\n",
    "\n",
    "print(\"✓ Inference loop function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 4: Power Logger (CRITICAL - Krishna Must Test This)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerLogger:\n",
    "    \"\"\"GPU power monitoring using nvidia-smi.\"\"\"\n",
    "    \n",
    "    def __init__(self, gpu_id: int = 0, poll_interval_ms: int = 100):\n",
    "        self.gpu_id = gpu_id\n",
    "        self.poll_interval_ms = poll_interval_ms\n",
    "        self.proc = None\n",
    "        self.samples = []\n",
    "        self.thread = None\n",
    "        self.stop_flag = False\n",
    "        \n",
    "    def start(self):\n",
    "        \"\"\"Start power monitoring.\"\"\"\n",
    "        print(f\"\\n[PowerLogger] Starting (poll: {self.poll_interval_ms}ms)...\")\n",
    "        \n",
    "        # CRITICAL: Test which command works on Kaggle\n",
    "        # Option 1: Millisecond interval (if supported)\n",
    "        cmd = [\n",
    "            'nvidia-smi',\n",
    "            '--query-gpu=power.draw',\n",
    "            '--format=csv,noheader,nounits',\n",
    "            f'--id={self.gpu_id}',\n",
    "            '-lms', str(self.poll_interval_ms)\n",
    "        ]\n",
    "        \n",
    "        # Option 2: Second interval (if -lms doesn't work)\n",
    "        # cmd = [\n",
    "        #     'nvidia-smi',\n",
    "        #     '--query-gpu=power.draw',\n",
    "        #     '--format=csv,noheader,nounits',\n",
    "        #     f'--id={self.gpu_id}',\n",
    "        #     '-l', '1'  # 1 second interval\n",
    "        # ]\n",
    "        \n",
    "        try:\n",
    "            self.proc = subprocess.Popen(\n",
    "                cmd,\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.PIPE,\n",
    "                universal_newlines=True,\n",
    "                bufsize=1\n",
    "            )\n",
    "            \n",
    "            self.stop_flag = False\n",
    "            self.thread = threading.Thread(target=self._collect_samples)\n",
    "            self.thread.daemon = True\n",
    "            self.thread.start()\n",
    "            \n",
    "            print(\"[PowerLogger] ✓ Started\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[PowerLogger] ✗ Failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _collect_samples(self):\n",
    "        \"\"\"Collect power samples in background.\"\"\"\n",
    "        while not self.stop_flag and self.proc and self.proc.poll() is None:\n",
    "            line = self.proc.stdout.readline()\n",
    "            if line:\n",
    "                try:\n",
    "                    power = float(line.strip())\n",
    "                    self.samples.append(power)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "    \n",
    "    def stop(self) -> List[float]:\n",
    "        \"\"\"Stop and return samples.\"\"\"\n",
    "        print(f\"[PowerLogger] Stopping...\")\n",
    "        \n",
    "        self.stop_flag = True\n",
    "        \n",
    "        if self.proc:\n",
    "            self.proc.terminate()\n",
    "            try:\n",
    "                self.proc.wait(timeout=2)\n",
    "            except subprocess.TimeoutExpired:\n",
    "                self.proc.kill()\n",
    "        \n",
    "        if self.thread:\n",
    "            self.thread.join(timeout=2)\n",
    "        \n",
    "        print(f\"[PowerLogger] ✓ Stopped - {len(self.samples)} samples\")\n",
    "        \n",
    "        if len(self.samples) == 0:\n",
    "            print(\"[PowerLogger] ⚠ WARNING: No samples collected!\")\n",
    "            print(\"  Krishna: Test nvidia-smi command manually\")\n",
    "        \n",
    "        return self.samples.copy()\n",
    "\n",
    "\n",
    "print(\"✓ PowerLogger class defined\")\n",
    "print(\"⚠ Krishna: TEST THIS FIRST before running experiments!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST POWERLOGGER FIRST! (Run this cell before anything else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL TEST - Run this first!\n",
    "print(\"Testing PowerLogger...\")\n",
    "print(\"This will run for 5 seconds\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "logger = PowerLogger(gpu_id=0, poll_interval_ms=100)\n",
    "logger.start()\n",
    "time.sleep(5)\n",
    "samples = logger.stop()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Samples collected: {len(samples)}\")\n",
    "\n",
    "if len(samples) > 0:\n",
    "    print(f\"  Sample values: {samples[:5]}\")\n",
    "    print(f\"  Mean power: {np.mean(samples):.2f} W\")\n",
    "    print(\"\\n✓ PowerLogger WORKS! You can proceed.\")\n",
    "else:\n",
    "    print(\"\\n✗ PowerLogger returned 0 samples!\")\n",
    "    print(\"\\nDebugging steps:\")\n",
    "    print(\"1. Test nvidia-smi manually:\")\n",
    "    print(\"   !timeout 10 nvidia-smi --query-gpu=power.draw --format=csv,noheader,nounits -l 1\")\n",
    "    print(\"2. Try different interval formats:\")\n",
    "    print(\"   -lms 100  (milliseconds)\")\n",
    "    print(\"   -l 1      (seconds)\")\n",
    "    print(\"3. Update PowerLogger.start() command based on what works\")\n",
    "    print(\"\\n⚠ DO NOT PROCEED until this works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 5: Energy & Latency Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_energy_metrics(power_samples: List[float], total_time: float, num_inferences: int) -> Dict:\n",
    "    \"\"\"Compute energy and latency metrics.\"\"\"\n",
    "    if len(power_samples) == 0:\n",
    "        raise ValueError(\"No power samples - cannot compute energy\")\n",
    "    \n",
    "    avg_power = float(np.mean(power_samples))\n",
    "    std_power = float(np.std(power_samples))\n",
    "    energy_total = avg_power * total_time\n",
    "    energy_per_inference = energy_total / num_inferences\n",
    "    latency_per_sample = total_time / num_inferences\n",
    "    throughput = num_inferences / total_time\n",
    "    \n",
    "    return {\n",
    "        \"avg_power_w\": avg_power,\n",
    "        \"std_power_w\": std_power,\n",
    "        \"energy_total_j\": energy_total,\n",
    "        \"energy_per_inference_j\": energy_per_inference,\n",
    "        \"energy_per_inference_mj\": energy_per_inference * 1000,\n",
    "        \"latency_per_sample_s\": latency_per_sample,\n",
    "        \"latency_per_sample_ms\": latency_per_sample * 1000,\n",
    "        \"throughput_samples_s\": throughput,\n",
    "        \"total_time_s\": total_time,\n",
    "        \"num_inferences\": num_inferences,\n",
    "        \"num_power_samples\": len(power_samples),\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"✓ Energy computation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 6: Measure with Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_with_power(model, batch_iter, num_loops: int, logger: PowerLogger):\n",
    "    \"\"\"Run inference with power monitoring.\"\"\"\n",
    "    logger.start()\n",
    "    time.sleep(0.5)  # Let logger stabilize\n",
    "    \n",
    "    total_time = run_inference_loop(model, batch_iter, num_loops)\n",
    "    \n",
    "    power_samples = logger.stop()\n",
    "    \n",
    "    return total_time, power_samples\n",
    "\n",
    "\n",
    "print(\"✓ Measurement function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 7: Multi-Trial Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments_for_precision(config, precision: str, num_trials: int = 5):\n",
    "    \"\"\"Run multiple trials for one precision.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"RUNNING EXPERIMENTS: {precision.upper()}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load dataset once\n",
    "    input_ids, attention_mask, labels = load_pre_tokenized_dataset(\n",
    "        config.dataset_path, config.device\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for trial in range(num_trials):\n",
    "        print(f\"\\n{'─'*70}\")\n",
    "        print(f\"Trial {trial + 1}/{num_trials}\")\n",
    "        print(f\"{'─'*70}\")\n",
    "        \n",
    "        # Create batch iterator\n",
    "        batch_iter = batched_iterator(input_ids, attention_mask, config.batch_size)\n",
    "        \n",
    "        # Load model\n",
    "        model = load_model(precision, config.model_name, config.device)\n",
    "        \n",
    "        # Warmup\n",
    "        warmup(model, batch_iter, config.warmup_loops)\n",
    "        \n",
    "        # New iterator for measurement\n",
    "        batch_iter = batched_iterator(input_ids, attention_mask, config.batch_size)\n",
    "        \n",
    "        # Measure\n",
    "        print(f\"\\nRunning {config.num_loops} measurement iterations...\")\n",
    "        logger = PowerLogger(gpu_id=0, poll_interval_ms=config.poll_interval_ms)\n",
    "        \n",
    "        try:\n",
    "            total_time, power_samples = measure_with_power(\n",
    "                model, batch_iter, config.num_loops, logger\n",
    "            )\n",
    "            \n",
    "            num_inferences = config.num_loops * config.batch_size\n",
    "            metrics = compute_energy_metrics(power_samples, total_time, num_inferences)\n",
    "            \n",
    "            metrics[\"precision\"] = precision\n",
    "            metrics[\"trial\"] = trial\n",
    "            metrics[\"batch_size\"] = config.batch_size\n",
    "            metrics[\"seq_len\"] = config.seq_len\n",
    "            \n",
    "            results.append(metrics)\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\"\\n{'─'*70}\")\n",
    "            print(f\"Trial {trial + 1} Results:\")\n",
    "            print(f\"{'─'*70}\")\n",
    "            print(f\"  Latency:    {metrics['latency_per_sample_ms']:.3f} ms\")\n",
    "            print(f\"  Throughput: {metrics['throughput_samples_s']:.2f} samples/s\")\n",
    "            print(f\"  Avg Power:  {metrics['avg_power_w']:.2f} W\")\n",
    "            print(f\"  Energy:     {metrics['energy_per_inference_mj']:.3f} mJ\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ Trial {trial + 1} failed: {e}\")\n",
    "            continue\n",
    "        \n",
    "        finally:\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"✓ Multi-trial runner defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 8: Aggregate Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_trials(trial_results: List[Dict]) -> Dict:\n",
    "    \"\"\"Aggregate metrics across trials.\"\"\"\n",
    "    if not trial_results:\n",
    "        raise ValueError(\"No trial results\")\n",
    "    \n",
    "    metrics_keys = [\n",
    "        'avg_power_w', 'energy_per_inference_j', 'energy_per_inference_mj',\n",
    "        'latency_per_sample_s', 'latency_per_sample_ms', 'throughput_samples_s'\n",
    "    ]\n",
    "    \n",
    "    aggregated = {\n",
    "        'precision': trial_results[0]['precision'],\n",
    "        'batch_size': trial_results[0]['batch_size'],\n",
    "        'seq_len': trial_results[0]['seq_len'],\n",
    "        'num_trials': len(trial_results)\n",
    "    }\n",
    "    \n",
    "    for key in metrics_keys:\n",
    "        values = [r[key] for r in trial_results]\n",
    "        aggregated[f'{key}_mean'] = float(np.mean(values))\n",
    "        aggregated[f'{key}_std'] = float(np.std(values))\n",
    "        aggregated[f'{key}_min'] = float(np.min(values))\n",
    "        aggregated[f'{key}_max'] = float(np.max(values))\n",
    "    \n",
    "    return aggregated\n",
    "\n",
    "\n",
    "print(\"✓ Aggregation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENT 1: Test with FP32 (Small Scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test - 1 trial, 50 loops\n",
    "# Use this to debug PowerLogger\n",
    "\n",
    "test_config = ExperimentConfig(\n",
    "    num_loops=50,\n",
    "    warmup_loops=10,\n",
    "    num_trials=1\n",
    ")\n",
    "\n",
    "print(\"Running QUICK TEST (1 trial, 50 loops)...\")\n",
    "test_results = run_experiments_for_precision(test_config, \"fp32\", num_trials=1)\n",
    "\n",
    "if test_results:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✓ TEST SUCCESSFUL\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"PowerLogger is working! Proceed to full experiments.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✗ TEST FAILED\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Fix PowerLogger before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENT 2: Full FP32 (5 Trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full FP32 experiment\n",
    "# Run this after test succeeds\n",
    "\n",
    "full_config = ExperimentConfig(\n",
    "    num_loops=200,\n",
    "    warmup_loops=50,\n",
    "    num_trials=5\n",
    ")\n",
    "\n",
    "print(\"Running FULL FP32 EXPERIMENT (5 trials, 200 loops)...\")\n",
    "print(\"This will take ~5-10 minutes\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fp32_results = run_experiments_for_precision(full_config, \"fp32\", num_trials=5)\n",
    "\n",
    "# Aggregate\n",
    "if fp32_results:\n",
    "    fp32_agg = aggregate_trials(fp32_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FP32 AGGREGATED RESULTS (5 trials)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Latency:    {fp32_agg['latency_per_sample_ms_mean']:.3f} ± {fp32_agg['latency_per_sample_ms_std']:.3f} ms\")\n",
    "    print(f\"Throughput: {fp32_agg['throughput_samples_s_mean']:.2f} ± {fp32_agg['throughput_samples_s_std']:.2f} samples/s\")\n",
    "    print(f\"Avg Power:  {fp32_agg['avg_power_w_mean']:.2f} ± {fp32_agg['avg_power_w_std']:.2f} W\")\n",
    "    print(f\"Energy:     {fp32_agg['energy_per_inference_mj_mean']:.3f} ± {fp32_agg['energy_per_inference_mj_std']:.3f} mJ\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENT 3: FP16 (When Thomas Provides Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment when Thomas provides FP16 model\n",
    "\n",
    "# fp16_results = run_experiments_for_precision(full_config, \"fp16\", num_trials=5)\n",
    "\n",
    "# if fp16_results:\n",
    "#     fp16_agg = aggregate_trials(fp16_results)\n",
    "#     \n",
    "#     print(\"\\n\" + \"=\"*70)\n",
    "#     print(\"FP16 AGGREGATED RESULTS\")\n",
    "#     print(\"=\"*70)\n",
    "#     print(f\"Latency:    {fp16_agg['latency_per_sample_ms_mean']:.3f} ± {fp16_agg['latency_per_sample_ms_std']:.3f} ms\")\n",
    "#     print(f\"Throughput: {fp16_agg['throughput_samples_s_mean']:.2f} ± {fp16_agg['throughput_samples_s_std']:.2f} samples/s\")\n",
    "#     print(f\"Avg Power:  {fp16_agg['avg_power_w_mean']:.2f} ± {fp16_agg['avg_power_w_std']:.2f} W\")\n",
    "#     print(f\"Energy:     {fp16_agg['energy_per_inference_mj_mean']:.3f} ± {fp16_agg['energy_per_inference_mj_std']:.3f} mJ\")\n",
    "\n",
    "print(\"FP16 experiment ready (currently commented out)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENT 4: INT8 (When Thomas Provides Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment when Thomas provides INT8 model\n",
    "\n",
    "# int8_results = run_experiments_for_precision(full_config, \"int8\", num_trials=5)\n",
    "\n",
    "# if int8_results:\n",
    "#     int8_agg = aggregate_trials(int8_results)\n",
    "#     \n",
    "#     print(\"\\n\" + \"=\"*70)\n",
    "#     print(\"INT8 AGGREGATED RESULTS\")\n",
    "#     print(\"=\"*70)\n",
    "#     print(f\"Latency:    {int8_agg['latency_per_sample_ms_mean']:.3f} ± {int8_agg['latency_per_sample_ms_std']:.3f} ms\")\n",
    "#     print(f\"Throughput: {int8_agg['throughput_samples_s_mean']:.2f} ± {int8_agg['throughput_samples_s_std']:.2f} samples/s\")\n",
    "#     print(f\"Avg Power:  {int8_agg['avg_power_w_mean']:.2f} ± {int8_agg['avg_power_w_std']:.2f} W\")\n",
    "#     print(f\"Energy:     {int8_agg['energy_per_inference_mj_mean']:.3f} ± {int8_agg['energy_per_inference_mj_std']:.3f} mJ\")\n",
    "\n",
    "print(\"INT8 experiment ready (currently commented out)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save aggregated results\n",
    "output_dir = Path('/kaggle/working/energy_results')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Collect all aggregated results\n",
    "all_agg_results = []\n",
    "\n",
    "if 'fp32_agg' in locals():\n",
    "    all_agg_results.append(fp32_agg)\n",
    "\n",
    "# if 'fp16_agg' in locals():\n",
    "#     all_agg_results.append(fp16_agg)\n",
    "\n",
    "# if 'int8_agg' in locals():\n",
    "#     all_agg_results.append(int8_agg)\n",
    "\n",
    "if all_agg_results:\n",
    "    # Save as CSV\n",
    "    df_agg = pd.DataFrame(all_agg_results)\n",
    "    df_agg.to_csv(output_dir / 'energy_results_aggregated.csv', index=False)\n",
    "    print(f\"✓ Saved: {output_dir / 'energy_results_aggregated.csv'}\")\n",
    "    \n",
    "    # Save as JSON\n",
    "    with open(output_dir / 'energy_results_aggregated.json', 'w') as f:\n",
    "        json.dump(all_agg_results, f, indent=2)\n",
    "    print(f\"✓ Saved: {output_dir / 'energy_results_aggregated.json'}\")\n",
    "    \n",
    "    # Display\n",
    "    print(\"\\nResults Table:\")\n",
    "    display(df_agg[['precision', 'latency_per_sample_ms_mean', 'avg_power_w_mean', 'energy_per_inference_mj_mean']])\n",
    "else:\n",
    "    print(\"No results to save yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Taara's Accuracy Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate latency matches Taara's results\n",
    "if 'fp32_agg' in locals():\n",
    "    print(\"Validation Check:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Your FP32 latency:  {fp32_agg['latency_per_sample_ms_mean']:.3f} ms\")\n",
    "    print(f\"Taara's FP32 latency: 2.32 ms (from 0.116s / 50 samples)\")\n",
    "    \n",
    "    diff_pct = abs(fp32_agg['latency_per_sample_ms_mean'] - 2.32) / 2.32 * 100\n",
    "    print(f\"Difference: {diff_pct:.1f}%\")\n",
    "    \n",
    "    if diff_pct < 10:\n",
    "        print(\"✓ Latencies match within 10% - validation passed!\")\n",
    "    else:\n",
    "        print(\"⚠ Latencies differ by >10% - check configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions (Save Config, Merge with Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_config_to_file(config: ExperimentConfig, output_path: str):\n",
    "    \"\"\"Save configuration to JSON file.\"\"\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(asdict(config), f, indent=2)\n",
    "    print(f\"✓ Config saved to {output_path}\")\n",
    "\n",
    "\n",
    "def merge_with_accuracy_results(energy_results: Dict, accuracy_results_path: str) -> Dict:\n",
    "    \"\"\"Merge energy results with Taara's accuracy results.\"\"\"\n",
    "    with open(accuracy_results_path, 'r') as f:\n",
    "        acc_results = json.load(f)\n",
    "    \n",
    "    merged = {**energy_results, **acc_results}\n",
    "    return merged\n",
    "\n",
    "\n",
    "# Save current config\n",
    "save_config_to_file(config, '/kaggle/working/energy_results/experiment_config.json')\n",
    "\n",
    "print(\"\\n✓ Utility functions defined\")\n",
    "print(\"  - save_config_to_file(): Save experiment config\")\n",
    "print(\"  - merge_with_accuracy_results(): Merge with Taara's data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Energy + Accuracy (After Both Complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this after energy experiments complete and Taara has accuracy results\n",
    "\n",
    "# Example: Merge FP32 energy with FP32 accuracy\n",
    "# if 'fp32_agg' in locals():\n",
    "#     merged_fp32 = merge_with_accuracy_results(\n",
    "#         energy_results=fp32_agg,\n",
    "#         accuracy_results_path='/kaggle/working/results/fp32_baseline.json'\n",
    "#     )\n",
    "#     \n",
    "#     print(\"\\nMerged FP32 Results:\")\n",
    "#     print(\"=\"*60)\n",
    "#     print(f\"  Accuracy:   {merged_fp32['accuracy']*100:.2f}%\")\n",
    "#     print(f\"  Latency:    {merged_fp32['latency_per_sample_ms_mean']:.3f} ms\")\n",
    "#     print(f\"  Power:      {merged_fp32['avg_power_w_mean']:.2f} W\")\n",
    "#     print(f\"  Energy:     {merged_fp32['energy_per_inference_mj_mean']:.3f} mJ\")\n",
    "#     print(\"=\"*60)\n",
    "\n",
    "print(\"Merge function ready (currently commented out)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENERGY MEASUREMENT HARNESS - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nCompleted Experiments:\")\n",
    "if 'fp32_results' in locals() and fp32_results:\n",
    "    print(\"  ✓ FP32: 5 trials complete\")\n",
    "else:\n",
    "    print(\"  ○ FP32: Not run yet\")\n",
    "\n",
    "# if 'fp16_results' in locals() and fp16_results:\n",
    "#     print(\"  ✓ FP16: 5 trials complete\")\n",
    "# else:\n",
    "print(\"  ○ FP16: Waiting for Thomas\")\n",
    "\n",
    "# if 'int8_results' in locals() and int8_results:\n",
    "#     print(\"  ✓ INT8: 5 trials complete\")\n",
    "# else:\n",
    "print(\"  ○ INT8: Waiting for Thomas\")\n",
    "\n",
    "print(\"\\nOutput Files:\")\n",
    "if (output_dir / 'energy_results_aggregated.csv').exists():\n",
    "    print(f\"  ✓ {output_dir / 'energy_results_aggregated.csv'}\")\n",
    "    print(f\"  ✓ {output_dir / 'energy_results_aggregated.json'}\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Share energy results with Taara\")\n",
    "print(\"  2. Taara will merge with accuracy results\")\n",
    "print(\"  3. Generate comparison plots\")\n",
    "print(\"  4. Write report\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Krishna: Great work!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

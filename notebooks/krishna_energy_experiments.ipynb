{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!cd /kaggle/working/energy_aware_quantization && git pull origin main","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T19:18:54.511254Z","iopub.execute_input":"2025-12-01T19:18:54.511516Z","iopub.status.idle":"2025-12-01T19:18:54.886001Z","shell.execute_reply.started":"2025-12-01T19:18:54.511496Z","shell.execute_reply":"2025-12-01T19:18:54.885236Z"}},"outputs":[{"name":"stdout","text":"remote: Enumerating objects: 8, done.\u001b[K\nremote: Counting objects: 100% (8/8), done.\u001b[K\nremote: Compressing objects: 100% (2/2), done.\u001b[K\nremote: Total 5 (delta 3), reused 5 (delta 3), pack-reused 0 (from 0)\u001b[K\nUnpacking objects: 100% (5/5), 3.68 KiB | 1.84 MiB/s, done.\nFrom https://github.com/krishkc5/energy_aware_quantization\n * branch            main       -> FETCH_HEAD\n   7e29686..f2e7dcf  main       -> origin/main\nUpdating 7e29686..f2e7dcf\nFast-forward\n MANUAL_POLLING_FIX.md | 219 \u001b[32m++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[m\n src/power_logger.py   | 128 \u001b[32m+++++++++++\u001b[m\u001b[31m------------------\u001b[m\n 2 files changed, 267 insertions(+), 80 deletions(-)\n create mode 100644 MANUAL_POLLING_FIX.md\n","output_type":"stream"}],"execution_count":62},{"cell_type":"markdown","source":"# Energy-Aware Quantization Experiments\n## Krishna's Complete Measurement Harness on Kaggle\n\nThis notebook runs FP32, FP16, and INT8 experiments with comprehensive energy measurements.\n\n**Setup:**\n1. Enable GPU: Settings ‚Üí Accelerator ‚Üí GPU P100/T4\n2. Enable Internet: Settings ‚Üí Internet ‚Üí On (to clone GitHub repo)","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Clone or Update Repository","metadata":{}},{"cell_type":"code","source":"import os\nfrom pathlib import Path\n\nrepo_path = Path(\"/kaggle/working/energy_aware_quantization\")\n\nif repo_path.exists():\n    print(\"Repository exists, pulling latest changes...\")\n    !cd /kaggle/working/energy_aware_quantization && git pull origin main\nelse:\n    print(\"Cloning repository...\")\n    !cd /kaggle/working && git clone https://github.com/YOUR_USERNAME/energy_aware_quantization.git\n\nprint(\"\\n‚úì Repository ready!\")\n\n# Change to repo directory\nos.chdir(\"/kaggle/working/energy_aware_quantization\")\nprint(f\"Working directory: {os.getcwd()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T19:19:03.478132Z","iopub.execute_input":"2025-12-01T19:19:03.478997Z","iopub.status.idle":"2025-12-01T19:19:03.798136Z","shell.execute_reply.started":"2025-12-01T19:19:03.478955Z","shell.execute_reply":"2025-12-01T19:19:03.797186Z"}},"outputs":[{"name":"stdout","text":"Repository exists, pulling latest changes...\nFrom https://github.com/krishkc5/energy_aware_quantization\n * branch            main       -> FETCH_HEAD\nAlready up to date.\n\n‚úì Repository ready!\nWorking directory: /kaggle/working/energy_aware_quantization\n","output_type":"stream"}],"execution_count":63},{"cell_type":"markdown","source":"## Step 2: Install Dependencies","metadata":{}},{"cell_type":"code","source":"# Install requirements (most should already be in Kaggle)\n!pip install -q torch transformers numpy pandas tqdm\n\nprint(\"Dependencies installed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T19:19:07.050695Z","iopub.execute_input":"2025-12-01T19:19:07.051469Z","iopub.status.idle":"2025-12-01T19:19:10.517767Z","shell.execute_reply.started":"2025-12-01T19:19:07.051438Z","shell.execute_reply":"2025-12-01T19:19:10.516747Z"}},"outputs":[{"name":"stdout","text":"Dependencies installed\n","output_type":"stream"}],"execution_count":64},{"cell_type":"markdown","source":"## Step 3: Verify Installation and GPU","metadata":{}},{"cell_type":"code","source":"import sys\nimport torch\n\n# Add src to path\nsys.path.insert(0, '/kaggle/working/energy_aware_quantization')\n\nprint(\"=\"*70)\nprint(\"SYSTEM CHECK\")\nprint(\"=\"*70)\n\nprint(f\"\\nPython: {sys.version}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"CUDA version: {torch.version.cuda}\")\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    print(\"\\n‚úì GPU is ready!\")\nelse:\n    print(\"\\n No GPU! Please enable GPU in Settings.\")\n\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T19:19:12.087627Z","iopub.execute_input":"2025-12-01T19:19:12.088410Z","iopub.status.idle":"2025-12-01T19:19:12.094626Z","shell.execute_reply.started":"2025-12-01T19:19:12.088378Z","shell.execute_reply":"2025-12-01T19:19:12.093738Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nSYSTEM CHECK\n======================================================================\n\nPython: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\nPyTorch: 2.6.0+cu124\nCUDA available: True\nCUDA version: 12.4\nGPU: Tesla P100-PCIE-16GB\nGPU memory: 17.06 GB\n\n‚úì GPU is ready!\n======================================================================\n","output_type":"stream"}],"execution_count":65},{"cell_type":"markdown","source":"## Step 4: Check Datasets\n\nThe pre-tokenized datasets should be in the repo.","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\ndatasets_dir = Path(\"/kaggle/working/energy_aware_quantization/datasets\")\n\nprint(\"Checking datasets...\\n\")\n\nvariants = [\"tokenized_data\", \"tokenized_data_small\", \"tokenized_data_large\", \"tokenized_data_standard\"]\n\nfor variant in variants:\n    variant_dir = datasets_dir / variant\n    if variant_dir.exists():\n        files = list(variant_dir.glob(\"*.pt\")) + list(variant_dir.glob(\"*.json\"))\n        print(f\"‚úì {variant}: {len(files)} files\")\n    else:\n        print(f\"‚ùå {variant}: NOT FOUND\")\n\nprint(\"\\n‚úì Datasets verified\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T19:19:14.611096Z","iopub.execute_input":"2025-12-01T19:19:14.611371Z","iopub.status.idle":"2025-12-01T19:19:14.618014Z","shell.execute_reply.started":"2025-12-01T19:19:14.611350Z","shell.execute_reply":"2025-12-01T19:19:14.617319Z"}},"outputs":[{"name":"stdout","text":"Checking datasets...\n\n‚úì tokenized_data: 4 files\n‚úì tokenized_data_small: 4 files\n‚úì tokenized_data_large: 4 files\n‚úì tokenized_data_standard: 4 files\n\n‚úì Datasets verified\n","output_type":"stream"}],"execution_count":66},{"cell_type":"markdown","source":"## Step 5: Test Import of Measurement Modules","metadata":{}},{"cell_type":"code","source":"# Test imports\nprint(\"Testing module imports...\\n\")\n\ntry:\n    from src import load_pre_tokenized, warmup, PowerLogger, run_inference, compute_energy\n    print(\"‚úì src modules imported\")\nexcept ImportError as e:\n    print(f\"‚ùå Failed to import src: {e}\")\n\ntry:\n    from models import load_model, get_model_info\n    print(\"‚úì models module imported\")\nexcept ImportError as e:\n    print(f\"‚ùå Failed to import models: {e}\")\n\nprint(\"\\n‚úì All imports successful!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T19:19:17.182541Z","iopub.execute_input":"2025-12-01T19:19:17.182831Z","iopub.status.idle":"2025-12-01T19:19:17.188412Z","shell.execute_reply.started":"2025-12-01T19:19:17.182810Z","shell.execute_reply":"2025-12-01T19:19:17.187613Z"}},"outputs":[{"name":"stdout","text":"Testing module imports...\n\n‚úì src modules imported\n‚úì models module imported\n\n‚úì All imports successful!\n","output_type":"stream"}],"execution_count":67},{"cell_type":"markdown","source":"## Step 6: Quick Test - Load Dataset and Model","metadata":{}},{"cell_type":"code","source":"from src import load_pre_tokenized\nfrom models import load_model\n\nprint(\"Testing dataset and model loading...\\n\")\n\n# Load small dataset for quick test\ninput_ids, mask, labels, metadata = load_pre_tokenized(\n    \"datasets/tokenized_data\",\n    device=\"cuda\"\n)\n\nprint(f\"\\nDataset loaded: {input_ids.shape[0]} samples\")\n\n# Load FP32 model\nmodel = load_model(\n    \"distilbert-base-uncased-finetuned-sst-2-english\",\n    precision=\"fp32\",\n    device=\"cuda\",\n    verbose=True\n)\n\nprint(\"\\n‚úì Quick test passed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T19:19:19.823057Z","iopub.execute_input":"2025-12-01T19:19:19.823338Z","iopub.status.idle":"2025-12-01T19:19:20.101546Z","shell.execute_reply.started":"2025-12-01T19:19:19.823318Z","shell.execute_reply":"2025-12-01T19:19:20.100934Z"}},"outputs":[{"name":"stdout","text":"Testing dataset and model loading...\n\n Loaded 50 samples from datasets/tokenized_data\n  - Input shape: torch.Size([50, 128])\n  - Mask shape: torch.Size([50, 128])\n  - Labels shape: torch.Size([50])\n  - Device: cuda:0\n  - Max sequence length: 128\n\nDataset loaded: 50 samples\n\nLoading model: distilbert-base-uncased-finetuned-sst-2-english\nPrecision: fp32\nDevice: cuda\n‚úì Model loaded successfully\n  - Parameters: 66,955,010\n  - Model size: 255.42 MB\n  - Parameter dtype: torch.float32\n  - Parameter device: cuda:0\n\n‚úì Quick test passed!\n","output_type":"stream"}],"execution_count":68},{"cell_type":"markdown","source":"## Option A: Run Experiments via Python Script\n\n**Yes, you CAN run .py files in Kaggle notebooks using `!python`**","metadata":{}},{"cell_type":"markdown","source":"### Experiment 1: FP32 Baseline","metadata":{}},{"cell_type":"code","source":"# Run FP32 experiment\n!python src/measure_energy.py \\\n    --precision fp32 \\\n    --dataset datasets/tokenized_data \\\n    --num_iters 1000 \\\n    --trial 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T19:19:24.780991Z","iopub.execute_input":"2025-12-01T19:19:24.781284Z","iopub.status.idle":"2025-12-01T19:21:17.066277Z","shell.execute_reply.started":"2025-12-01T19:19:24.781266Z","shell.execute_reply":"2025-12-01T19:21:17.065443Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nENERGY-AWARE QUANTIZATION EXPERIMENT\n======================================================================\nPrecision: fp32\nModel: distilbert-base-uncased-finetuned-sst-2-english\nDataset: datasets/tokenized_data\nTrial: 1\n======================================================================\n\nOutput will be saved to: results/fp32/trial_1_20251201_191929.csv\n\n======================================================================\nSTEP 1: Checking GPU\n======================================================================\n GPU ready for measurements\n  - Device: Tesla P100-PCIE-16GB\n  - Compute capability: 6.0\n  - Total memory: 17.06 GB\n  - Multi-processors: 56\n  - Memory allocated: 0.00 GB\n  - Memory reserved: 0.00 GB\n\n======================================================================\nSTEP 2: Loading Dataset\n======================================================================\n Loaded 50 samples from datasets/tokenized_data\n  - Input shape: torch.Size([50, 128])\n  - Mask shape: torch.Size([50, 128])\n  - Labels shape: torch.Size([50])\n  - Device: cuda:0\n  - Max sequence length: 128\n Dataset validation passed\n  - Batch size: 50\n  - Sequence length: 128\n  - Unique labels: [0, 1]\n\n======================================================================\nSTEP 3: Loading Model\n======================================================================\n\nLoading model: distilbert-base-uncased-finetuned-sst-2-english\nPrecision: fp32\nDevice: cuda\n2025-12-01 19:19:31.163048: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764616771.185211     634 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764616771.192097     634 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n‚úì Model loaded successfully\n  - Parameters: 66,955,010\n  - Model size: 255.42 MB\n  - Parameter dtype: torch.float32\n  - Parameter device: cuda:0\n‚úì Model validation passed\n\n======================================================================\nSTEP 4: Warming Up GPU\n======================================================================\nWarming up GPU: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08<00:00, 11.47iter/s]\n Warmup completed: 100 iterations\n  - Average time: 87.05 ms/iter\n  - Min time: 86.00 ms\n  - Max time: 95.05 ms\n\n======================================================================\nSTEP 5: Setting Up Power Logger\n======================================================================\n nvidia-smi available, current power: 80.76 W\nPower logger configured:\n  - Sample interval: 100 ms\n  - GPU ID: 0\n\n======================================================================\nSTEP 6: Running Benchmark\n======================================================================\nStarting power logger with manual polling (interval: 100 ms)\n Power logger started (interval: 100 ms)\n\n============================================================\nRUNNING TIMED INFERENCE\n============================================================\nRunning inference:  14%|‚ñà‚ñà‚ñå                | 138/1000 [00:12<01:15, 11.37iter/s]  Power samples collected: 100\nRunning inference:  28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 282/1000 [00:24<01:03, 11.28iter/s]  Power samples collected: 200\nRunning inference:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 426/1000 [00:37<00:51, 11.20iter/s]  Power samples collected: 300\nRunning inference:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 568/1000 [00:50<00:38, 11.17iter/s]  Power samples collected: 400\nRunning inference:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 712/1000 [01:03<00:25, 11.23iter/s]  Power samples collected: 500\nRunning inference:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 858/1000 [01:16<00:12, 11.23iter/s]  Power samples collected: 600\nRunning inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:28<00:00, 11.26iter/s]\n\n Inference completed: 1000 iterations\n  - Total time: 88.775 s\n  - Mean latency: 88.64 ms\n  - Std latency: 0.85 ms\n  - Min latency: 86.47 ms\n  - Max latency: 90.82 ms\n  - Throughput: 563.22 samples/s\n\n============================================================\nCOMPUTING ACCURACY\n============================================================\n\n Inference with accuracy completed\n  - Accuracy: 86.00% (43/50)\n  - Mean latency: 89.10 ms\n\n============================================================\nBENCHMARK SUMMARY\n============================================================\nLatency (mean): 88.64 ms\nThroughput: 563.22 samples/s\nAccuracy: 86.00%\n============================================================\n  Power samples collected: 700\n Power logger stopped (702 samples collected)\n\n Collected 702 power samples\n Power samples validated\n\n======================================================================\nSTEP 7: Computing Energy Metrics\n======================================================================\n  Mean Power: 235.30 W\n  Power Std: 21.52 W\n  Total Energy: 20888.487 J\n  Energy/Inference: 20.888487 J\n  Energy/Inference: 20888.487 mJ\n\n======================================================================\nSTEP 8: Saving Results\n======================================================================\n\n Results saved to: results/fp32/trial_1_20251201_191929.csv\n Results saved to: results/fp32/trial_1_20251201_191929.json\n\n======================================================================\nEXPERIMENT SUMMARY\n======================================================================\n\nConfiguration:\n  Model: distilbert-base-uncased-finetuned-sst-2-english\n  Precision: fp32\n  Dataset: datasets/tokenized_data\n  Iterations: 1000\n  Trial: 1\n\nPerformance Metrics:\n  Mean Latency: 88.64 ms\n  Std Latency: 0.85 ms\n  Throughput: 563.22 samples/s\n\nPower Metrics:\n  Mean Power: 235.30 W\n  Std Power: 21.52 W\n  Min Power: 41.35 W\n  Max Power: 251.98 W\n\nEnergy Metrics:\n  Total Energy: 20888.487 J\n  Energy/Inference: 20.888487 J\n  Energy/Inference: 20888.487 mJ\n  Inferences/Joule: 0.05\n\nAccuracy Metrics:\n  Accuracy: 86.00%\n  Correct: 43/50\n\nMemory Usage:\n  Peak Allocated: 0.50 GB\n  Peak Reserved: 0.59 GB\n======================================================================\n\n Experiment completed successfully!\n","output_type":"stream"}],"execution_count":69},{"cell_type":"markdown","source":"### Experiment 2: FP16","metadata":{}},{"cell_type":"code","source":"# Run FP16 experiment\n!python src/measure_energy.py \\\n    --precision fp16 \\\n    --dataset datasets/tokenized_data \\\n    --num_iters 1000 \\\n    --trial 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T19:23:13.811177Z","iopub.execute_input":"2025-12-01T19:23:13.811939Z","iopub.status.idle":"2025-12-01T19:25:01.533829Z","shell.execute_reply.started":"2025-12-01T19:23:13.811907Z","shell.execute_reply":"2025-12-01T19:25:01.532756Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nENERGY-AWARE QUANTIZATION EXPERIMENT\n======================================================================\nPrecision: fp16\nModel: distilbert-base-uncased-finetuned-sst-2-english\nDataset: datasets/tokenized_data\nTrial: 1\n======================================================================\n\nOutput will be saved to: results/fp16/trial_1_20251201_192319.csv\n\n======================================================================\nSTEP 1: Checking GPU\n======================================================================\n GPU ready for measurements\n  - Device: Tesla P100-PCIE-16GB\n  - Compute capability: 6.0\n  - Total memory: 17.06 GB\n  - Multi-processors: 56\n  - Memory allocated: 0.00 GB\n  - Memory reserved: 0.00 GB\n\n======================================================================\nSTEP 2: Loading Dataset\n======================================================================\n Loaded 50 samples from datasets/tokenized_data\n  - Input shape: torch.Size([50, 128])\n  - Mask shape: torch.Size([50, 128])\n  - Labels shape: torch.Size([50])\n  - Device: cuda:0\n  - Max sequence length: 128\n Dataset validation passed\n  - Batch size: 50\n  - Sequence length: 128\n  - Unique labels: [0, 1]\n\n======================================================================\nSTEP 3: Loading Model\n======================================================================\n\nLoading model: distilbert-base-uncased-finetuned-sst-2-english\nPrecision: fp16\nDevice: cuda\n2025-12-01 19:23:20.729644: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764617000.752197    2056 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764617000.759162    2056 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n‚úì Model loaded successfully\n  - Parameters: 66,955,010\n  - Model size: 127.71 MB\n  - Parameter dtype: torch.float16\n  - Parameter device: cuda:0\n‚úì Model validation passed\n\n======================================================================\nSTEP 4: Warming Up GPU\n======================================================================\nWarming up GPU: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08<00:00, 11.95iter/s]\n Warmup completed: 100 iterations\n  - Average time: 83.53 ms/iter\n  - Min time: 82.98 ms\n  - Max time: 92.67 ms\n\n======================================================================\nSTEP 5: Setting Up Power Logger\n======================================================================\n nvidia-smi available, current power: 79.33 W\nPower logger configured:\n  - Sample interval: 100 ms\n  - GPU ID: 0\n\n======================================================================\nSTEP 6: Running Benchmark\n======================================================================\nStarting power logger with manual polling (interval: 100 ms)\n Power logger started (interval: 100 ms)\n\n============================================================\nRUNNING TIMED INFERENCE\n============================================================\nRunning inference:  14%|‚ñà‚ñà‚ñã                | 144/1000 [00:12<01:11, 11.99iter/s]  Power samples collected: 100\nRunning inference:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 298/1000 [00:24<00:59, 11.88iter/s]  Power samples collected: 200\nRunning inference:  45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 450/1000 [00:37<00:46, 11.84iter/s]  Power samples collected: 300\nRunning inference:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 604/1000 [00:50<00:33, 11.99iter/s]  Power samples collected: 400\nRunning inference:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 756/1000 [01:03<00:20, 12.00iter/s]  Power samples collected: 500\nRunning inference:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 908/1000 [01:16<00:07, 11.98iter/s]  Power samples collected: 600\nRunning inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:23<00:00, 11.95iter/s]\n\n Inference completed: 1000 iterations\n  - Total time: 83.680 s\n  - Mean latency: 83.54 ms\n  - Std latency: 0.58 ms\n  - Min latency: 82.99 ms\n  - Max latency: 86.15 ms\n  - Throughput: 597.51 samples/s\n\n============================================================\nCOMPUTING ACCURACY\n============================================================\n\n Inference with accuracy completed\n  - Accuracy: 86.00% (43/50)\n  - Mean latency: 83.63 ms\n\n============================================================\nBENCHMARK SUMMARY\n============================================================\nLatency (mean): 83.54 ms\nThroughput: 597.51 samples/s\nAccuracy: 86.00%\n============================================================\n Power logger stopped (664 samples collected)\n\n Collected 664 power samples\n Power samples validated\n\n======================================================================\nSTEP 7: Computing Energy Metrics\n======================================================================\n  Mean Power: 234.23 W\n  Power Std: 21.75 W\n  Total Energy: 19600.280 J\n  Energy/Inference: 19.600280 J\n  Energy/Inference: 19600.280 mJ\n\n======================================================================\nSTEP 8: Saving Results\n======================================================================\n\n Results saved to: results/fp16/trial_1_20251201_192319.csv\n Results saved to: results/fp16/trial_1_20251201_192319.json\n\n======================================================================\nEXPERIMENT SUMMARY\n======================================================================\n\nConfiguration:\n  Model: distilbert-base-uncased-finetuned-sst-2-english\n  Precision: fp16\n  Dataset: datasets/tokenized_data\n  Iterations: 1000\n  Trial: 1\n\nPerformance Metrics:\n  Mean Latency: 83.54 ms\n  Std Latency: 0.58 ms\n  Throughput: 597.51 samples/s\n\nPower Metrics:\n  Mean Power: 234.23 W\n  Std Power: 21.75 W\n  Min Power: 42.08 W\n  Max Power: 247.62 W\n\nEnergy Metrics:\n  Total Energy: 19600.280 J\n  Energy/Inference: 19.600280 J\n  Energy/Inference: 19600.280 mJ\n  Inferences/Joule: 0.05\n\nAccuracy Metrics:\n  Accuracy: 86.00%\n  Correct: 43/50\n\nMemory Usage:\n  Peak Allocated: 0.26 GB\n  Peak Reserved: 0.30 GB\n======================================================================\n\n Experiment completed successfully!\n","output_type":"stream"}],"execution_count":70},{"cell_type":"markdown","source":"### Experiment 3: INT8","metadata":{}},{"cell_type":"code","source":"# Run INT8 experiment\n!python src/measure_energy.py \\\n    --precision int8 \\\n    --dataset datasets/tokenized_data \\\n    --num_iters 1000 \\\n    --trial 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T19:26:28.417791Z","iopub.execute_input":"2025-12-01T19:26:28.418600Z","iopub.status.idle":"2025-12-01T19:28:21.234987Z","shell.execute_reply.started":"2025-12-01T19:26:28.418565Z","shell.execute_reply":"2025-12-01T19:28:21.234221Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nENERGY-AWARE QUANTIZATION EXPERIMENT\n======================================================================\nPrecision: int8\nModel: distilbert-base-uncased-finetuned-sst-2-english\nDataset: datasets/tokenized_data\nTrial: 1\n======================================================================\n\nOutput will be saved to: results/int8/trial_1_20251201_192633.csv\n\n======================================================================\nSTEP 1: Checking GPU\n======================================================================\n GPU ready for measurements\n  - Device: Tesla P100-PCIE-16GB\n  - Compute capability: 6.0\n  - Total memory: 17.06 GB\n  - Multi-processors: 56\n  - Memory allocated: 0.00 GB\n  - Memory reserved: 0.00 GB\n\n======================================================================\nSTEP 2: Loading Dataset\n======================================================================\n Loaded 50 samples from datasets/tokenized_data\n  - Input shape: torch.Size([50, 128])\n  - Mask shape: torch.Size([50, 128])\n  - Labels shape: torch.Size([50])\n  - Device: cuda:0\n  - Max sequence length: 128\n Dataset validation passed\n  - Batch size: 50\n  - Sequence length: 128\n  - Unique labels: [0, 1]\n\n======================================================================\nSTEP 3: Loading Model\n======================================================================\n\nLoading model: distilbert-base-uncased-finetuned-sst-2-english\nPrecision: int8\nDevice: cuda\n2025-12-01 19:26:35.200064: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764617195.222553    3404 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764617195.229819    3404 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n  Using CUDA-compatible INT8 (simulated quantization)\n  ‚úì Quantized 38 Linear layers to INT8 precision\n  ‚úì Running on CUDA (simulated INT8 compute)\n‚úì Model loaded successfully\n  - Parameters: 66,955,010\n  - Model size: 255.42 MB\n  - Parameter dtype: torch.float32\n  - Parameter device: cuda:0\n‚úì Model validation passed\n\n======================================================================\nSTEP 4: Warming Up GPU\n======================================================================\nWarming up GPU: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08<00:00, 11.46iter/s]\n Warmup completed: 100 iterations\n  - Average time: 87.09 ms/iter\n  - Min time: 86.32 ms\n  - Max time: 95.14 ms\n\n======================================================================\nSTEP 5: Setting Up Power Logger\n======================================================================\n nvidia-smi available, current power: 80.07 W\nPower logger configured:\n  - Sample interval: 100 ms\n  - GPU ID: 0\n\n======================================================================\nSTEP 6: Running Benchmark\n======================================================================\nStarting power logger with manual polling (interval: 100 ms)\n Power logger started (interval: 100 ms)\n\n============================================================\nRUNNING TIMED INFERENCE\n============================================================\nRunning inference:  14%|‚ñà‚ñà‚ñå                | 138/1000 [00:12<01:15, 11.42iter/s]  Power samples collected: 100\nRunning inference:  28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 284/1000 [00:25<01:03, 11.24iter/s]  Power samples collected: 200\nRunning inference:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 428/1000 [00:37<00:50, 11.23iter/s]  Power samples collected: 300\nRunning inference:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 574/1000 [00:50<00:37, 11.26iter/s]  Power samples collected: 400\nRunning inference:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 718/1000 [01:03<00:24, 11.29iter/s]  Power samples collected: 500\nRunning inference:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 862/1000 [01:16<00:12, 11.29iter/s]  Power samples collected: 600\nRunning inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:28<00:00, 11.28iter/s]\n\n Inference completed: 1000 iterations\n  - Total time: 88.614 s\n  - Mean latency: 88.48 ms\n  - Std latency: 0.73 ms\n  - Min latency: 86.70 ms\n  - Max latency: 90.18 ms\n  - Throughput: 564.24 samples/s\n\n============================================================\nCOMPUTING ACCURACY\n============================================================\n\n Inference with accuracy completed\n  - Accuracy: 88.00% (44/50)\n  - Mean latency: 88.11 ms\n\n============================================================\nBENCHMARK SUMMARY\n============================================================\nLatency (mean): 88.48 ms\nThroughput: 564.24 samples/s\nAccuracy: 88.00%\n============================================================\n Power logger stopped (699 samples collected)\n\n Collected 699 power samples\n Power samples validated\n\n======================================================================\nSTEP 7: Computing Energy Metrics\n======================================================================\n  Mean Power: 234.53 W\n  Power Std: 20.85 W\n  Total Energy: 20782.520 J\n  Energy/Inference: 20.782520 J\n  Energy/Inference: 20782.520 mJ\n\n======================================================================\nSTEP 8: Saving Results\n======================================================================\n\n Results saved to: results/int8/trial_1_20251201_192633.csv\n Results saved to: results/int8/trial_1_20251201_192633.json\n\n======================================================================\nEXPERIMENT SUMMARY\n======================================================================\n\nConfiguration:\n  Model: distilbert-base-uncased-finetuned-sst-2-english\n  Precision: int8\n  Dataset: datasets/tokenized_data\n  Iterations: 1000\n  Trial: 1\n\nPerformance Metrics:\n  Mean Latency: 88.48 ms\n  Std Latency: 0.73 ms\n  Throughput: 564.24 samples/s\n\nPower Metrics:\n  Mean Power: 234.53 W\n  Std Power: 20.85 W\n  Min Power: 42.33 W\n  Max Power: 251.86 W\n\nEnergy Metrics:\n  Total Energy: 20782.520 J\n  Energy/Inference: 20.782520 J\n  Energy/Inference: 20782.520 mJ\n  Inferences/Joule: 0.05\n\nAccuracy Metrics:\n  Accuracy: 88.00%\n  Correct: 44/50\n\nMemory Usage:\n  Peak Allocated: 0.50 GB\n  Peak Reserved: 0.61 GB\n======================================================================\n\n Experiment completed successfully!\n","output_type":"stream"}],"execution_count":71},{"cell_type":"markdown","source":"## Option B: Run Experiments Programmatically (Pure Notebook)\n\nIf you prefer to run everything inside the notebook without calling external scripts:","metadata":{}},{"cell_type":"code","source":"import time\nfrom datetime import datetime\nfrom pathlib import Path\nimport pandas as pd\nimport json\n\nfrom src import (\n    load_pre_tokenized,\n    warmup,\n    check_gpu_ready,\n    PowerLogger,\n    run_steady_state_benchmark,\n    compute_energy_with_timing,\n    get_memory_stats\n)\nfrom models import load_model, get_model_info\n\ndef run_experiment(precision=\"fp32\", dataset_path=\"datasets/tokenized_data\", num_iters=1000):\n    \"\"\"\n    Run a complete experiment for a given precision.\n    \"\"\"\n    print(\"=\"*70)\n    print(f\"RUNNING {precision.upper()} EXPERIMENT\")\n    print(\"=\"*70)\n    \n    # Check GPU\n    check_gpu_ready(verbose=True)\n    \n    # Load dataset\n    print(\"\\nLoading dataset...\")\n    input_ids, mask, labels, metadata = load_pre_tokenized(dataset_path, device=\"cuda\")\n    \n    # Load model\n    print(f\"\\nLoading {precision} model...\")\n    model = load_model(\n        \"distilbert-base-uncased-finetuned-sst-2-english\",\n        precision=precision,\n        device=\"cuda\",\n        verbose=True\n    )\n    model_info = get_model_info(model)\n    \n    # Warmup\n    print(\"\\nWarming up GPU...\")\n    warmup(model, input_ids, mask, num_steps=100, verbose=True)\n    \n    # Start power logger\n    print(\"\\nStarting power logger...\")\n    power_logger = PowerLogger(sample_interval_ms=100, gpu_id=0, verbose=False)\n    power_logger.start()\n    time.sleep(0.5)  # Let it stabilize\n    \n    # Run benchmark\n    print(\"\\nRunning benchmark...\")\n    results = run_steady_state_benchmark(\n        model, input_ids, mask, labels,\n        num_iters=num_iters,\n        compute_accuracy=True,\n        verbose=True\n    )\n    \n    # Stop power logger\n    time.sleep(0.5)\n    power_logger.stop()\n    power_samples = power_logger.read()\n    \n    print(f\"\\nCollected {len(power_samples)} power samples\")\n    \n    # Compute energy\n    energy_results = compute_energy_with_timing(power_samples, results)\n    results.update(energy_results)\n    results.update(model_info)\n    results.update(get_memory_stats())\n    \n    # Add metadata\n    results[\"precision\"] = precision\n    results[\"timestamp\"] = datetime.now().isoformat()\n    \n    # Save results\n    output_dir = Path(f\"results/{precision}\")\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    csv_path = output_dir / f\"trial_1_{timestamp}.csv\"\n    json_path = output_dir / f\"trial_1_{timestamp}.json\"\n    \n    df = pd.DataFrame([results])\n    df.to_csv(csv_path, index=False)\n    \n    with open(json_path, \"w\") as f:\n        json.dump(results, f, indent=2)\n    \n    print(f\"\\n‚úì Results saved to {csv_path}\")\n    \n    # Print summary\n    print(\"\\n\" + \"=\"*70)\n    print(\"RESULTS SUMMARY\")\n    print(\"=\"*70)\n    print(f\"Latency:  {results['mean_latency']*1000:.2f} ms\")\n    print(f\"Power:    {results['mean_power_w']:.2f} W\")\n    print(f\"Energy:   {results['energy_per_inference_mj']:.2f} mJ/inference\")\n    print(f\"Accuracy: {results['accuracy']*100:.2f}%\")\n    print(\"=\"*70)\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T19:15:32.826104Z","iopub.execute_input":"2025-12-01T19:15:32.826698Z","iopub.status.idle":"2025-12-01T19:15:33.064623Z","shell.execute_reply.started":"2025-12-01T19:15:32.826667Z","shell.execute_reply":"2025-12-01T19:15:33.063672Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1793255679.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m from src import (\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mload_pre_tokenized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mwarmup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'compute_energy_with_timing' from 'src' (/kaggle/working/energy_aware_quantization/src/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'compute_energy_with_timing' from 'src' (/kaggle/working/energy_aware_quantization/src/__init__.py)","output_type":"error"}],"execution_count":60},{"cell_type":"markdown","source":"### Run FP32 Experiment","metadata":{}},{"cell_type":"code","source":"fp32_results = run_experiment(precision=\"fp32\", num_iters=1000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T19:15:54.963246Z","iopub.execute_input":"2025-12-01T19:15:54.963525Z","iopub.status.idle":"2025-12-01T19:15:54.985551Z","shell.execute_reply.started":"2025-12-01T19:15:54.963504Z","shell.execute_reply":"2025-12-01T19:15:54.984649Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2551633607.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfp32_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fp32\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'run_experiment' is not defined"],"ename":"NameError","evalue":"name 'run_experiment' is not defined","output_type":"error"}],"execution_count":61},{"cell_type":"markdown","source":"### Run FP16 Experiment","metadata":{}},{"cell_type":"code","source":"fp16_results = run_experiment(precision=\"fp16\", num_iters=1000)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Run INT8 Experiment","metadata":{}},{"cell_type":"code","source":"int8_results = run_experiment(precision=\"int8\", num_iters=1000)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 7: Analyze and Compare Results","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Load all results\nresults_data = []\n\nfor precision in [\"fp32\", \"fp16\", \"int8\"]:\n    results_dir = Path(f\"results/{precision}\")\n    csv_files = list(results_dir.glob(\"*.csv\"))\n    \n    if csv_files:\n        df = pd.read_csv(csv_files[-1])  # Get latest\n        results_data.append(df)\n\nif results_data:\n    all_results = pd.concat(results_data, ignore_index=True)\n    \n    # Create comparison table\n    print(\"=\"*80)\n    print(\"COMPARISON TABLE\")\n    print(\"=\"*80)\n    \n    comparison = all_results[[\n        \"precision\",\n        \"mean_latency\",\n        \"throughput\",\n        \"mean_power_w\",\n        \"energy_per_inference_mj\",\n        \"accuracy\",\n        \"model_size_mb\"\n    ]].copy()\n    \n    # Format columns\n    comparison[\"mean_latency\"] = comparison[\"mean_latency\"] * 1000  # to ms\n    comparison[\"accuracy\"] = comparison[\"accuracy\"] * 100  # to %\n    \n    comparison.columns = [\n        \"Precision\",\n        \"Latency (ms)\",\n        \"Throughput (samp/s)\",\n        \"Power (W)\",\n        \"Energy (mJ)\",\n        \"Accuracy (%)\",\n        \"Model Size (MB)\"\n    ]\n    \n    print(comparison.to_string(index=False))\n    print(\"=\"*80)\n    \n    # Calculate improvements vs FP32\n    fp32_row = comparison[comparison[\"Precision\"] == \"fp32\"].iloc[0]\n    \n    print(\"\\nIMPROVEMENTS vs FP32:\")\n    print(\"-\"*80)\n    \n    for _, row in comparison.iterrows():\n        if row[\"Precision\"] != \"fp32\":\n            speedup = fp32_row[\"Latency (ms)\"] / row[\"Latency (ms)\"]\n            energy_reduction = (fp32_row[\"Energy (mJ)\"] - row[\"Energy (mJ)\"]) / fp32_row[\"Energy (mJ)\"] * 100\n            accuracy_delta = row[\"Accuracy (%)\"] - fp32_row[\"Accuracy (%)\"]\n            \n            print(f\"{row['Precision'].upper()}:\")\n            print(f\"  Speedup:          {speedup:.2f}x\")\n            print(f\"  Energy reduction: {energy_reduction:.1f}%\")\n            print(f\"  Accuracy delta:   {accuracy_delta:+.2f}%\")\n            print()\n    \n    print(\"=\"*80)\nelse:\n    print(\"No results found. Run experiments first!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 8: Visualize Results (Optional)","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nif len(results_data) >= 2:\n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n    \n    precisions = all_results[\"precision\"].tolist()\n    \n    # Latency\n    axes[0, 0].bar(precisions, all_results[\"mean_latency\"] * 1000)\n    axes[0, 0].set_title(\"Latency\")\n    axes[0, 0].set_ylabel(\"ms\")\n    \n    # Energy\n    axes[0, 1].bar(precisions, all_results[\"energy_per_inference_mj\"])\n    axes[0, 1].set_title(\"Energy per Inference\")\n    axes[0, 1].set_ylabel(\"mJ\")\n    \n    # Power\n    axes[1, 0].bar(precisions, all_results[\"mean_power_w\"])\n    axes[1, 0].set_title(\"Mean Power\")\n    axes[1, 0].set_ylabel(\"Watts\")\n    \n    # Accuracy\n    axes[1, 1].bar(precisions, all_results[\"accuracy\"] * 100)\n    axes[1, 1].set_title(\"Accuracy\")\n    axes[1, 1].set_ylabel(\"%\")\n    axes[1, 1].set_ylim([80, 100])\n    \n    plt.tight_layout()\n    plt.savefig(\"results/comparison_plots.png\", dpi=150, bbox_inches=\"tight\")\n    plt.show()\n    \n    print(\"‚úì Plots saved to results/comparison_plots.png\")\nelse:\n    print(\"Need at least 2 precision modes to plot. Run more experiments!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 9: Export Results\n\nDownload results for your report.","metadata":{}},{"cell_type":"code","source":"# Create a zip of all results\n!zip -r results.zip results/\n\nprint(\"‚úì Results zipped\")\nprint(\"\\nYou can download 'results.zip' from the output panel on the right.\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Summary\n\nThis notebook provides two ways to run experiments:\n\n1. **Option A**: Call the Python script with `!python src/measure_energy.py ...`\n   - Easiest approach\n   - Uses the production script\n   - Good for multiple trials\n\n2. **Option B**: Run everything programmatically in the notebook\n   - More control\n   - Better for debugging\n   - Immediate access to results\n\n**Both work on Kaggle!** Choose whichever you prefer.\n\n### Next Steps:\n\n1. Run all three precision modes (FP32, FP16, INT8)\n2. Run multiple trials (3-5) for statistical significance\n3. Analyze results and create plots\n4. Export results for your report\n\nGood luck! üöÄ","metadata":{}}]}
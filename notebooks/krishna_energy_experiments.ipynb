{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy-Aware Quantization Experiments\n",
    "## Krishna's Complete Measurement Harness on Kaggle\n",
    "\n",
    "This notebook runs FP32, FP16, and INT8 experiments with comprehensive energy measurements.\n",
    "\n",
    "**Setup:**\n",
    "1. Enable GPU: Settings â†’ Accelerator â†’ GPU P100/T4\n",
    "2. Enable Internet: Settings â†’ Internet â†’ On (to clone GitHub repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Clone or Update Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "repo_path = Path(\"/kaggle/working/energy_aware_quantization\")\n",
    "\n",
    "if repo_path.exists():\n",
    "    print(\"Repository exists, pulling latest changes...\")\n",
    "    !cd /kaggle/working/energy_aware_quantization && git pull origin main\n",
    "else:\n",
    "    print(\"Cloning repository...\")\n",
    "    !cd /kaggle/working && git clone https://github.com/YOUR_USERNAME/energy_aware_quantization.git\n",
    "\n",
    "print(\"\\nâœ“ Repository ready!\")\n",
    "\n",
    "# Change to repo directory\n",
    "os.chdir(\"/kaggle/working/energy_aware_quantization\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements (most should already be in Kaggle)\n",
    "!pip install -q torch transformers numpy pandas tqdm\n",
    "\n",
    "print(\"Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Verify Installation and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, '/kaggle/working/energy_aware_quantization')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SYSTEM CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nPython: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(\"\\nâœ“ GPU is ready!\")\n",
    "else:\n",
    "    print(\"\\n No GPU! Please enable GPU in Settings.\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Check Datasets\n",
    "\n",
    "The pre-tokenized datasets should be in the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "datasets_dir = Path(\"/kaggle/working/energy_aware_quantization/datasets\")\n",
    "\n",
    "print(\"Checking datasets...\\n\")\n",
    "\n",
    "variants = [\"tokenized_data\", \"tokenized_data_small\", \"tokenized_data_large\", \"tokenized_data_standard\"]\n",
    "\n",
    "for variant in variants:\n",
    "    variant_dir = datasets_dir / variant\n",
    "    if variant_dir.exists():\n",
    "        files = list(variant_dir.glob(\"*.pt\")) + list(variant_dir.glob(\"*.json\"))\n",
    "        print(f\"âœ“ {variant}: {len(files)} files\")\n",
    "    else:\n",
    "        print(f\"âŒ {variant}: NOT FOUND\")\n",
    "\n",
    "print(\"\\nâœ“ Datasets verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test Import of Measurement Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test imports\n",
    "print(\"Testing module imports...\\n\")\n",
    "\n",
    "try:\n",
    "    from src import load_pre_tokenized, warmup, PowerLogger, run_inference, compute_energy\n",
    "    print(\"âœ“ src modules imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Failed to import src: {e}\")\n",
    "\n",
    "try:\n",
    "    from models import load_model, get_model_info\n",
    "    print(\"âœ“ models module imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Failed to import models: {e}\")\n",
    "\n",
    "print(\"\\nâœ“ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Quick Test - Load Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import load_pre_tokenized\n",
    "from models import load_model\n",
    "\n",
    "print(\"Testing dataset and model loading...\\n\")\n",
    "\n",
    "# Load small dataset for quick test\n",
    "input_ids, mask, labels, metadata = load_pre_tokenized(\n",
    "    \"datasets/tokenized_data\",\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset loaded: {input_ids.shape[0]} samples\")\n",
    "\n",
    "# Load FP32 model\n",
    "model = load_model(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    precision=\"fp32\",\n",
    "    device=\"cuda\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Quick test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option A: Run Experiments via Python Script\n",
    "\n",
    "**Yes, you CAN run .py files in Kaggle notebooks using `!python`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: FP32 Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run FP32 experiment\n",
    "!python src/measure_energy.py \\\n",
    "    --precision fp32 \\\n",
    "    --dataset datasets/tokenized_data \\\n",
    "    --num_iters 1000 \\\n",
    "    --trial 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run FP16 experiment\n",
    "!python src/measure_energy.py \\\n",
    "    --precision fp16 \\\n",
    "    --dataset datasets/tokenized_data \\\n",
    "    --num_iters 1000 \\\n",
    "    --trial 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: INT8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run INT8 experiment\n",
    "!python src/measure_energy.py \\\n",
    "    --precision int8 \\\n",
    "    --dataset datasets/tokenized_data \\\n",
    "    --num_iters 1000 \\\n",
    "    --trial 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option B: Run Experiments Programmatically (Pure Notebook)\n",
    "\n",
    "If you prefer to run everything inside the notebook without calling external scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from src import (\n",
    "    load_pre_tokenized,\n",
    "    warmup,\n",
    "    check_gpu_ready,\n",
    "    PowerLogger,\n",
    "    run_steady_state_benchmark,\n",
    "    compute_energy_with_timing,\n",
    "    get_memory_stats\n",
    ")\n",
    "from models import load_model, get_model_info\n",
    "\n",
    "def run_experiment(precision=\"fp32\", dataset_path=\"datasets/tokenized_data\", num_iters=1000):\n",
    "    \"\"\"\n",
    "    Run a complete experiment for a given precision.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(f\"RUNNING {precision.upper()} EXPERIMENT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check GPU\n",
    "    check_gpu_ready(verbose=True)\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"\\nLoading dataset...\")\n",
    "    input_ids, mask, labels, metadata = load_pre_tokenized(dataset_path, device=\"cuda\")\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"\\nLoading {precision} model...\")\n",
    "    model = load_model(\n",
    "        \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        precision=precision,\n",
    "        device=\"cuda\",\n",
    "        verbose=True\n",
    "    )\n",
    "    model_info = get_model_info(model)\n",
    "    \n",
    "    # Warmup\n",
    "    print(\"\\nWarming up GPU...\")\n",
    "    warmup(model, input_ids, mask, num_steps=100, verbose=True)\n",
    "    \n",
    "    # Start power logger\n",
    "    print(\"\\nStarting power logger...\")\n",
    "    power_logger = PowerLogger(sample_interval_ms=100, gpu_id=0, verbose=False)\n",
    "    power_logger.start()\n",
    "    time.sleep(0.5)  # Let it stabilize\n",
    "    \n",
    "    # Run benchmark\n",
    "    print(\"\\nRunning benchmark...\")\n",
    "    results = run_steady_state_benchmark(\n",
    "        model, input_ids, mask, labels,\n",
    "        num_iters=num_iters,\n",
    "        compute_accuracy=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Stop power logger\n",
    "    time.sleep(0.5)\n",
    "    power_logger.stop()\n",
    "    power_samples = power_logger.read()\n",
    "    \n",
    "    print(f\"\\nCollected {len(power_samples)} power samples\")\n",
    "    \n",
    "    # Compute energy\n",
    "    energy_results = compute_energy_with_timing(power_samples, results)\n",
    "    results.update(energy_results)\n",
    "    results.update(model_info)\n",
    "    results.update(get_memory_stats())\n",
    "    \n",
    "    # Add metadata\n",
    "    results[\"precision\"] = precision\n",
    "    results[\"timestamp\"] = datetime.now().isoformat()\n",
    "    \n",
    "    # Save results\n",
    "    output_dir = Path(f\"results/{precision}\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_path = output_dir / f\"trial_1_{timestamp}.csv\"\n",
    "    json_path = output_dir / f\"trial_1_{timestamp}.json\"\n",
    "    \n",
    "    df = pd.DataFrame([results])\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ“ Results saved to {csv_path}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RESULTS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Latency:  {results['mean_latency']*1000:.2f} ms\")\n",
    "    print(f\"Power:    {results['mean_power_w']:.2f} W\")\n",
    "    print(f\"Energy:   {results['energy_per_inference_mj']:.2f} mJ/inference\")\n",
    "    print(f\"Accuracy: {results['accuracy']*100:.2f}%\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run FP32 Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_results = run_experiment(precision=\"fp32\", num_iters=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run FP16 Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp16_results = run_experiment(precision=\"fp16\", num_iters=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run INT8 Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int8_results = run_experiment(precision=\"int8\", num_iters=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Analyze and Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load all results\n",
    "results_data = []\n",
    "\n",
    "for precision in [\"fp32\", \"fp16\", \"int8\"]:\n",
    "    results_dir = Path(f\"results/{precision}\")\n",
    "    csv_files = list(results_dir.glob(\"*.csv\"))\n",
    "    \n",
    "    if csv_files:\n",
    "        df = pd.read_csv(csv_files[-1])  # Get latest\n",
    "        results_data.append(df)\n",
    "\n",
    "if results_data:\n",
    "    all_results = pd.concat(results_data, ignore_index=True)\n",
    "    \n",
    "    # Create comparison table\n",
    "    print(\"=\"*80)\n",
    "    print(\"COMPARISON TABLE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    comparison = all_results[[\n",
    "        \"precision\",\n",
    "        \"mean_latency\",\n",
    "        \"throughput\",\n",
    "        \"mean_power_w\",\n",
    "        \"energy_per_inference_mj\",\n",
    "        \"accuracy\",\n",
    "        \"model_size_mb\"\n",
    "    ]].copy()\n",
    "    \n",
    "    # Format columns\n",
    "    comparison[\"mean_latency\"] = comparison[\"mean_latency\"] * 1000  # to ms\n",
    "    comparison[\"accuracy\"] = comparison[\"accuracy\"] * 100  # to %\n",
    "    \n",
    "    comparison.columns = [\n",
    "        \"Precision\",\n",
    "        \"Latency (ms)\",\n",
    "        \"Throughput (samp/s)\",\n",
    "        \"Power (W)\",\n",
    "        \"Energy (mJ)\",\n",
    "        \"Accuracy (%)\",\n",
    "        \"Model Size (MB)\"\n",
    "    ]\n",
    "    \n",
    "    print(comparison.to_string(index=False))\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Calculate improvements vs FP32\n",
    "    fp32_row = comparison[comparison[\"Precision\"] == \"fp32\"].iloc[0]\n",
    "    \n",
    "    print(\"\\nIMPROVEMENTS vs FP32:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for _, row in comparison.iterrows():\n",
    "        if row[\"Precision\"] != \"fp32\":\n",
    "            speedup = fp32_row[\"Latency (ms)\"] / row[\"Latency (ms)\"]\n",
    "            energy_reduction = (fp32_row[\"Energy (mJ)\"] - row[\"Energy (mJ)\"]) / fp32_row[\"Energy (mJ)\"] * 100\n",
    "            accuracy_delta = row[\"Accuracy (%)\"] - fp32_row[\"Accuracy (%)\"]\n",
    "            \n",
    "            print(f\"{row['Precision'].upper()}:\")\n",
    "            print(f\"  Speedup:          {speedup:.2f}x\")\n",
    "            print(f\"  Energy reduction: {energy_reduction:.1f}%\")\n",
    "            print(f\"  Accuracy delta:   {accuracy_delta:+.2f}%\")\n",
    "            print()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"No results found. Run experiments first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Visualize Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if len(results_data) >= 2:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    precisions = all_results[\"precision\"].tolist()\n",
    "    \n",
    "    # Latency\n",
    "    axes[0, 0].bar(precisions, all_results[\"mean_latency\"] * 1000)\n",
    "    axes[0, 0].set_title(\"Latency\")\n",
    "    axes[0, 0].set_ylabel(\"ms\")\n",
    "    \n",
    "    # Energy\n",
    "    axes[0, 1].bar(precisions, all_results[\"energy_per_inference_mj\"])\n",
    "    axes[0, 1].set_title(\"Energy per Inference\")\n",
    "    axes[0, 1].set_ylabel(\"mJ\")\n",
    "    \n",
    "    # Power\n",
    "    axes[1, 0].bar(precisions, all_results[\"mean_power_w\"])\n",
    "    axes[1, 0].set_title(\"Mean Power\")\n",
    "    axes[1, 0].set_ylabel(\"Watts\")\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1, 1].bar(precisions, all_results[\"accuracy\"] * 100)\n",
    "    axes[1, 1].set_title(\"Accuracy\")\n",
    "    axes[1, 1].set_ylabel(\"%\")\n",
    "    axes[1, 1].set_ylim([80, 100])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"results/comparison_plots.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ“ Plots saved to results/comparison_plots.png\")\n",
    "else:\n",
    "    print(\"Need at least 2 precision modes to plot. Run more experiments!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Export Results\n",
    "\n",
    "Download results for your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip of all results\n",
    "!zip -r results.zip results/\n",
    "\n",
    "print(\"âœ“ Results zipped\")\n",
    "print(\"\\nYou can download 'results.zip' from the output panel on the right.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides two ways to run experiments:\n",
    "\n",
    "1. **Option A**: Call the Python script with `!python src/measure_energy.py ...`\n",
    "   - Easiest approach\n",
    "   - Uses the production script\n",
    "   - Good for multiple trials\n",
    "\n",
    "2. **Option B**: Run everything programmatically in the notebook\n",
    "   - More control\n",
    "   - Better for debugging\n",
    "   - Immediate access to results\n",
    "\n",
    "**Both work on Kaggle!** Choose whichever you prefer.\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Run all three precision modes (FP32, FP16, INT8)\n",
    "2. Run multiple trials (3-5) for statistical significance\n",
    "3. Analyze results and create plots\n",
    "4. Export results for your report\n",
    "\n",
    "Good luck! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

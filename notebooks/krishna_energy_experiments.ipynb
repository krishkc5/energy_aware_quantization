{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!cd /kaggle/working/energy_aware_quantization && git pull origin main","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T18:23:57.660780Z","iopub.execute_input":"2025-12-01T18:23:57.661516Z","iopub.status.idle":"2025-12-01T18:23:57.980892Z","shell.execute_reply.started":"2025-12-01T18:23:57.661484Z","shell.execute_reply":"2025-12-01T18:23:57.980116Z"}},"outputs":[{"name":"stdout","text":"From https://github.com/krishkc5/energy_aware_quantization\n * branch            main       -> FETCH_HEAD\nAlready up to date.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# Energy-Aware Quantization Experiments\n## Krishna's Complete Measurement Harness on Kaggle\n\nThis notebook runs FP32, FP16, and INT8 experiments with comprehensive energy measurements.\n\n**Setup:**\n1. Enable GPU: Settings â†’ Accelerator â†’ GPU P100/T4\n2. Enable Internet: Settings â†’ Internet â†’ On (to clone GitHub repo)","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Clone or Update Repository","metadata":{}},{"cell_type":"code","source":"import os\nfrom pathlib import Path\n\nrepo_path = Path(\"/kaggle/working/energy_aware_quantization\")\n\nif repo_path.exists():\n    print(\"Repository exists, pulling latest changes...\")\n    !cd /kaggle/working/energy_aware_quantization && git pull origin main\nelse:\n    print(\"Cloning repository...\")\n    !cd /kaggle/working && git clone https://github.com/YOUR_USERNAME/energy_aware_quantization.git\n\nprint(\"\\nâœ“ Repository ready!\")\n\n# Change to repo directory\nos.chdir(\"/kaggle/working/energy_aware_quantization\")\nprint(f\"Working directory: {os.getcwd()}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 2: Install Dependencies","metadata":{}},{"cell_type":"code","source":"# Install requirements (most should already be in Kaggle)\n!pip install -q torch transformers numpy pandas tqdm\n\nprint(\"Dependencies installed\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3: Verify Installation and GPU","metadata":{}},{"cell_type":"code","source":"import sys\nimport torch\n\n# Add src to path\nsys.path.insert(0, '/kaggle/working/energy_aware_quantization')\n\nprint(\"=\"*70)\nprint(\"SYSTEM CHECK\")\nprint(\"=\"*70)\n\nprint(f\"\\nPython: {sys.version}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"CUDA version: {torch.version.cuda}\")\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    print(\"\\nâœ“ GPU is ready!\")\nelse:\n    print(\"\\n No GPU! Please enable GPU in Settings.\")\n\nprint(\"=\"*70)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 4: Check Datasets\n\nThe pre-tokenized datasets should be in the repo.","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\ndatasets_dir = Path(\"/kaggle/working/energy_aware_quantization/datasets\")\n\nprint(\"Checking datasets...\\n\")\n\nvariants = [\"tokenized_data\", \"tokenized_data_small\", \"tokenized_data_large\", \"tokenized_data_standard\"]\n\nfor variant in variants:\n    variant_dir = datasets_dir / variant\n    if variant_dir.exists():\n        files = list(variant_dir.glob(\"*.pt\")) + list(variant_dir.glob(\"*.json\"))\n        print(f\"âœ“ {variant}: {len(files)} files\")\n    else:\n        print(f\"âŒ {variant}: NOT FOUND\")\n\nprint(\"\\nâœ“ Datasets verified\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 5: Test Import of Measurement Modules","metadata":{}},{"cell_type":"code","source":"# Test imports\nprint(\"Testing module imports...\\n\")\n\ntry:\n    from src import load_pre_tokenized, warmup, PowerLogger, run_inference, compute_energy\n    print(\"âœ“ src modules imported\")\nexcept ImportError as e:\n    print(f\"âŒ Failed to import src: {e}\")\n\ntry:\n    from models import load_model, get_model_info\n    print(\"âœ“ models module imported\")\nexcept ImportError as e:\n    print(f\"âŒ Failed to import models: {e}\")\n\nprint(\"\\nâœ“ All imports successful!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 6: Quick Test - Load Dataset and Model","metadata":{}},{"cell_type":"code","source":"from src import load_pre_tokenized\nfrom models import load_model\n\nprint(\"Testing dataset and model loading...\\n\")\n\n# Load small dataset for quick test\ninput_ids, mask, labels, metadata = load_pre_tokenized(\n    \"datasets/tokenized_data\",\n    device=\"cuda\"\n)\n\nprint(f\"\\nDataset loaded: {input_ids.shape[0]} samples\")\n\n# Load FP32 model\nmodel = load_model(\n    \"distilbert-base-uncased-finetuned-sst-2-english\",\n    precision=\"fp32\",\n    device=\"cuda\",\n    verbose=True\n)\n\nprint(\"\\nâœ“ Quick test passed!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Option A: Run Experiments via Python Script\n\n**Yes, you CAN run .py files in Kaggle notebooks using `!python`**","metadata":{}},{"cell_type":"markdown","source":"### Experiment 1: FP32 Baseline","metadata":{}},{"cell_type":"code","source":"# Run FP32 experiment\n!python src/measure_energy.py \\\n    --precision fp32 \\\n    --dataset datasets/tokenized_data \\\n    --num_iters 1000 \\\n    --trial 1","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Experiment 2: FP16","metadata":{}},{"cell_type":"code","source":"# Run FP16 experiment\n!python src/measure_energy.py \\\n    --precision fp16 \\\n    --dataset datasets/tokenized_data \\\n    --num_iters 1000 \\\n    --trial 1","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Experiment 3: INT8","metadata":{}},{"cell_type":"code","source":"# Run INT8 experiment\n!python src/measure_energy.py \\\n    --precision int8 \\\n    --dataset datasets/tokenized_data \\\n    --num_iters 1000 \\\n    --trial 1","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Option B: Run Experiments Programmatically (Pure Notebook)\n\nIf you prefer to run everything inside the notebook without calling external scripts:","metadata":{}},{"cell_type":"code","source":"import time\nfrom datetime import datetime\nfrom pathlib import Path\nimport pandas as pd\nimport json\n\nfrom src import (\n    load_pre_tokenized,\n    warmup,\n    check_gpu_ready,\n    PowerLogger,\n    run_steady_state_benchmark,\n    compute_energy_with_timing,\n    get_memory_stats\n)\nfrom models import load_model, get_model_info\n\ndef run_experiment(precision=\"fp32\", dataset_path=\"datasets/tokenized_data\", num_iters=1000):\n    \"\"\"\n    Run a complete experiment for a given precision.\n    \"\"\"\n    print(\"=\"*70)\n    print(f\"RUNNING {precision.upper()} EXPERIMENT\")\n    print(\"=\"*70)\n    \n    # Check GPU\n    check_gpu_ready(verbose=True)\n    \n    # Load dataset\n    print(\"\\nLoading dataset...\")\n    input_ids, mask, labels, metadata = load_pre_tokenized(dataset_path, device=\"cuda\")\n    \n    # Load model\n    print(f\"\\nLoading {precision} model...\")\n    model = load_model(\n        \"distilbert-base-uncased-finetuned-sst-2-english\",\n        precision=precision,\n        device=\"cuda\",\n        verbose=True\n    )\n    model_info = get_model_info(model)\n    \n    # Warmup\n    print(\"\\nWarming up GPU...\")\n    warmup(model, input_ids, mask, num_steps=100, verbose=True)\n    \n    # Start power logger\n    print(\"\\nStarting power logger...\")\n    power_logger = PowerLogger(sample_interval_ms=100, gpu_id=0, verbose=False)\n    power_logger.start()\n    time.sleep(0.5)  # Let it stabilize\n    \n    # Run benchmark\n    print(\"\\nRunning benchmark...\")\n    results = run_steady_state_benchmark(\n        model, input_ids, mask, labels,\n        num_iters=num_iters,\n        compute_accuracy=True,\n        verbose=True\n    )\n    \n    # Stop power logger\n    time.sleep(0.5)\n    power_logger.stop()\n    power_samples = power_logger.read()\n    \n    print(f\"\\nCollected {len(power_samples)} power samples\")\n    \n    # Compute energy\n    energy_results = compute_energy_with_timing(power_samples, results)\n    results.update(energy_results)\n    results.update(model_info)\n    results.update(get_memory_stats())\n    \n    # Add metadata\n    results[\"precision\"] = precision\n    results[\"timestamp\"] = datetime.now().isoformat()\n    \n    # Save results\n    output_dir = Path(f\"results/{precision}\")\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    csv_path = output_dir / f\"trial_1_{timestamp}.csv\"\n    json_path = output_dir / f\"trial_1_{timestamp}.json\"\n    \n    df = pd.DataFrame([results])\n    df.to_csv(csv_path, index=False)\n    \n    with open(json_path, \"w\") as f:\n        json.dump(results, f, indent=2)\n    \n    print(f\"\\nâœ“ Results saved to {csv_path}\")\n    \n    # Print summary\n    print(\"\\n\" + \"=\"*70)\n    print(\"RESULTS SUMMARY\")\n    print(\"=\"*70)\n    print(f\"Latency:  {results['mean_latency']*1000:.2f} ms\")\n    print(f\"Power:    {results['mean_power_w']:.2f} W\")\n    print(f\"Energy:   {results['energy_per_inference_mj']:.2f} mJ/inference\")\n    print(f\"Accuracy: {results['accuracy']*100:.2f}%\")\n    print(\"=\"*70)\n    \n    return results","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Run FP32 Experiment","metadata":{}},{"cell_type":"code","source":"fp32_results = run_experiment(precision=\"fp32\", num_iters=1000)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Run FP16 Experiment","metadata":{}},{"cell_type":"code","source":"fp16_results = run_experiment(precision=\"fp16\", num_iters=1000)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Run INT8 Experiment","metadata":{}},{"cell_type":"code","source":"int8_results = run_experiment(precision=\"int8\", num_iters=1000)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 7: Analyze and Compare Results","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Load all results\nresults_data = []\n\nfor precision in [\"fp32\", \"fp16\", \"int8\"]:\n    results_dir = Path(f\"results/{precision}\")\n    csv_files = list(results_dir.glob(\"*.csv\"))\n    \n    if csv_files:\n        df = pd.read_csv(csv_files[-1])  # Get latest\n        results_data.append(df)\n\nif results_data:\n    all_results = pd.concat(results_data, ignore_index=True)\n    \n    # Create comparison table\n    print(\"=\"*80)\n    print(\"COMPARISON TABLE\")\n    print(\"=\"*80)\n    \n    comparison = all_results[[\n        \"precision\",\n        \"mean_latency\",\n        \"throughput\",\n        \"mean_power_w\",\n        \"energy_per_inference_mj\",\n        \"accuracy\",\n        \"model_size_mb\"\n    ]].copy()\n    \n    # Format columns\n    comparison[\"mean_latency\"] = comparison[\"mean_latency\"] * 1000  # to ms\n    comparison[\"accuracy\"] = comparison[\"accuracy\"] * 100  # to %\n    \n    comparison.columns = [\n        \"Precision\",\n        \"Latency (ms)\",\n        \"Throughput (samp/s)\",\n        \"Power (W)\",\n        \"Energy (mJ)\",\n        \"Accuracy (%)\",\n        \"Model Size (MB)\"\n    ]\n    \n    print(comparison.to_string(index=False))\n    print(\"=\"*80)\n    \n    # Calculate improvements vs FP32\n    fp32_row = comparison[comparison[\"Precision\"] == \"fp32\"].iloc[0]\n    \n    print(\"\\nIMPROVEMENTS vs FP32:\")\n    print(\"-\"*80)\n    \n    for _, row in comparison.iterrows():\n        if row[\"Precision\"] != \"fp32\":\n            speedup = fp32_row[\"Latency (ms)\"] / row[\"Latency (ms)\"]\n            energy_reduction = (fp32_row[\"Energy (mJ)\"] - row[\"Energy (mJ)\"]) / fp32_row[\"Energy (mJ)\"] * 100\n            accuracy_delta = row[\"Accuracy (%)\"] - fp32_row[\"Accuracy (%)\"]\n            \n            print(f\"{row['Precision'].upper()}:\")\n            print(f\"  Speedup:          {speedup:.2f}x\")\n            print(f\"  Energy reduction: {energy_reduction:.1f}%\")\n            print(f\"  Accuracy delta:   {accuracy_delta:+.2f}%\")\n            print()\n    \n    print(\"=\"*80)\nelse:\n    print(\"No results found. Run experiments first!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 8: Visualize Results (Optional)","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nif len(results_data) >= 2:\n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n    \n    precisions = all_results[\"precision\"].tolist()\n    \n    # Latency\n    axes[0, 0].bar(precisions, all_results[\"mean_latency\"] * 1000)\n    axes[0, 0].set_title(\"Latency\")\n    axes[0, 0].set_ylabel(\"ms\")\n    \n    # Energy\n    axes[0, 1].bar(precisions, all_results[\"energy_per_inference_mj\"])\n    axes[0, 1].set_title(\"Energy per Inference\")\n    axes[0, 1].set_ylabel(\"mJ\")\n    \n    # Power\n    axes[1, 0].bar(precisions, all_results[\"mean_power_w\"])\n    axes[1, 0].set_title(\"Mean Power\")\n    axes[1, 0].set_ylabel(\"Watts\")\n    \n    # Accuracy\n    axes[1, 1].bar(precisions, all_results[\"accuracy\"] * 100)\n    axes[1, 1].set_title(\"Accuracy\")\n    axes[1, 1].set_ylabel(\"%\")\n    axes[1, 1].set_ylim([80, 100])\n    \n    plt.tight_layout()\n    plt.savefig(\"results/comparison_plots.png\", dpi=150, bbox_inches=\"tight\")\n    plt.show()\n    \n    print(\"âœ“ Plots saved to results/comparison_plots.png\")\nelse:\n    print(\"Need at least 2 precision modes to plot. Run more experiments!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 9: Export Results\n\nDownload results for your report.","metadata":{}},{"cell_type":"code","source":"# Create a zip of all results\n!zip -r results.zip results/\n\nprint(\"âœ“ Results zipped\")\nprint(\"\\nYou can download 'results.zip' from the output panel on the right.\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Summary\n\nThis notebook provides two ways to run experiments:\n\n1. **Option A**: Call the Python script with `!python src/measure_energy.py ...`\n   - Easiest approach\n   - Uses the production script\n   - Good for multiple trials\n\n2. **Option B**: Run everything programmatically in the notebook\n   - More control\n   - Better for debugging\n   - Immediate access to results\n\n**Both work on Kaggle!** Choose whichever you prefer.\n\n### Next Steps:\n\n1. Run all three precision modes (FP32, FP16, INT8)\n2. Run multiple trials (3-5) for statistical significance\n3. Analyze results and create plots\n4. Export results for your report\n\nGood luck! ðŸš€","metadata":{}}]}
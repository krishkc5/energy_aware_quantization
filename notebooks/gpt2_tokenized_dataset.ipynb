{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GPT-2 Small: Pre-tokenized Dataset for Energy Measurement\n## WikiText-2 with ZERO I/O Overhead\n\nThis notebook creates a pre-tokenized dataset for GPT-2 Small with **ZERO I/O overhead** during energy measurements.\n\n**Dataset**: WikiText-2 (standard language modeling benchmark)\n\n**Model**: GPT-2 Small (124M parameters)\n\n**Task**: Next-token prediction (language modeling)\n\n**Key Feature**: All data pre-tokenized and loaded to GPU once - no CPU‚ÜíGPU transfers during inference!","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Verify GPU Access","metadata":{}},{"cell_type":"code","source":"import torch\n\nprint(\"=\"*60)\nprint(\"GPU CHECK\")\nprint(\"=\"*60)\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"CUDA version: {torch.version.cuda}\")\n    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n    print(f\"Device count: {torch.cuda.device_count()}\")\n    \n    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"Total GPU memory: {total_memory:.2f} GB\")\n    print(\"\\n‚úì GPU is ready!\")\nelse:\n    print(\"\\n‚ö†Ô∏è WARNING: GPU not available!\")\n\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T01:22:22.266344Z","iopub.execute_input":"2025-12-04T01:22:22.267091Z","iopub.status.idle":"2025-12-04T01:22:22.273752Z","shell.execute_reply.started":"2025-12-04T01:22:22.267045Z","shell.execute_reply":"2025-12-04T01:22:22.272988Z"}},"outputs":[{"name":"stdout","text":"============================================================\nGPU CHECK\n============================================================\nPyTorch version: 2.6.0+cu124\nCUDA available: True\nCUDA version: 12.4\nDevice name: Tesla T4\nDevice count: 2\nTotal GPU memory: 15.83 GB\n\n‚úì GPU is ready!\n============================================================\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Step 2: Install Dependencies","metadata":{}},{"cell_type":"code","source":"# Install required packages\n!pip install -q transformers datasets accelerate\n\nprint(\"‚úì Dependencies installed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T01:22:22.275173Z","iopub.execute_input":"2025-12-04T01:22:22.275669Z","iopub.status.idle":"2025-12-04T01:22:25.812174Z","shell.execute_reply.started":"2025-12-04T01:22:22.275652Z","shell.execute_reply":"2025-12-04T01:22:25.811358Z"}},"outputs":[{"name":"stdout","text":"‚úì Dependencies installed\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Step 3: Define Dataset Preparation Functions for GPT-2","metadata":{}},{"cell_type":"code","source":"\"\"\"\nGPT-2 Dataset Preparation Module\nPre-tokenize WikiText-2 to eliminate I/O overhead during energy measurement\n\"\"\"\n\nimport torch\nfrom transformers import GPT2Tokenizer\nfrom datasets import load_dataset\nfrom pathlib import Path\nimport json\n\n\ndef prepare_gpt2_tokenized_dataset(\n    num_samples: int = 10000,\n    max_length: int = 128,\n    output_dir: str = \"/kaggle/working/gpt2_tokenized_data\",\n    seed: int = 42\n):\n    \"\"\"\n    Pre-tokenize WikiText-2 dataset for GPT-2 and save to disk.\n    \n    Args:\n        num_samples: Number of sequences to tokenize (set high to use full dataset, ~4000 available)\n        max_length: Maximum sequence length (128 is good for GPT-2)\n        output_dir: Directory to save tokenized data\n        seed: Random seed for reproducibility\n    \"\"\"\n    \n    print(\"=\"*60)\n    print(\"Pre-tokenizing WikiText-2 for GPT-2 Small\")\n    print(\"=\"*60)\n    \n    # Create output directory\n    output_path = Path(output_dir)\n    output_path.mkdir(exist_ok=True)\n    \n    # Load tokenizer\n    print(\"\\n[1/5] Loading GPT-2 tokenizer...\")\n    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n    \n    # GPT-2 tokenizer doesn't have a pad token by default\n    tokenizer.pad_token = tokenizer.eos_token\n    \n    # Load WikiText-2 dataset\n    print(f\"[2/5] Loading WikiText-2 test set...\")\n    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n    \n    # Filter out empty lines and very short texts\n    print(f\"[3/5] Filtering and selecting sequences (seed={seed})...\")\n    \n    # Filter non-empty texts with sufficient length\n    def is_valid_text(example):\n        text = example['text'].strip()\n        return len(text) > 50  # At least 50 characters\n    \n    dataset = dataset.filter(is_valid_text)\n    \n    print(f\"   Total valid sequences available: {len(dataset)}\")\n    \n    # Select samples (up to available) - will use ALL if num_samples > available\n    actual_samples = min(num_samples, len(dataset))\n    dataset = dataset.shuffle(seed=seed).select(range(actual_samples))\n    \n    if actual_samples == len(dataset):\n        print(f\"   Using FULL dataset: {actual_samples} samples\")\n    else:\n        print(f\"   Selected: {actual_samples} samples\")\n    \n    # Tokenize all examples\n    print(f\"[4/5] Tokenizing with max_length={max_length}...\")\n    texts = [example['text'] for example in dataset]\n    \n    # Tokenize in batch\n    encodings = tokenizer(\n        texts,\n        padding='max_length',\n        truncation=True,\n        max_length=max_length,\n        return_tensors='pt'\n    )\n    \n    # For language modeling, labels are the input_ids shifted by 1\n    # We'll just use input_ids as labels (model handles the shift internally)\n    labels = encodings['input_ids'].clone()\n    \n    # Save tensors\n    print(f\"[5/5] Saving to {output_dir}...\")\n    torch.save(encodings['input_ids'], output_path / 'input_ids.pt')\n    torch.save(encodings['attention_mask'], output_path / 'attention_mask.pt')\n    torch.save(labels, output_path / 'labels.pt')\n    \n    # Save metadata\n    metadata = {\n        'num_samples': actual_samples,\n        'max_length': max_length,\n        'dataset_name': 'wikitext-2',\n        'model': 'gpt2',\n        'task': 'language_modeling',\n        'seed': seed,\n        'tokenizer': 'gpt2',\n    }\n    \n    with open(output_path / 'metadata.json', 'w') as f:\n        json.dump(metadata, f, indent=2)\n    \n    # Print summary\n    print(\"\\n\" + \"=\"*60)\n    print(\"Dataset Preparation Complete!\")\n    print(\"=\"*60)\n    print(f\"Number of samples:     {len(texts)}\")\n    print(f\"Max sequence length:   {max_length}\")\n    print(f\"Dataset:               WikiText-2\")\n    print(f\"Task:                  Language Modeling\")\n    print(f\"\\nSaved files:\")\n    print(f\"  - input_ids.pt       {encodings['input_ids'].shape}\")\n    print(f\"  - attention_mask.pt  {encodings['attention_mask'].shape}\")\n    print(f\"  - labels.pt          {labels.shape}\")\n    print(f\"  - metadata.json\")\n    \n    # Calculate approximate size\n    total_size_mb = (encodings['input_ids'].element_size() * encodings['input_ids'].nelement() * 3) / (1024**2)\n    print(f\"\\nTotal dataset size: ~{total_size_mb:.2f} MB\")\n    print(\"=\"*60)\n    \n    # Show examples\n    print(\"\\nFirst 3 text samples:\")\n    for i in range(min(3, len(texts))):\n        preview = texts[i][:100].replace('\\n', ' ')\n        print(f\"\\n{i+1}. {preview}...\")\n    \n    return metadata\n\n\nclass GPT2PreTokenizedDataset:\n    \"\"\"\n    Efficient dataset class for pre-tokenized GPT-2 data.\n    Zero I/O overhead during iteration.\n    \"\"\"\n    \n    def __init__(self, data_dir: str = \"/kaggle/working/gpt2_tokenized_data\"):\n        \"\"\"Load pre-tokenized dataset from disk.\"\"\"\n        data_path = Path(data_dir)\n        \n        # Load all data into memory once\n        self.input_ids = torch.load(data_path / 'input_ids.pt')\n        self.attention_mask = torch.load(data_path / 'attention_mask.pt')\n        self.labels = torch.load(data_path / 'labels.pt')\n        \n        with open(data_path / 'metadata.json', 'r') as f:\n            self.metadata = json.load(f)\n        \n        self.num_samples = len(self.labels)\n    \n    def __len__(self):\n        return self.num_samples\n    \n    def __getitem__(self, idx):\n        \"\"\"Get a single example.\"\"\"\n        return {\n            'input_ids': self.input_ids[idx],\n            'attention_mask': self.attention_mask[idx],\n            'labels': self.labels[idx]\n        }\n    \n    def to_device(self, device):\n        \"\"\"Move all tensors to device (GPU) at once.\"\"\"\n        self.input_ids = self.input_ids.to(device)\n        self.attention_mask = self.attention_mask.to(device)\n        self.labels = self.labels.to(device)\n        return self\n\n\nprint(\"‚úì GPT-2 dataset preparation functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T01:22:25.813329Z","iopub.execute_input":"2025-12-04T01:22:25.813625Z","iopub.status.idle":"2025-12-04T01:22:25.829012Z","shell.execute_reply.started":"2025-12-04T01:22:25.813600Z","shell.execute_reply":"2025-12-04T01:22:25.828275Z"}},"outputs":[{"name":"stdout","text":"‚úì GPT-2 dataset preparation functions defined\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Step 4: Create Pre-tokenized Dataset\n\nCreate dataset from **FULL WikiText-2 test set** (~4000 sequences after filtering).","metadata":{}},{"cell_type":"code","source":"# Create the tokenized dataset using FULL WikiText-2 test set\n# Set num_samples very high (10000) - it will automatically use all available sequences\nmetadata = prepare_gpt2_tokenized_dataset(\n    num_samples=10000,  # Will use all available (typically ~4000 after filtering)\n    max_length=128,\n    output_dir='/kaggle/working/gpt2_tokenized_data',\n    seed=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T01:22:25.830378Z","iopub.execute_input":"2025-12-04T01:22:25.830679Z","iopub.status.idle":"2025-12-04T01:22:29.130696Z","shell.execute_reply.started":"2025-12-04T01:22:25.830662Z","shell.execute_reply":"2025-12-04T01:22:29.129893Z"}},"outputs":[{"name":"stdout","text":"============================================================\nPre-tokenizing WikiText-2 for GPT-2 Small\n============================================================\n\n[1/5] Loading GPT-2 tokenizer...\n[2/5] Loading WikiText-2 test set...\n[3/5] Filtering and selecting sequences (seed=42)...\n   Total valid sequences available: 1940\n   Using FULL dataset: 1940 samples\n[4/5] Tokenizing with max_length=128...\n[5/5] Saving to /kaggle/working/gpt2_tokenized_data...\n\n============================================================\nDataset Preparation Complete!\n============================================================\nNumber of samples:     1940\nMax sequence length:   128\nDataset:               WikiText-2\nTask:                  Language Modeling\n\nSaved files:\n  - input_ids.pt       torch.Size([1940, 128])\n  - attention_mask.pt  torch.Size([1940, 128])\n  - labels.pt          torch.Size([1940, 128])\n  - metadata.json\n\nTotal dataset size: ~5.68 MB\n============================================================\n\nFirst 3 text samples:\n\n1.  San Lorenzo Colossal Head 7 ( also known as San Lorenzo Monument 53 ) measures 2 @.@ 7 metres ( 8 @...\n\n2.  Pool champion Willie Mosconi has a cameo appearance as Willie , who holds the stakes for Eddie and ...\n\n3.  ‚Ä† ‚Äî A game postponed from Round 7 , held in Round 8 , was played with Victoria Aces as the away tea...\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Step 5: Verify the Dataset","metadata":{}},{"cell_type":"code","source":"# Load and verify the dataset\ndataset = GPT2PreTokenizedDataset('/kaggle/working/gpt2_tokenized_data')\n\nprint(\"=\"*60)\nprint(\"Dataset Verification\")\nprint(\"=\"*60)\nprint(f\"Number of samples: {len(dataset)}\")\nprint(f\"Metadata: {dataset.metadata}\")\n\n# Check first example\nexample = dataset[0]\nprint(f\"\\nFirst example:\")\nprint(f\"  input_ids shape:      {example['input_ids'].shape}\")\nprint(f\"  attention_mask shape: {example['attention_mask'].shape}\")\nprint(f\"  labels shape:         {example['labels'].shape}\")\n\n# Decode first example to verify\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ndecoded_text = tokenizer.decode(example['input_ids'], skip_special_tokens=True)\nprint(f\"\\nDecoded text (first 100 chars): {decoded_text[:100]}...\")\n\nprint(f\"\\n‚úì Dataset verified!\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T01:22:29.132947Z","iopub.execute_input":"2025-12-04T01:22:29.133182Z","iopub.status.idle":"2025-12-04T01:22:29.415751Z","shell.execute_reply.started":"2025-12-04T01:22:29.133163Z","shell.execute_reply":"2025-12-04T01:22:29.415121Z"}},"outputs":[{"name":"stdout","text":"============================================================\nDataset Verification\n============================================================\nNumber of samples: 1940\nMetadata: {'num_samples': 1940, 'max_length': 128, 'dataset_name': 'wikitext-2', 'model': 'gpt2', 'task': 'language_modeling', 'seed': 42, 'tokenizer': 'gpt2'}\n\nFirst example:\n  input_ids shape:      torch.Size([128])\n  attention_mask shape: torch.Size([128])\n  labels shape:         torch.Size([128])\n\nDecoded text (first 100 chars):  San Lorenzo Colossal Head 7 ( also known as San Lorenzo Monument 53 ) measures 2 @.@ 7 metres ( 8 @...\n\n‚úì Dataset verified!\n============================================================\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Step 6: Load GPT-2 Model","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel\nimport torch\n\n# Setup device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load GPT-2 Small model\nprint(\"\\nLoading GPT-2 Small model...\")\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n\nmodel = model.to(device)\nmodel.eval()\n\nprint(\"‚úì GPT-2 Small model loaded and moved to\", device)\n\n# Show model size\nparam_count = sum(p.numel() for p in model.parameters())\nmodel_size_mb = sum(p.element_size() * p.numel() for p in model.parameters()) / (1024 ** 2)\nprint(f\"\\nModel parameters: {param_count:,} ({param_count/1e6:.1f}M)\")\nprint(f\"Model size (FP32): {model_size_mb:.2f} MB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T01:22:29.416397Z","iopub.execute_input":"2025-12-04T01:22:29.416636Z","iopub.status.idle":"2025-12-04T01:22:29.895814Z","shell.execute_reply.started":"2025-12-04T01:22:29.416619Z","shell.execute_reply":"2025-12-04T01:22:29.895029Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\nLoading GPT-2 Small model...\n‚úì GPT-2 Small model loaded and moved to cuda\n\nModel parameters: 124,439,808 (124.4M)\nModel size (FP32): 474.70 MB\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## Step 7: Move Dataset to GPU (ONE TIME)\n\n**KEY FEATURE:** Move all data to GPU once. During measurement, there will be ZERO CPU‚ÜíGPU transfers!","metadata":{}},{"cell_type":"code","source":"# Reload dataset and move to GPU\ndataset = GPT2PreTokenizedDataset('/kaggle/working/gpt2_tokenized_data')\n\nif torch.cuda.is_available():\n    print(\"Moving dataset to GPU...\")\n    dataset.to_device(device)\n    print(\"‚úì Dataset on GPU\")\n    \n    # Verify\n    sample = dataset[0]\n    print(f\"\\nVerification:\")\n    print(f\"  Input_ids device: {sample['input_ids'].device}\")\n    print(f\"  Labels device:    {sample['labels'].device}\")\n    \n    # Check GPU memory\n    allocated = torch.cuda.memory_allocated(0) / 1e6\n    print(f\"\\nGPU memory allocated: {allocated:.2f} MB\")\nelse:\n    print(\"CPU mode - dataset stays in CPU memory\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T01:22:29.896643Z","iopub.execute_input":"2025-12-04T01:22:29.896891Z","iopub.status.idle":"2025-12-04T01:22:29.908880Z","shell.execute_reply.started":"2025-12-04T01:22:29.896874Z","shell.execute_reply":"2025-12-04T01:22:29.908116Z"}},"outputs":[{"name":"stdout","text":"Moving dataset to GPU...\n‚úì Dataset on GPU\n\nVerification:\n  Input_ids device: cuda:0\n  Labels device:    cuda:0\n\nGPU memory allocated: 612.25 MB\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## Step 8: Run Baseline Inference (FP32)\n\nTest the complete pipeline with **ZERO I/O** during inference.","metadata":{}},{"cell_type":"code","source":"import time\nimport torch.nn.functional as F\n\nprint(\"=\"*60)\nprint(\"FP32 Baseline Inference (ZERO I/O!)\")\nprint(\"=\"*60)\n\n# Warmup\nprint(\"\\nWarming up (10 iterations)...\")\nwith torch.no_grad():\n    for i in range(10):\n        sample = dataset[i]\n        _ = model(\n            input_ids=sample['input_ids'].unsqueeze(0),\n            attention_mask=sample['attention_mask'].unsqueeze(0)\n        )\n\nif torch.cuda.is_available():\n    torch.cuda.synchronize()\n\nprint(\"‚úì Warmup complete\")\n\n# Actual inference measurement\nprint(\"\\nRunning inference on all samples...\")\ntotal_loss = 0\nnum_tokens = 0\n\nstart_time = time.perf_counter()\n\nwith torch.no_grad():\n    for i in range(len(dataset)):\n        sample = dataset[i]\n        \n        # Forward pass (NO I/O!)\n        outputs = model(\n            input_ids=sample['input_ids'].unsqueeze(0),\n            attention_mask=sample['attention_mask'].unsqueeze(0),\n            labels=sample['labels'].unsqueeze(0)\n        )\n        \n        total_loss += outputs.loss.item()\n        num_tokens += sample['attention_mask'].sum().item()\n\nif torch.cuda.is_available():\n    torch.cuda.synchronize()\n\nend_time = time.perf_counter()\n\n# Results\nlatency = end_time - start_time\navg_loss = total_loss / len(dataset)\nperplexity = torch.exp(torch.tensor(avg_loss)).item()\nthroughput = len(dataset) / latency\ntokens_per_sec = num_tokens / latency\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Results\")\nprint(\"=\"*60)\nprint(f\"Average Loss:    {avg_loss:.4f}\")\nprint(f\"Perplexity:      {perplexity:.2f}\")\nprint(f\"Latency:         {latency:.3f} seconds\")\nprint(f\"Throughput:      {throughput:.2f} samples/second\")\nprint(f\"Tokens/sec:      {tokens_per_sec:.2f}\")\nprint(f\"Per-sample:      {latency/len(dataset)*1000:.2f} ms\")\nprint(\"=\"*60)\n\nprint(\"\\n‚úì Baseline inference complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T01:22:29.909718Z","iopub.execute_input":"2025-12-04T01:22:29.910061Z","iopub.status.idle":"2025-12-04T01:22:52.030186Z","shell.execute_reply.started":"2025-12-04T01:22:29.910036Z","shell.execute_reply":"2025-12-04T01:22:52.029372Z"}},"outputs":[{"name":"stdout","text":"============================================================\nFP32 Baseline Inference (ZERO I/O!)\n============================================================\n\nWarming up (10 iterations)...\n‚úì Warmup complete\n\nRunning inference on all samples...\n\n============================================================\nResults\n============================================================\nAverage Loss:    5.3825\nPerplexity:      217.57\nLatency:         21.943 seconds\nThroughput:      88.41 samples/second\nTokens/sec:      9019.46\nPer-sample:      11.31 ms\n============================================================\n\n‚úì Baseline inference complete!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Step 9: Test Power Monitoring","metadata":{}},{"cell_type":"code","source":"import subprocess\n\ntry:\n    result = subprocess.run(\n        ['nvidia-smi', '--query-gpu=name,power.draw', '--format=csv,noheader'],\n        capture_output=True,\n        text=True,\n        timeout=2\n    )\n    \n    if result.returncode == 0:\n        print(\"=\"*60)\n        print(\"GPU Power Monitoring Available\")\n        print(\"=\"*60)\n        print(result.stdout.strip())\n        print(\"\\n‚úì nvidia-smi is available for power monitoring\")\n    else:\n        print(\"‚ö†Ô∏è nvidia-smi not responding properly\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è nvidia-smi not available: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T01:22:52.031025Z","iopub.execute_input":"2025-12-04T01:22:52.031284Z","iopub.status.idle":"2025-12-04T01:22:52.069263Z","shell.execute_reply.started":"2025-12-04T01:22:52.031257Z","shell.execute_reply":"2025-12-04T01:22:52.068707Z"}},"outputs":[{"name":"stdout","text":"============================================================\nGPU Power Monitoring Available\n============================================================\nTesla T4, 33.85 W\nTesla T4, 11.71 W\n\n‚úì nvidia-smi is available for power monitoring\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Step 10: Summary","metadata":{}},{"cell_type":"code","source":"print(\"=\"*70)\nprint(\" \"*15 + \"GPT-2 DATASET PREPARATION COMPLETE ‚úì\")\nprint(\"=\"*70)\n\nprint(\"\\nüìÅ Files Created:\")\nimport os\ndata_dir = '/kaggle/working/gpt2_tokenized_data'\nif os.path.exists(data_dir):\n    for f in os.listdir(data_dir):\n        fpath = os.path.join(data_dir, f)\n        size = os.path.getsize(fpath) / 1024\n        print(f\"  - {f:25s} {size:>8.1f} KB\")\n\nprint(\"\\n‚úì Accomplished:\")\nprint(f\"  ‚Ä¢ Created pre-tokenized WikiText-2 dataset ({len(dataset)} samples)\")\nprint(\"  ‚Ä¢ Verified zero I/O during iteration\")\nprint(\"  ‚Ä¢ Tested with GPT-2 Small FP32 model\")\nprint(f\"  ‚Ä¢ Baseline perplexity: {perplexity:.2f}\")\nprint(\"  ‚Ä¢ Dataset on GPU (zero transfer cost during inference)\")\nprint(\"  ‚Ä¢ Ready for energy measurement\")\nprint(\"  ‚Ä¢ MUCH MORE DATA than DistilBERT version (1000 vs 50 samples!)\")\n\nprint(\"\\nüìä Key Metrics (FP32 Baseline):\")\nprint(f\"  ‚Ä¢ Perplexity:  {perplexity:.2f}\")\nprint(f\"  ‚Ä¢ Latency:     {latency:.3f} s\")\nprint(f\"  ‚Ä¢ Throughput:  {throughput:.2f} samples/s\")\nprint(f\"  ‚Ä¢ Tokens/sec:  {tokens_per_sec:.2f}\")\nprint(f\"  ‚Ä¢ Device:      {device}\")\n\nprint(\"\\nüéØ Next Steps:\")\nprint(\"  1. Use this dataset in final_quantization_benchmark_GPT2.ipynb\")\nprint(\"  2. Benchmark FP32, FP16, and Mixed Precision\")\nprint(\"  3. Measure energy consumption for each format\")\n\nprint(\"\\n‚ö° Critical Achievement:\")\nprint(\"  ZERO I/O during inference measurement!\")\nprint(f\"  {len(dataset)} samples = statistically significant results!\")\n\nprint(\"\\n\" + \"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T01:22:52.069998Z","iopub.execute_input":"2025-12-04T01:22:52.070240Z","iopub.status.idle":"2025-12-04T01:22:52.077993Z","shell.execute_reply.started":"2025-12-04T01:22:52.070214Z","shell.execute_reply":"2025-12-04T01:22:52.077240Z"}},"outputs":[{"name":"stdout","text":"======================================================================\n               GPT-2 DATASET PREPARATION COMPLETE ‚úì\n======================================================================\n\nüìÅ Files Created:\n  - metadata.json                  0.2 KB\n  - attention_mask.pt           1941.2 KB\n  - labels.pt                   1941.1 KB\n  - input_ids.pt                1941.2 KB\n\n‚úì Accomplished:\n  ‚Ä¢ Created pre-tokenized WikiText-2 dataset (1940 samples)\n  ‚Ä¢ Verified zero I/O during iteration\n  ‚Ä¢ Tested with GPT-2 Small FP32 model\n  ‚Ä¢ Baseline perplexity: 217.57\n  ‚Ä¢ Dataset on GPU (zero transfer cost during inference)\n  ‚Ä¢ Ready for energy measurement\n  ‚Ä¢ MUCH MORE DATA than DistilBERT version (1000 vs 50 samples!)\n\nüìä Key Metrics (FP32 Baseline):\n  ‚Ä¢ Perplexity:  217.57\n  ‚Ä¢ Latency:     21.943 s\n  ‚Ä¢ Throughput:  88.41 samples/s\n  ‚Ä¢ Tokens/sec:  9019.46\n  ‚Ä¢ Device:      cuda\n\nüéØ Next Steps:\n  1. Use this dataset in final_quantization_benchmark_GPT2.ipynb\n  2. Benchmark FP32, FP16, and Mixed Precision\n  3. Measure energy consumption for each format\n\n‚ö° Critical Achievement:\n  ZERO I/O during inference measurement!\n  1940 samples = statistically significant results!\n\n======================================================================\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## Optional: Create Dataset Archive","metadata":{}},{"cell_type":"code","source":"# Create a zip file for easy download/sharing\n!zip -r gpt2_tokenized_data.zip /kaggle/working/gpt2_tokenized_data/\n\nprint(\"\\n‚úì Dataset archived to gpt2_tokenized_data.zip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T01:22:52.078798Z","iopub.execute_input":"2025-12-04T01:22:52.079075Z","iopub.status.idle":"2025-12-04T01:22:52.435711Z","shell.execute_reply.started":"2025-12-04T01:22:52.079047Z","shell.execute_reply":"2025-12-04T01:22:52.434942Z"}},"outputs":[{"name":"stdout","text":"updating: kaggle/working/gpt2_tokenized_data/ (stored 0%)\nupdating: kaggle/working/gpt2_tokenized_data/metadata.json (deflated 29%)\nupdating: kaggle/working/gpt2_tokenized_data/attention_mask.pt (deflated 100%)\nupdating: kaggle/working/gpt2_tokenized_data/labels.pt (deflated 80%)\nupdating: kaggle/working/gpt2_tokenized_data/input_ids.pt (deflated 80%)\n\n‚úì Dataset archived to gpt2_tokenized_data.zip\n","output_type":"stream"}],"execution_count":22}]}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per-Layer Energy Profiling for DistilBERT\n",
    "## ESE 5390 Final Project: Layer-wise Energy Analysis\n",
    "\n",
    "This notebook analyzes the energy consumption of each layer in DistilBERT (FP32 baseline).\n",
    "\n",
    "**Goal**: Identify which layers consume the most energy to guide selective quantization strategies.\n",
    "\n",
    "**Approach**:\n",
    "1. Use PyTorch hooks to measure execution time per layer\n",
    "2. Monitor GPU power during inference\n",
    "3. Compute energy consumption per layer\n",
    "4. Visualize energy hotspots in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Auto-detect dataset path\nimport os\n\ncwd = os.getcwd()\nprint(f\"Current working directory: {cwd}\")\n\n# Check if we need to clone the repo (for Kaggle)\nrepo_path = Path(cwd) / \"energy_aware_quantization\"\nif not repo_path.exists() and \"kaggle\" in cwd.lower():\n    print(\"\\nâš™ï¸  Cloning repository (Kaggle environment detected)...\")\n    import subprocess\n    result = subprocess.run(\n        [\"git\", \"clone\", \"https://github.com/krishkc5/energy_aware_quantization.git\"],\n        cwd=cwd,\n        capture_output=True,\n        text=True\n    )\n    if result.returncode == 0:\n        print(\"âœ“ Repository cloned successfully\")\n    else:\n        print(f\"Warning: Could not clone repository: {result.stderr}\")\n\npossible_paths = [\n    Path(cwd) / \"energy_aware_quantization\" / \"datasets\" / \"tokenized_data\",  # Kaggle after clone\n    Path(cwd) / \"..\" / \"datasets\" / \"tokenized_data\",  # From notebooks/\n    Path(cwd) / \"datasets\" / \"tokenized_data\",          # From repo root\n    Path(cwd) / \"..\" / \"..\" / \"datasets\" / \"tokenized_data\",  # From deeper nesting\n]\n\ndataset_path = None\nfor path in possible_paths:\n    abs_path = path.resolve()\n    print(f\"Trying: {abs_path}\")\n    if abs_path.exists() and (abs_path / \"input_ids.pt\").exists():\n        dataset_path = str(abs_path)\n        break\n\nif dataset_path is None:\n    current = Path(cwd)\n    for _ in range(5):\n        test_path = current / \"datasets\" / \"tokenized_data\"\n        print(f\"Trying: {test_path.resolve()}\")\n        if test_path.exists() and (test_path / \"input_ids.pt\").exists():\n            dataset_path = str(test_path.resolve())\n            break\n        current = current.parent\n\nif dataset_path is None:\n    raise FileNotFoundError(\n        f\"Could not find dataset. Tried:\\n\" + \n        \"\\n\".join([f\"  - {p.resolve()}\" for p in possible_paths]) +\n        f\"\\n\\nCurrent directory: {cwd}\\n\" +\n        \"Please ensure the dataset exists or the repository is cloned.\"\n    )\n\nprint(f\"\\nâœ“ Found dataset at: {dataset_path}\")\n\n# Load dataset\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndataset_path = Path(dataset_path)\n\ninput_ids = torch.load(dataset_path / \"input_ids.pt\", map_location=device)\nattention_mask = torch.load(dataset_path / \"attention_mask.pt\", map_location=device)\nlabels = torch.load(dataset_path / \"labels.pt\", map_location=device)\n\nprint(f\"\\nâœ“ Loaded dataset:\")\nprint(f\"  - Samples: {input_ids.shape[0]}\")\nprint(f\"  - Sequence length: {input_ids.shape[1]}\")\nprint(f\"  - Device: {input_ids.device}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load DistilBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\nâœ“ Model loaded\")\n",
    "print(f\"  - Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  - Device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Print model architecture\n",
    "print(f\"\\nðŸ“‹ Model Architecture:\")\n",
    "for name, module in model.named_children():\n",
    "    print(f\"  - {name}: {module.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Layer Profiler with Timing Hooks\n",
    "\n",
    "We'll use PyTorch forward hooks to measure execution time for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerProfiler:\n",
    "    \"\"\"\n",
    "    Profile execution time of each layer using forward hooks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, device: str = \"cuda\"):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.hooks = []\n",
    "        self.layer_times = defaultdict(list)\n",
    "        self.layer_names = []\n",
    "        self.current_forward_start = None\n",
    "        \n",
    "        # Register hooks for all named modules\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        \"\"\"Register forward pre/post hooks on all layers.\"\"\"\n",
    "        \n",
    "        def make_pre_hook(name):\n",
    "            def pre_hook(module, input):\n",
    "                if self.device == \"cuda\":\n",
    "                    torch.cuda.synchronize()\n",
    "                self.layer_times[name + \"_start\"].append(time.perf_counter())\n",
    "            return pre_hook\n",
    "        \n",
    "        def make_post_hook(name):\n",
    "            def post_hook(module, input, output):\n",
    "                if self.device == \"cuda\":\n",
    "                    torch.cuda.synchronize()\n",
    "                self.layer_times[name + \"_end\"].append(time.perf_counter())\n",
    "            return post_hook\n",
    "        \n",
    "        # Register hooks on important layers\n",
    "        for name, module in self.model.named_modules():\n",
    "            # Skip container modules and focus on actual computation layers\n",
    "            if len(list(module.children())) == 0:  # Leaf modules only\n",
    "                if isinstance(module, (nn.Linear, nn.LayerNorm, nn.Dropout, nn.GELU, nn.Embedding)):\n",
    "                    self.layer_names.append(name)\n",
    "                    hook_pre = module.register_forward_pre_hook(make_pre_hook(name))\n",
    "                    hook_post = module.register_forward_hook(make_post_hook(name))\n",
    "                    self.hooks.append(hook_pre)\n",
    "                    self.hooks.append(hook_post)\n",
    "        \n",
    "        print(f\"âœ“ Registered hooks on {len(self.layer_names)} layers\")\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset timing statistics.\"\"\"\n",
    "        self.layer_times.clear()\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Remove all hooks.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks.clear()\n",
    "    \n",
    "    def get_layer_times(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute average execution time per layer.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping layer name to average time in seconds\n",
    "        \"\"\"\n",
    "        layer_avg_times = {}\n",
    "        \n",
    "        for name in self.layer_names:\n",
    "            start_times = self.layer_times.get(name + \"_start\", [])\n",
    "            end_times = self.layer_times.get(name + \"_end\", [])\n",
    "            \n",
    "            if len(start_times) == len(end_times) and len(start_times) > 0:\n",
    "                durations = [end - start for start, end in zip(start_times, end_times)]\n",
    "                layer_avg_times[name] = np.mean(durations)\n",
    "        \n",
    "        return layer_avg_times\n",
    "    \n",
    "    def get_layer_stats(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get detailed statistics for each layer.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with layer statistics\n",
    "        \"\"\"\n",
    "        stats = []\n",
    "        \n",
    "        for name in self.layer_names:\n",
    "            start_times = self.layer_times.get(name + \"_start\", [])\n",
    "            end_times = self.layer_times.get(name + \"_end\", [])\n",
    "            \n",
    "            if len(start_times) == len(end_times) and len(start_times) > 0:\n",
    "                durations = [end - start for start, end in zip(start_times, end_times)]\n",
    "                \n",
    "                stats.append({\n",
    "                    \"layer_name\": name,\n",
    "                    \"mean_time_ms\": np.mean(durations) * 1000,\n",
    "                    \"std_time_ms\": np.std(durations) * 1000,\n",
    "                    \"min_time_ms\": np.min(durations) * 1000,\n",
    "                    \"max_time_ms\": np.max(durations) * 1000,\n",
    "                    \"total_time_s\": np.sum(durations),\n",
    "                    \"num_calls\": len(durations)\n",
    "                })\n",
    "        \n",
    "        df = pd.DataFrame(stats)\n",
    "        \n",
    "        # Add percentage of total time\n",
    "        if len(df) > 0:\n",
    "            total_time = df[\"total_time_s\"].sum()\n",
    "            df[\"percent_total\"] = (df[\"total_time_s\"] / total_time) * 100\n",
    "        \n",
    "        return df.sort_values(\"total_time_s\", ascending=False)\n",
    "\n",
    "\n",
    "# Create profiler\n",
    "profiler = LayerProfiler(model, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Power Monitor\n",
    "\n",
    "Simple power monitoring using nvidia-smi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePowerMonitor:\n",
    "    \"\"\"\n",
    "    Simple power monitor using nvidia-smi.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, interval_ms: int = 50, gpu_id: int = 0):\n",
    "        self.interval_ms = interval_ms\n",
    "        self.gpu_id = gpu_id\n",
    "        self.samples = []\n",
    "        self.is_running = False\n",
    "        self._thread = None\n",
    "        self._lock = threading.Lock()\n",
    "    \n",
    "    def _poll(self):\n",
    "        \"\"\"Poll nvidia-smi for power readings.\"\"\"\n",
    "        interval_sec = self.interval_ms / 1000.0\n",
    "        \n",
    "        while self.is_running:\n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [\"nvidia-smi\", \"--query-gpu=power.draw\", \"--format=csv,noheader,nounits\", f\"--id={self.gpu_id}\"],\n",
    "                    capture_output=True, text=True, timeout=2\n",
    "                )\n",
    "                if result.returncode == 0:\n",
    "                    power = float(result.stdout.strip())\n",
    "                    with self._lock:\n",
    "                        self.samples.append((time.perf_counter(), power))\n",
    "            except:\n",
    "                pass\n",
    "            time.sleep(interval_sec)\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"Start monitoring.\"\"\"\n",
    "        with self._lock:\n",
    "            self.samples = []\n",
    "            self.is_running = True\n",
    "        \n",
    "        self._thread = threading.Thread(target=self._poll, daemon=True)\n",
    "        self._thread.start()\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop monitoring.\"\"\"\n",
    "        self.is_running = False\n",
    "        if self._thread:\n",
    "            self._thread.join(timeout=2)\n",
    "    \n",
    "    def get_samples(self) -> List[Tuple[float, float]]:\n",
    "        \"\"\"Get (timestamp, power) samples.\"\"\"\n",
    "        with self._lock:\n",
    "            return self.samples.copy()\n",
    "    \n",
    "    def get_mean_power(self) -> float:\n",
    "        \"\"\"Get mean power in Watts.\"\"\"\n",
    "        samples = self.get_samples()\n",
    "        if len(samples) == 0:\n",
    "            return 0.0\n",
    "        return np.mean([p for _, p in samples])\n",
    "\n",
    "\n",
    "power_monitor = SimplePowerMonitor(interval_ms=50)\n",
    "print(\"âœ“ Power monitor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Profiling Inference\n",
    "\n",
    "Run inference with layer profiling and power monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup\n",
    "print(\"Warming up GPU...\")\n",
    "with torch.no_grad():\n",
    "    for _ in range(50):\n",
    "        _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "print(\"âœ“ Warmup complete\\n\")\n",
    "\n",
    "# Reset profiler\n",
    "profiler.reset()\n",
    "\n",
    "# Run profiling with power monitoring\n",
    "num_iters = 100\n",
    "print(f\"Running {num_iters} profiling iterations...\")\n",
    "\n",
    "power_monitor.start()\n",
    "time.sleep(0.3)  # Let power monitor stabilize\n",
    "\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_iters):\n",
    "        _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"  Progress: {i+1}/{num_iters}\", end='\\r')\n",
    "\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "time.sleep(0.3)  # Capture trailing power samples\n",
    "power_monitor.stop()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "mean_power = power_monitor.get_mean_power()\n",
    "\n",
    "print(f\"\\n\\nâœ“ Profiling complete\")\n",
    "print(f\"  - Total time: {total_time:.3f}s\")\n",
    "print(f\"  - Mean power: {mean_power:.2f}W\")\n",
    "print(f\"  - Total energy: {mean_power * total_time:.3f}J\")\n",
    "print(f\"  - Power samples: {len(power_monitor.get_samples())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Per-Layer Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get layer statistics\n",
    "layer_stats = profiler.get_layer_stats()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PER-LAYER TIMING STATISTICS (Top 20)\")\n",
    "print(\"=\"*80)\n",
    "print(layer_stats.head(20).to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save to CSV\n",
    "output_dir = Path(\"../results\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "layer_stats.to_csv(output_dir / \"per_layer_timing_fp32.csv\", index=False)\n",
    "print(f\"\\nâœ“ Saved layer timing to: {output_dir / 'per_layer_timing_fp32.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compute Per-Layer Energy Consumption\n",
    "\n",
    "Estimate energy per layer using:\n",
    "- Layer execution time (from profiling)\n",
    "- Mean GPU power (from power monitor)\n",
    "\n",
    "**Energy per layer = (Layer time / Total time) Ã— Total energy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-layer energy\n",
    "total_energy = mean_power * total_time  # Joules\n",
    "total_measured_time = layer_stats[\"total_time_s\"].sum()\n",
    "\n",
    "layer_stats[\"energy_j\"] = (layer_stats[\"total_time_s\"] / total_measured_time) * total_energy\n",
    "layer_stats[\"energy_mj\"] = layer_stats[\"energy_j\"] * 1000\n",
    "layer_stats[\"energy_per_call_mj\"] = layer_stats[\"energy_mj\"] / layer_stats[\"num_calls\"]\n",
    "\n",
    "# Sort by energy consumption\n",
    "layer_stats = layer_stats.sort_values(\"energy_j\", ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"PER-LAYER ENERGY CONSUMPTION (Top 20)\")\n",
    "print(\"=\"*100)\n",
    "print(layer_stats[[\"layer_name\", \"mean_time_ms\", \"energy_mj\", \"energy_per_call_mj\", \"percent_total\"]].head(20).to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save to CSV\n",
    "layer_stats.to_csv(output_dir / \"per_layer_energy_fp32.csv\", index=False)\n",
    "print(f\"\\nâœ“ Saved layer energy to: {output_dir / 'per_layer_energy_fp32.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Group Energy by Layer Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract layer type from name\n",
    "def get_layer_type(name: str) -> str:\n",
    "    \"\"\"Extract layer type from full layer name.\"\"\"\n",
    "    if \"attention.q_lin\" in name or \"attention.k_lin\" in name or \"attention.v_lin\" in name:\n",
    "        return \"Attention Projection\"\n",
    "    elif \"attention.out_lin\" in name:\n",
    "        return \"Attention Output\"\n",
    "    elif \"ffn.lin1\" in name:\n",
    "        return \"FFN Layer 1\"\n",
    "    elif \"ffn.lin2\" in name:\n",
    "        return \"FFN Layer 2\"\n",
    "    elif \"sa_layer_norm\" in name or \"output_layer_norm\" in name:\n",
    "        return \"LayerNorm\"\n",
    "    elif \"embeddings\" in name:\n",
    "        return \"Embeddings\"\n",
    "    elif \"pre_classifier\" in name or \"classifier\" in name:\n",
    "        return \"Classifier\"\n",
    "    elif \"dropout\" in name:\n",
    "        return \"Dropout\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "layer_stats[\"layer_type\"] = layer_stats[\"layer_name\"].apply(get_layer_type)\n",
    "\n",
    "# Group by layer type\n",
    "type_energy = layer_stats.groupby(\"layer_type\").agg({\n",
    "    \"energy_j\": \"sum\",\n",
    "    \"energy_mj\": \"sum\",\n",
    "    \"total_time_s\": \"sum\",\n",
    "    \"layer_name\": \"count\"\n",
    "}).rename(columns={\"layer_name\": \"num_layers\"})\n",
    "\n",
    "type_energy[\"percent_energy\"] = (type_energy[\"energy_j\"] / type_energy[\"energy_j\"].sum()) * 100\n",
    "type_energy = type_energy.sort_values(\"energy_j\", ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENERGY CONSUMPTION BY LAYER TYPE\")\n",
    "print(\"=\"*80)\n",
    "print(type_energy.to_string())\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save to CSV\n",
    "type_energy.to_csv(output_dir / \"energy_by_layer_type_fp32.csv\")\n",
    "print(f\"\\nâœ“ Saved grouped energy to: {output_dir / 'energy_by_layer_type_fp32.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle(\"DistilBERT Per-Layer Energy Analysis (FP32)\", fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Top 15 layers by energy\n",
    "ax = axes[0, 0]\n",
    "top_layers = layer_stats.head(15)\n",
    "y_pos = range(len(top_layers))\n",
    "ax.barh(y_pos, top_layers[\"energy_mj\"], alpha=0.7)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels([name.split('.')[-1] if len(name.split('.')) > 1 else name for name in top_layers[\"layer_name\"]], fontsize=8)\n",
    "ax.set_xlabel(\"Energy (mJ)\", fontsize=10)\n",
    "ax.set_title(\"Top 15 Layers by Energy Consumption\", fontsize=12, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 2. Energy by layer type\n",
    "ax = axes[0, 1]\n",
    "type_energy_sorted = type_energy.sort_values(\"energy_mj\", ascending=True)\n",
    "y_pos = range(len(type_energy_sorted))\n",
    "colors = plt.cm.Set3(range(len(type_energy_sorted)))\n",
    "ax.barh(y_pos, type_energy_sorted[\"energy_mj\"], alpha=0.7, color=colors)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(type_energy_sorted.index, fontsize=10)\n",
    "ax.set_xlabel(\"Energy (mJ)\", fontsize=10)\n",
    "ax.set_title(\"Energy Consumption by Layer Type\", fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 3. Pie chart of energy distribution\n",
    "ax = axes[1, 0]\n",
    "type_energy_pie = type_energy[type_energy[\"percent_energy\"] > 1.0]  # Show only >1%\n",
    "ax.pie(type_energy_pie[\"percent_energy\"], labels=type_energy_pie.index, autopct='%1.1f%%', startangle=90)\n",
    "ax.set_title(\"Energy Distribution by Layer Type\", fontsize=12, fontweight='bold')\n",
    "\n",
    "# 4. Cumulative energy contribution\n",
    "ax = axes[1, 1]\n",
    "cumulative = layer_stats[\"energy_mj\"].cumsum() / layer_stats[\"energy_mj\"].sum() * 100\n",
    "ax.plot(range(len(cumulative)), cumulative, linewidth=2)\n",
    "ax.axhline(y=80, color='r', linestyle='--', alpha=0.7, label='80% threshold')\n",
    "ax.axhline(y=90, color='orange', linestyle='--', alpha=0.7, label='90% threshold')\n",
    "ax.set_xlabel(\"Number of Layers\", fontsize=10)\n",
    "ax.set_ylabel(\"Cumulative Energy (%)\", fontsize=10)\n",
    "ax.set_title(\"Cumulative Energy Contribution\", fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = output_dir / \"per_layer_energy_analysis_fp32.png\"\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nâœ“ Saved plots to: {plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key Insights\n",
    "\n",
    "Identify the most energy-hungry layers for targeted quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find how many layers account for 80% of energy\n",
    "cumulative_energy = layer_stats[\"energy_j\"].cumsum() / layer_stats[\"energy_j\"].sum()\n",
    "layers_for_80_pct = (cumulative_energy <= 0.80).sum()\n",
    "layers_for_90_pct = (cumulative_energy <= 0.90).sum()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ“Š Energy Distribution:\")\n",
    "print(f\"  - Total layers profiled: {len(layer_stats)}\")\n",
    "print(f\"  - Layers accounting for 80% of energy: {layers_for_80_pct} ({layers_for_80_pct/len(layer_stats)*100:.1f}%)\")\n",
    "print(f\"  - Layers accounting for 90% of energy: {layers_for_90_pct} ({layers_for_90_pct/len(layer_stats)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nðŸ”¥ Top Energy Consumers:\")\n",
    "top_3 = layer_stats.head(3)\n",
    "for i, row in top_3.iterrows():\n",
    "    print(f\"  {row['layer_name']}:\")\n",
    "    print(f\"    - Energy: {row['energy_mj']:.2f} mJ ({row['percent_total']:.1f}% of total)\")\n",
    "    print(f\"    - Avg time: {row['mean_time_ms']:.3f} ms per call\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Quantization Strategy Recommendations:\")\n",
    "print(f\"  1. Prioritize quantizing: {type_energy.head(3).index.tolist()}\")\n",
    "print(f\"  2. These layer types account for {type_energy.head(3)['percent_energy'].sum():.1f}% of total energy\")\n",
    "print(f\"  3. Consider keeping embeddings and LayerNorm in higher precision\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary_report = {\n",
    "    \"model_name\": model_name,\n",
    "    \"precision\": \"fp32\",\n",
    "    \"num_profiling_iterations\": num_iters,\n",
    "    \"total_time_s\": float(total_time),\n",
    "    \"mean_power_w\": float(mean_power),\n",
    "    \"total_energy_j\": float(total_energy),\n",
    "    \"num_layers_profiled\": len(layer_stats),\n",
    "    \"layers_for_80pct_energy\": int(layers_for_80_pct),\n",
    "    \"layers_for_90pct_energy\": int(layers_for_90_pct),\n",
    "    \"top_energy_layer\": layer_stats.iloc[0][\"layer_name\"],\n",
    "    \"top_energy_layer_mj\": float(layer_stats.iloc[0][\"energy_mj\"]),\n",
    "    \"top_energy_layer_percent\": float(layer_stats.iloc[0][\"percent_total\"]),\n",
    "    \"energy_by_type\": type_energy.to_dict(),\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "report_path = output_dir / \"per_layer_energy_summary_fp32.json\"\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Saved summary report to: {report_path}\")\n",
    "\n",
    "# Clean up\n",
    "profiler.remove_hooks()\n",
    "print(\"\\nâœ“ Profiling complete and hooks removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides:\n",
    "1. **Per-layer timing** using PyTorch forward hooks\n",
    "2. **Per-layer energy estimates** based on timing and power monitoring\n",
    "3. **Energy breakdown by layer type** (Attention, FFN, LayerNorm, etc.)\n",
    "4. **Visualizations** showing energy hotspots\n",
    "5. **Quantization recommendations** based on energy analysis\n",
    "\n",
    "### Key Takeaways:\n",
    "- Identifies which layers consume the most energy\n",
    "- Shows cumulative energy contribution\n",
    "- Guides selective quantization strategies\n",
    "- Helps prioritize optimization efforts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Energy-Aware Quantization Measurement Harness\n## ESE 5390 Final Project: LLM Quantization Energy Measurement\n\nThis notebook implements a complete measurement harness for measuring energy consumption, latency, and accuracy across different precision levels (FP32, FP16, INT8) for transformer models.","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/krishkc5/energy_aware_quantization.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T06:27:21.982444Z","iopub.execute_input":"2025-12-03T06:27:21.982645Z","iopub.status.idle":"2025-12-03T06:27:22.965148Z","shell.execute_reply.started":"2025-12-03T06:27:21.982628Z","shell.execute_reply":"2025-12-03T06:27:22.964192Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'energy_aware_quantization'...\nremote: Enumerating objects: 118, done.\u001b[K\nremote: Counting objects: 100% (118/118), done.\u001b[K\nremote: Compressing objects: 100% (93/93), done.\u001b[K\nremote: Total 118 (delta 48), reused 77 (delta 21), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (118/118), 314.03 KiB | 4.30 MiB/s, done.\nResolving deltas: 100% (48/48), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## 0. Setup and Imports","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport numpy as np\nimport pandas as pd\nimport json\nimport time\nimport subprocess\nimport threading\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T06:27:44.429849Z","iopub.execute_input":"2025-12-03T06:27:44.430523Z","iopub.status.idle":"2025-12-03T06:27:55.937157Z","shell.execute_reply.started":"2025-12-03T06:27:44.430491Z","shell.execute_reply":"2025-12-03T06:27:55.936381Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.6.0+cu124\nCUDA available: True\nCUDA device: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## 1. Configuration Management","metadata":{}},{"cell_type":"code","source":"def create_default_config() -> Dict:\n    \"\"\"\n    Create default experiment configuration.\n    Modify these values as needed for your experiments.\n    \"\"\"\n    config = {\n        \"model_name\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n        \"precision\": \"fp32\",  # Will be overridden per experiment\n        \"batch_size\": 8,\n        \"seq_len\": 128,\n        \"num_loops\": 300,  # Number of measurement iterations\n        \"warmup_loops\": 50,  # Number of warmup iterations\n        \"dataset_path\": \"./data/pre_tokenized.pt\",  # Path to pre-tokenized dataset\n        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n        \"num_trials\": 5,  # Number of trials per precision level\n        \"power_poll_interval_ms\": 100,  # Power sampling interval\n        \"results_dir\": \"./results\"\n    }\n    return config\n\ndef load_config(config_path: Optional[str] = None) -> Dict:\n    \"\"\"\n    Load configuration from JSON file or use defaults.\n    \"\"\"\n    if config_path and Path(config_path).exists():\n        with open(config_path, 'r') as f:\n            config = json.load(f)\n        print(f\"Loaded config from {config_path}\")\n    else:\n        config = create_default_config()\n        print(\"Using default configuration\")\n    \n    # Create results directory if it doesn't exist\n    Path(config[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n    \n    return config\n\ndef save_config(config: Dict, path: str):\n    \"\"\"\n    Save configuration to JSON file.\n    \"\"\"\n    with open(path, 'w') as f:\n        json.dump(config, f, indent=2)\n    print(f\"Config saved to {path}\")\n\n# Load configuration\nconfig = load_config()\nprint(\"\\nConfiguration:\")\nprint(json.dumps(config, indent=2))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Dataset Loading (Zero-IO During Measurement)","metadata":{}},{"cell_type":"code","source":"def load_pre_tokenized_dataset(dataset_path: str, device: str) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Load pre-tokenized dataset and move to GPU.\n    This ensures zero IO during measurement loops.\n    \n    Expected format: dict with keys 'input_ids', 'attention_mask', 'labels'\n    \"\"\"\n    print(f\"Loading dataset from {dataset_path}...\")\n    data = torch.load(dataset_path)\n    \n    input_ids = data[\"input_ids\"].to(device)\n    attention_mask = data[\"attention_mask\"].to(device)\n    labels = data[\"labels\"].to(device)\n    \n    print(f\"Dataset loaded: {input_ids.shape[0]} samples\")\n    print(f\"Sequence length: {input_ids.shape[1]}\")\n    print(f\"Memory on device: {input_ids.element_size() * input_ids.nelement() / 1024**2:.2f} MB\")\n    \n    return input_ids, attention_mask, labels\n\ndef batched_iterator(input_ids: torch.Tensor, attention_mask: torch.Tensor, batch_size: int):\n    \"\"\"\n    Create a cycling batch iterator with no IO.\n    Wraps around when reaching the end of the dataset.\n    \"\"\"\n    N = input_ids.size(0)\n    idx = 0\n    \n    while True:\n        end_idx = idx + batch_size\n        \n        # Get batch\n        if end_idx <= N:\n            batch_ids = input_ids[idx:end_idx]\n            batch_mask = attention_mask[idx:end_idx]\n        else:\n            # Wrap around if needed\n            idx = 0\n            batch_ids = input_ids[idx:idx+batch_size]\n            batch_mask = attention_mask[idx:idx+batch_size]\n        \n        # Ensure we have full batch\n        if batch_ids.size(0) < batch_size:\n            idx = 0\n            continue\n        \n        yield batch_ids, batch_mask\n        idx = (idx + batch_size) % N","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Model Loading Per Precision","metadata":{}},{"cell_type":"code","source":"def load_model(model_name: str, precision: str, device: str):\n    \"\"\"\n    Load model with specified precision level.\n    \n    Args:\n        model_name: HuggingFace model identifier\n        precision: 'fp32', 'fp16', or 'int8'\n        device: 'cuda' or 'cpu'\n    \n    Returns:\n        model: Loaded model in eval mode\n    \"\"\"\n    print(f\"\\nLoading model: {model_name} with precision: {precision}\")\n    \n    # Load base model\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_name,\n        torch_dtype=torch.float32  # Start with FP32\n    )\n    \n    model.to(device)\n    model.eval()\n    \n    # Apply precision conversion\n    if precision == \"fp16\":\n        print(\"Converting to FP16...\")\n        model = model.half()\n    \n    elif precision == \"int8\":\n        print(\"Applying INT8 quantization...\")\n        # Post-training dynamic quantization\n        if device == \"cpu\":\n            model = torch.quantization.quantize_dynamic(\n                model,\n                {nn.Linear},  # Quantize linear layers\n                dtype=torch.qint8\n            )\n        else:\n            # For GPU, you might use different quantization methods\n            # This is a placeholder - adjust based on your quantization approach\n            print(\"Note: INT8 quantization on GPU requires specific libraries (e.g., bitsandbytes)\")\n            print(\"Using FP16 as fallback for GPU INT8...\")\n            model = model.half()\n    \n    elif precision != \"fp32\":\n        raise ValueError(f\"Unknown precision: {precision}\")\n    \n    # Calculate model size\n    param_size = sum(p.nelement() * p.element_size() for p in model.parameters())\n    buffer_size = sum(b.nelement() * b.element_size() for b in model.buffers())\n    model_size_mb = (param_size + buffer_size) / 1024**2\n    \n    print(f\"Model size: {model_size_mb:.2f} MB\")\n    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n    \n    return model\n\ndef get_model_info(model) -> Dict:\n    \"\"\"\n    Get detailed model information.\n    \"\"\"\n    param_size = sum(p.nelement() * p.element_size() for p in model.parameters())\n    buffer_size = sum(b.nelement() * b.element_size() for b in model.buffers())\n    \n    return {\n        \"model_size_mb\": (param_size + buffer_size) / 1024**2,\n        \"num_parameters\": sum(p.numel() for p in model.parameters()),\n        \"num_trainable_params\": sum(p.numel() for p in model.parameters() if p.requires_grad)\n    }","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Warmup Phase","metadata":{}},{"cell_type":"code","source":"def warmup(model, batch_iter, num_iters: int, device: str):\n    \"\"\"\n    Warmup phase to stabilize GPU clocks and compile kernels.\n    No timing or power logging during this phase.\n    \"\"\"\n    print(f\"Running warmup for {num_iters} iterations...\")\n    \n    with torch.no_grad():\n        for i in range(num_iters):\n            input_ids, attention_mask = next(batch_iter)\n            _ = model(input_ids, attention_mask=attention_mask)\n            \n            if i % 10 == 0:\n                print(f\"  Warmup iteration {i}/{num_iters}\", end='\\r')\n    \n    if device == \"cuda\":\n        torch.cuda.synchronize()\n    \n    print(f\"\\nWarmup complete.\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Power Logger","metadata":{}},{"cell_type":"code","source":"class PowerLogger:\n    \"\"\"\n    Power logger using nvidia-smi to sample GPU power consumption.\n    \"\"\"\n    \n    def __init__(self, poll_interval_ms: int = 100, device_id: int = 0):\n        \"\"\"\n        Args:\n            poll_interval_ms: Sampling interval in milliseconds\n            device_id: GPU device ID to monitor\n        \"\"\"\n        self.poll_interval_ms = poll_interval_ms\n        self.device_id = device_id\n        self.proc = None\n        self.samples = []\n        self.reader_thread = None\n        self.stop_flag = threading.Event()\n    \n    def start(self):\n        \"\"\"\n        Start power logging subprocess.\n        \"\"\"\n        print(\"Starting power logger...\")\n        self.samples = []\n        self.stop_flag.clear()\n        \n        # nvidia-smi command for power monitoring\n        # Adjust this command based on your environment (especially Kaggle)\n        cmd = [\n            \"nvidia-smi\",\n            \"--query-gpu=power.draw\",\n            \"--format=csv,noheader,nounits\",\n            f\"--loop-ms={self.poll_interval_ms}\",\n            f\"--id={self.device_id}\"\n        ]\n        \n        try:\n            self.proc = subprocess.Popen(\n                cmd,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                universal_newlines=True,\n                bufsize=1\n            )\n            \n            # Start reader thread\n            self.reader_thread = threading.Thread(target=self._read_output)\n            self.reader_thread.start()\n            \n            # Give it a moment to start\n            time.sleep(0.2)\n            \n        except Exception as e:\n            print(f\"Warning: Could not start nvidia-smi: {e}\")\n            print(\"Power measurements will not be available.\")\n            self.proc = None\n    \n    def _read_output(self):\n        \"\"\"\n        Read power samples from subprocess in background thread.\n        \"\"\"\n        if self.proc is None:\n            return\n        \n        while not self.stop_flag.is_set():\n            line = self.proc.stdout.readline()\n            if not line:\n                break\n            \n            try:\n                power = float(line.strip())\n                self.samples.append(power)\n            except ValueError:\n                continue\n    \n    def stop(self):\n        \"\"\"\n        Stop power logging and collect samples.\n        \"\"\"\n        print(\"Stopping power logger...\")\n        self.stop_flag.set()\n        \n        if self.proc:\n            self.proc.terminate()\n            try:\n                self.proc.wait(timeout=2)\n            except subprocess.TimeoutExpired:\n                self.proc.kill()\n        \n        if self.reader_thread:\n            self.reader_thread.join(timeout=2)\n        \n        print(f\"Collected {len(self.samples)} power samples\")\n    \n    def get_samples(self) -> List[float]:\n        \"\"\"\n        Return collected power samples in watts.\n        \"\"\"\n        return self.samples.copy()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Timed Inference Loop","metadata":{}},{"cell_type":"code","source":"def run_inference_loop(model, batch_iter, num_loops: int, device: str) -> float:\n    \"\"\"\n    Run timed inference loop without power logging.\n    \n    Args:\n        model: Model to run inference on\n        batch_iter: Batch iterator\n        num_loops: Number of inference iterations\n        device: Device type\n    \n    Returns:\n        total_time: Total time in seconds\n    \"\"\"\n    print(f\"Running {num_loops} timed inference iterations...\")\n    \n    # Synchronize before starting timer\n    if device == \"cuda\":\n        torch.cuda.synchronize()\n    \n    start = time.perf_counter()\n    \n    with torch.no_grad():\n        for i in range(num_loops):\n            input_ids, attention_mask = next(batch_iter)\n            _ = model(input_ids, attention_mask=attention_mask)\n            \n            if i % 50 == 0:\n                print(f\"  Iteration {i}/{num_loops}\", end='\\r')\n    \n    # Synchronize before stopping timer\n    if device == \"cuda\":\n        torch.cuda.synchronize()\n    \n    end = time.perf_counter()\n    total_time = end - start\n    \n    print(f\"\\nInference complete. Total time: {total_time:.3f}s\")\n    \n    return total_time","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Combined Measurement (Time + Power)","metadata":{}},{"cell_type":"code","source":"def measure_with_power(model, batch_iter, num_loops: int, device: str, \n                       poll_interval_ms: int = 100) -> Tuple[float, List[float]]:\n    \"\"\"\n    Run timed inference with simultaneous power logging.\n    \n    Returns:\n        total_time: Total inference time in seconds\n        power_samples: List of power measurements in watts\n    \"\"\"\n    logger = PowerLogger(poll_interval_ms=poll_interval_ms)\n    \n    # Start power logging\n    logger.start()\n    \n    # Small delay to ensure logger is running\n    time.sleep(0.5)\n    \n    # Run timed inference\n    total_time = run_inference_loop(model, batch_iter, num_loops, device)\n    \n    # Stop power logging\n    logger.stop()\n    \n    power_samples = logger.get_samples()\n    \n    return total_time, power_samples","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Energy and Latency Computation","metadata":{}},{"cell_type":"code","source":"def compute_energy_metrics(power_samples: List[float], total_time: float, \n                          num_inferences: int, batch_size: int) -> Dict:\n    \"\"\"\n    Compute energy and latency metrics from power samples and timing.\n    \n    Args:\n        power_samples: List of power measurements in watts\n        total_time: Total inference time in seconds\n        num_inferences: Total number of inference iterations\n        batch_size: Batch size used\n    \n    Returns:\n        Dictionary with computed metrics\n    \"\"\"\n    metrics = {}\n    \n    # Latency metrics\n    total_samples = num_inferences * batch_size\n    metrics[\"latency_per_sample_s\"] = total_time / total_samples\n    metrics[\"latency_per_sample_ms\"] = metrics[\"latency_per_sample_s\"] * 1000\n    metrics[\"latency_per_batch_s\"] = total_time / num_inferences\n    metrics[\"latency_per_batch_ms\"] = metrics[\"latency_per_batch_s\"] * 1000\n    metrics[\"throughput_samples_per_sec\"] = total_samples / total_time\n    \n    # Power and energy metrics\n    if len(power_samples) > 0:\n        metrics[\"avg_power_w\"] = float(np.mean(power_samples))\n        metrics[\"std_power_w\"] = float(np.std(power_samples))\n        metrics[\"min_power_w\"] = float(np.min(power_samples))\n        metrics[\"max_power_w\"] = float(np.max(power_samples))\n        metrics[\"num_power_samples\"] = len(power_samples)\n        \n        # Energy computation: P * t\n        metrics[\"energy_total_j\"] = metrics[\"avg_power_w\"] * total_time\n        metrics[\"energy_per_sample_j\"] = metrics[\"energy_total_j\"] / total_samples\n        metrics[\"energy_per_sample_mj\"] = metrics[\"energy_per_sample_j\"] * 1000\n        metrics[\"energy_per_batch_j\"] = metrics[\"energy_total_j\"] / num_inferences\n    else:\n        print(\"Warning: No power samples collected\")\n        metrics[\"avg_power_w\"] = None\n        metrics[\"energy_total_j\"] = None\n        metrics[\"energy_per_sample_j\"] = None\n    \n    # GPU memory metrics (if available)\n    if torch.cuda.is_available():\n        metrics[\"peak_memory_mb\"] = torch.cuda.max_memory_allocated() / 1024**2\n        metrics[\"current_memory_mb\"] = torch.cuda.memory_allocated() / 1024**2\n    \n    return metrics\n\ndef print_metrics(metrics: Dict, precision: str, trial: int):\n    \"\"\"\n    Pretty print metrics.\n    \"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"Metrics for {precision.upper()} - Trial {trial}\")\n    print(f\"{'='*60}\")\n    print(f\"Latency per sample: {metrics['latency_per_sample_ms']:.3f} ms\")\n    print(f\"Throughput: {metrics['throughput_samples_per_sec']:.2f} samples/sec\")\n    \n    if metrics['avg_power_w'] is not None:\n        print(f\"Average power: {metrics['avg_power_w']:.2f} W\")\n        print(f\"Energy per sample: {metrics['energy_per_sample_mj']:.3f} mJ\")\n        print(f\"Total energy: {metrics['energy_total_j']:.3f} J\")\n    \n    if 'peak_memory_mb' in metrics:\n        print(f\"Peak GPU memory: {metrics['peak_memory_mb']:.2f} MB\")\n    print(f\"{'='*60}\\n\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Multi-Trial Runner","metadata":{}},{"cell_type":"code","source":"def run_experiments_for_precision(config: Dict, precision: str, \n                                 num_trials: int = 5) -> List[Dict]:\n    \"\"\"\n    Run multiple trials for a given precision level.\n    \n    Args:\n        config: Experiment configuration\n        precision: Precision level ('fp32', 'fp16', 'int8')\n        num_trials: Number of trials to run\n    \n    Returns:\n        List of metric dictionaries, one per trial\n    \"\"\"\n    print(f\"\\n{'#'*60}\")\n    print(f\"# Running experiments for {precision.upper()}\")\n    print(f\"# Number of trials: {num_trials}\")\n    print(f\"{'#'*60}\\n\")\n    \n    device = config[\"device\"]\n    \n    # Load dataset once (stays on GPU)\n    print(\"Loading dataset...\")\n    input_ids, attention_mask, labels = load_pre_tokenized_dataset(\n        config[\"dataset_path\"], \n        device\n    )\n    \n    results = []\n    \n    for trial in range(num_trials):\n        print(f\"\\n{'*'*60}\")\n        print(f\"* Trial {trial + 1}/{num_trials} for {precision.upper()}\")\n        print(f\"{'*'*60}\")\n        \n        # Reset GPU memory tracking\n        if torch.cuda.is_available():\n            torch.cuda.reset_peak_memory_stats()\n            torch.cuda.empty_cache()\n        \n        # Load model for this trial\n        model = load_model(config[\"model_name\"], precision, device)\n        model_info = get_model_info(model)\n        \n        # Create batch iterator\n        batch_iter = batched_iterator(input_ids, attention_mask, config[\"batch_size\"])\n        \n        # Warmup\n        warmup(model, batch_iter, config[\"warmup_loops\"], device)\n        \n        # Measurement with power logging\n        total_time, power_samples = measure_with_power(\n            model, \n            batch_iter, \n            config[\"num_loops\"], \n            device,\n            config[\"power_poll_interval_ms\"]\n        )\n        \n        # Compute metrics\n        metrics = compute_energy_metrics(\n            power_samples,\n            total_time,\n            config[\"num_loops\"],\n            config[\"batch_size\"]\n        )\n        \n        # Add metadata\n        metrics[\"precision\"] = precision\n        metrics[\"trial\"] = trial\n        metrics[\"batch_size\"] = config[\"batch_size\"]\n        metrics[\"seq_len\"] = config[\"seq_len\"]\n        metrics[\"num_loops\"] = config[\"num_loops\"]\n        metrics[\"model_size_mb\"] = model_info[\"model_size_mb\"]\n        metrics[\"num_parameters\"] = model_info[\"num_parameters\"]\n        \n        # Print and store\n        print_metrics(metrics, precision, trial)\n        results.append(metrics)\n        \n        # Clean up\n        del model\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n    \n    return results","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 10. Aggregate Results Across Trials","metadata":{}},{"cell_type":"code","source":"def aggregate_trials(trial_results: List[Dict]) -> Dict:\n    \"\"\"\n    Aggregate metrics across multiple trials.\n    \n    Returns:\n        Dictionary with mean and std for each metric\n    \"\"\"\n    if not trial_results:\n        return {}\n    \n    # Extract numeric metrics\n    numeric_keys = [\n        \"latency_per_sample_ms\", \"latency_per_batch_ms\",\n        \"throughput_samples_per_sec\",\n        \"avg_power_w\", \"energy_per_sample_j\", \"energy_per_sample_mj\",\n        \"energy_total_j\", \"peak_memory_mb\"\n    ]\n    \n    aggregated = {}\n    \n    # Copy metadata from first trial\n    aggregated[\"precision\"] = trial_results[0][\"precision\"]\n    aggregated[\"batch_size\"] = trial_results[0][\"batch_size\"]\n    aggregated[\"seq_len\"] = trial_results[0][\"seq_len\"]\n    aggregated[\"num_trials\"] = len(trial_results)\n    aggregated[\"model_size_mb\"] = trial_results[0][\"model_size_mb\"]\n    \n    # Compute mean and std for each metric\n    for key in numeric_keys:\n        if key in trial_results[0] and trial_results[0][key] is not None:\n            values = [r[key] for r in trial_results if key in r and r[key] is not None]\n            if values:\n                aggregated[f\"{key}_mean\"] = float(np.mean(values))\n                aggregated[f\"{key}_std\"] = float(np.std(values))\n                aggregated[f\"{key}_min\"] = float(np.min(values))\n                aggregated[f\"{key}_max\"] = float(np.max(values))\n    \n    return aggregated\n\ndef print_aggregated_results(agg: Dict):\n    \"\"\"\n    Print aggregated results in a readable format.\n    \"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"AGGREGATED RESULTS: {agg['precision'].upper()}\")\n    print(f\"Number of trials: {agg['num_trials']}\")\n    print(f\"{'='*70}\")\n    \n    print(f\"\\nLatency:\")\n    if 'latency_per_sample_ms_mean' in agg:\n        print(f\"  Per sample: {agg['latency_per_sample_ms_mean']:.3f} ± {agg['latency_per_sample_ms_std']:.3f} ms\")\n    \n    print(f\"\\nThroughput:\")\n    if 'throughput_samples_per_sec_mean' in agg:\n        print(f\"  {agg['throughput_samples_per_sec_mean']:.2f} ± {agg['throughput_samples_per_sec_std']:.2f} samples/sec\")\n    \n    print(f\"\\nPower:\")\n    if 'avg_power_w_mean' in agg:\n        print(f\"  Average: {agg['avg_power_w_mean']:.2f} ± {agg['avg_power_w_std']:.2f} W\")\n    \n    print(f\"\\nEnergy:\")\n    if 'energy_per_sample_mj_mean' in agg:\n        print(f\"  Per sample: {agg['energy_per_sample_mj_mean']:.3f} ± {agg['energy_per_sample_mj_std']:.3f} mJ\")\n    \n    print(f\"\\nMemory:\")\n    print(f\"  Model size: {agg['model_size_mb']:.2f} MB\")\n    if 'peak_memory_mb_mean' in agg:\n        print(f\"  Peak GPU memory: {agg['peak_memory_mb_mean']:.2f} ± {agg['peak_memory_mb_std']:.2f} MB\")\n    \n    print(f\"{'='*70}\\n\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 11. Results Writing","metadata":{}},{"cell_type":"code","source":"def write_results_to_csv(all_results: List[Dict], output_path: str):\n    \"\"\"\n    Write aggregated results to CSV file.\n    \"\"\"\n    df = pd.DataFrame(all_results)\n    df.to_csv(output_path, index=False)\n    print(f\"Results written to {output_path}\")\n\ndef write_detailed_results(trial_results: List[Dict], output_path: str):\n    \"\"\"\n    Write detailed per-trial results to CSV.\n    \"\"\"\n    df = pd.DataFrame(trial_results)\n    df.to_csv(output_path, index=False)\n    print(f\"Detailed results written to {output_path}\")\n\ndef save_results_json(results: Dict, output_path: str):\n    \"\"\"\n    Save results as JSON for easy loading later.\n    \"\"\"\n    with open(output_path, 'w') as f:\n        json.dump(results, f, indent=2)\n    print(f\"Results saved to {output_path}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 12. Main Experiment Runner","metadata":{}},{"cell_type":"code","source":"def run_all_experiments(config: Dict, precisions: List[str]):\n    \"\"\"\n    Main entry point: run experiments for all precision levels.\n    \n    Args:\n        config: Experiment configuration\n        precisions: List of precision levels to test (e.g., ['fp32', 'fp16', 'int8'])\n    \"\"\"\n    print(\"\\n\" + \"#\"*70)\n    print(\"# STARTING ENERGY MEASUREMENT EXPERIMENTS\")\n    print(\"#\"*70)\n    print(f\"\\nConfiguration:\")\n    print(f\"  Model: {config['model_name']}\")\n    print(f\"  Batch size: {config['batch_size']}\")\n    print(f\"  Sequence length: {config['seq_len']}\")\n    print(f\"  Measurement loops: {config['num_loops']}\")\n    print(f\"  Warmup loops: {config['warmup_loops']}\")\n    print(f\"  Trials per precision: {config['num_trials']}\")\n    print(f\"  Device: {config['device']}\")\n    print(f\"  Precisions to test: {precisions}\")\n    \n    all_aggregated = []\n    all_detailed = []\n    \n    for precision in precisions:\n        # Run trials for this precision\n        trial_results = run_experiments_for_precision(\n            config, \n            precision, \n            num_trials=config[\"num_trials\"]\n        )\n        \n        # Aggregate results\n        aggregated = aggregate_trials(trial_results)\n        print_aggregated_results(aggregated)\n        \n        # Store results\n        all_aggregated.append(aggregated)\n        all_detailed.extend(trial_results)\n        \n        # Save intermediate results\n        results_dir = Path(config[\"results_dir\"])\n        save_results_json(\n            {\"aggregated\": aggregated, \"trials\": trial_results},\n            results_dir / f\"{precision}_results.json\"\n        )\n    \n    # Write final results\n    results_dir = Path(config[\"results_dir\"])\n    write_results_to_csv(all_aggregated, results_dir / \"results_aggregated.csv\")\n    write_detailed_results(all_detailed, results_dir / \"results_detailed.csv\")\n    \n    print(\"\\n\" + \"#\"*70)\n    print(\"# EXPERIMENTS COMPLETE\")\n    print(\"#\"*70)\n    print(f\"\\nResults saved to: {results_dir}\")\n    \n    return all_aggregated, all_detailed","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 13. Example: Create Pre-tokenized Dataset\n\nThis cell shows how to prepare a pre-tokenized dataset that can be loaded directly to GPU.","metadata":{}},{"cell_type":"code","source":"def create_pre_tokenized_dataset(model_name: str, dataset_name: str = \"sst2\", \n                                 num_samples: int = 1000, output_path: str = \"./data/pre_tokenized.pt\"):\n    \"\"\"\n    Create and save a pre-tokenized dataset.\n    This is done ONCE before running experiments.\n    \n    Example for SST-2 dataset.\n    \"\"\"\n    from datasets import load_dataset\n    \n    print(f\"Creating pre-tokenized dataset...\")\n    print(f\"  Model: {model_name}\")\n    print(f\"  Dataset: {dataset_name}\")\n    print(f\"  Samples: {num_samples}\")\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    # Load dataset\n    if dataset_name == \"sst2\":\n        dataset = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n    else:\n        raise ValueError(f\"Dataset {dataset_name} not implemented\")\n    \n    # Limit to num_samples\n    if len(dataset) > num_samples:\n        dataset = dataset.select(range(num_samples))\n    \n    # Tokenize\n    def tokenize_function(examples):\n        return tokenizer(\n            examples[\"sentence\"],\n            padding=\"max_length\",\n            truncation=True,\n            max_length=128,\n            return_tensors=\"pt\"\n        )\n    \n    # Process in batches\n    all_input_ids = []\n    all_attention_mask = []\n    all_labels = []\n    \n    for item in dataset:\n        encoded = tokenizer(\n            item[\"sentence\"],\n            padding=\"max_length\",\n            truncation=True,\n            max_length=128,\n            return_tensors=\"pt\"\n        )\n        all_input_ids.append(encoded[\"input_ids\"])\n        all_attention_mask.append(encoded[\"attention_mask\"])\n        all_labels.append(item[\"label\"])\n    \n    # Stack into tensors\n    input_ids = torch.cat(all_input_ids, dim=0)\n    attention_mask = torch.cat(all_attention_mask, dim=0)\n    labels = torch.tensor(all_labels)\n    \n    # Save\n    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n    torch.save({\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels\n    }, output_path)\n    \n    print(f\"\\nDataset saved to: {output_path}\")\n    print(f\"  Shape: {input_ids.shape}\")\n    print(f\"  Size: {(input_ids.element_size() * input_ids.nelement() + \n                       attention_mask.element_size() * attention_mask.nelement() + \n                       labels.element_size() * labels.nelement()) / 1024**2:.2f} MB\")\n\n# Uncomment to create dataset\n# create_pre_tokenized_dataset(\n#     model_name=\"distilbert-base-uncased-finetuned-sst-2-english\",\n#     dataset_name=\"sst2\",\n#     num_samples=1000,\n#     output_path=\"./data/pre_tokenized.pt\"\n# )","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 14. RUN EXPERIMENTS\n\nThis is the main execution cell. Run this after:\n1. Creating your pre-tokenized dataset\n2. Adjusting the config as needed","metadata":{}},{"cell_type":"code","source":"# Define precision levels to test\nprecisions_to_test = [\"fp32\", \"fp16\"]  # Add \"int8\" when ready\n\n# Update config if needed\nconfig[\"num_trials\"] = 5\nconfig[\"num_loops\"] = 300\nconfig[\"warmup_loops\"] = 50\nconfig[\"batch_size\"] = 8\n\n# Save config\nsave_config(config, Path(config[\"results_dir\"]) / \"experiment_config.json\")\n\n# Run all experiments\naggregated_results, detailed_results = run_all_experiments(config, precisions_to_test)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 15. Results Analysis and Visualization","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\n\ndef plot_comparison(aggregated_results: List[Dict]):\n    \"\"\"\n    Create comparison plots for different precision levels.\n    \"\"\"\n    df = pd.DataFrame(aggregated_results)\n    \n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    \n    # Latency comparison\n    ax = axes[0, 0]\n    if 'latency_per_sample_ms_mean' in df.columns:\n        x = range(len(df))\n        ax.bar(x, df['latency_per_sample_ms_mean'], \n               yerr=df['latency_per_sample_ms_std'],\n               capsize=5, alpha=0.7)\n        ax.set_xticks(x)\n        ax.set_xticklabels(df['precision'].str.upper())\n        ax.set_ylabel('Latency (ms)')\n        ax.set_title('Latency per Sample')\n        ax.grid(axis='y', alpha=0.3)\n    \n    # Energy comparison\n    ax = axes[0, 1]\n    if 'energy_per_sample_mj_mean' in df.columns:\n        x = range(len(df))\n        ax.bar(x, df['energy_per_sample_mj_mean'],\n               yerr=df['energy_per_sample_mj_std'],\n               capsize=5, alpha=0.7, color='orange')\n        ax.set_xticks(x)\n        ax.set_xticklabels(df['precision'].str.upper())\n        ax.set_ylabel('Energy (mJ)')\n        ax.set_title('Energy per Sample')\n        ax.grid(axis='y', alpha=0.3)\n    \n    # Throughput comparison\n    ax = axes[1, 0]\n    if 'throughput_samples_per_sec_mean' in df.columns:\n        x = range(len(df))\n        ax.bar(x, df['throughput_samples_per_sec_mean'],\n               yerr=df['throughput_samples_per_sec_std'],\n               capsize=5, alpha=0.7, color='green')\n        ax.set_xticks(x)\n        ax.set_xticklabels(df['precision'].str.upper())\n        ax.set_ylabel('Throughput (samples/sec)')\n        ax.set_title('Inference Throughput')\n        ax.grid(axis='y', alpha=0.3)\n    \n    # Model size comparison\n    ax = axes[1, 1]\n    if 'model_size_mb' in df.columns:\n        x = range(len(df))\n        ax.bar(x, df['model_size_mb'], alpha=0.7, color='purple')\n        ax.set_xticks(x)\n        ax.set_xticklabels(df['precision'].str.upper())\n        ax.set_ylabel('Model Size (MB)')\n        ax.set_title('Model Size on Disk')\n        ax.grid(axis='y', alpha=0.3)\n    \n    plt.tight_layout()\n    \n    # Save figure\n    results_dir = Path(config[\"results_dir\"])\n    plt.savefig(results_dir / \"comparison_plots.png\", dpi=300, bbox_inches='tight')\n    print(f\"\\nPlots saved to {results_dir / 'comparison_plots.png'}\")\n    plt.show()\n\n# Generate plots\nif aggregated_results:\n    plot_comparison(aggregated_results)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 16. Summary Statistics Table","metadata":{}},{"cell_type":"code","source":"def create_summary_table(aggregated_results: List[Dict]):\n    \"\"\"\n    Create a formatted summary table.\n    \"\"\"\n    summary_data = []\n    \n    for result in aggregated_results:\n        row = {\n            \"Precision\": result[\"precision\"].upper(),\n            \"Latency (ms)\": f\"{result.get('latency_per_sample_ms_mean', 0):.3f} ± {result.get('latency_per_sample_ms_std', 0):.3f}\",\n            \"Throughput (samples/s)\": f\"{result.get('throughput_samples_per_sec_mean', 0):.2f} ± {result.get('throughput_samples_per_sec_std', 0):.2f}\",\n            \"Energy (mJ)\": f\"{result.get('energy_per_sample_mj_mean', 0):.3f} ± {result.get('energy_per_sample_mj_std', 0):.3f}\" if result.get('energy_per_sample_mj_mean') else \"N/A\",\n            \"Power (W)\": f\"{result.get('avg_power_w_mean', 0):.2f} ± {result.get('avg_power_w_std', 0):.2f}\" if result.get('avg_power_w_mean') else \"N/A\",\n            \"Model Size (MB)\": f\"{result.get('model_size_mb', 0):.2f}\",\n            \"Peak Memory (MB)\": f\"{result.get('peak_memory_mb_mean', 0):.2f}\" if result.get('peak_memory_mb_mean') else \"N/A\"\n        }\n        summary_data.append(row)\n    \n    df = pd.DataFrame(summary_data)\n    \n    print(\"\\n\" + \"=\"*100)\n    print(\"SUMMARY TABLE\")\n    print(\"=\"*100)\n    print(df.to_string(index=False))\n    print(\"=\"*100 + \"\\n\")\n    \n    # Save to CSV\n    results_dir = Path(config[\"results_dir\"])\n    df.to_csv(results_dir / \"summary_table.csv\", index=False)\n    print(f\"Summary table saved to {results_dir / 'summary_table.csv'}\")\n    \n    return df\n\nif aggregated_results:\n    summary_df = create_summary_table(aggregated_results)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Notes and Next Steps\n\n### Checklist for \"Done\" Measurement Harness:\n\n- [ ] Dataset loads from .pt → tensors on GPU, no IO in loop\n- [ ] Model loads once per trial, moved to GPU, correct precision\n- [ ] Warmup runs N iterations, no timing/power\n- [ ] Timed loop yields stable total_time (small variation across trials)\n- [ ] PowerLogger returns non-empty list of watt values during loop\n- [ ] Energy metrics compute without errors\n- [ ] Per-precision results appear in CSV/JSON with all metrics\n- [ ] No code inside measurement loop does: disk IO, tokenization, CPU↔GPU copies, model loading\n\n### Important Notes:\n\n1. **Power Monitoring on Kaggle**: The nvidia-smi command may need adjustment for Kaggle environments. Test with a simple cell first.\n\n2. **INT8 Quantization**: The current INT8 implementation uses PyTorch's dynamic quantization for CPU. For GPU, you may need to use libraries like `bitsandbytes` or implement custom quantization.\n\n3. **Dataset Creation**: Make sure to run the dataset creation cell once before running experiments.\n\n4. **Memory Management**: The harness includes GPU memory cleanup between trials to ensure fair comparison.\n\n5. **Results**: All results are saved in multiple formats (CSV, JSON) for easy analysis and sharing.","metadata":{}}]}
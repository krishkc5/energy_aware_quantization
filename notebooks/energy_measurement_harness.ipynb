{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy-Aware Quantization Measurement Harness\n",
    "## ESE 5390 Final Project: Accurate Energy Measurement for Quantized LLMs\n",
    "\n",
    "This notebook implements a zero-I/O measurement harness for measuring:\n",
    "- **Energy consumption** (per inference)\n",
    "- **Latency** (per sample and per batch)\n",
    "- **Throughput** (samples/second)\n",
    "- **Accuracy** (classification accuracy)\n",
    "- **Memory usage** (GPU memory)\n",
    "\n",
    "Across three precision levels:\n",
    "- **FP32**: Full precision baseline\n",
    "- **FP16**: Half precision\n",
    "- **INT8**: 8-bit quantized (simulated on GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/krishkc5/energy_aware_quantization.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading (Zero-I/O Design)\n",
    "\n",
    "Load pre-tokenized tensors directly to GPU before any measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretokenized_dataset(dataset_path: str, device: str = \"cuda\") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Dict]:\n",
    "    \"\"\"\n",
    "    Load pre-tokenized dataset from separate .pt files.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to directory containing input_ids.pt, attention_mask.pt, labels.pt, metadata.json\n",
    "        device: Device to load tensors to ('cuda' or 'cpu')\n",
    "    \n",
    "    Returns:\n",
    "        input_ids: [N, seq_len] tensor on device\n",
    "        attention_mask: [N, seq_len] tensor on device\n",
    "        labels: [N] tensor on device\n",
    "        metadata: Dictionary with dataset info\n",
    "    \"\"\"\n",
    "    dataset_path = Path(dataset_path)\n",
    "    print(f\"\\nLoading dataset from: {dataset_path}\")\n",
    "    \n",
    "    # Load tensors\n",
    "    input_ids = torch.load(dataset_path / \"input_ids.pt\", map_location=device)\n",
    "    attention_mask = torch.load(dataset_path / \"attention_mask.pt\", map_location=device)\n",
    "    labels = torch.load(dataset_path / \"labels.pt\", map_location=device)\n",
    "    \n",
    "    # Load metadata\n",
    "    with open(dataset_path / \"metadata.json\", 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"✓ Loaded {input_ids.shape[0]} samples\")\n",
    "    print(f\"  - Sequence length: {input_ids.shape[1]}\")\n",
    "    print(f\"  - Device: {input_ids.device}\")\n",
    "    print(f\"  - Dataset: {metadata.get('dataset_name', 'unknown')}\")\n",
    "    print(f\"  - Labels: {metadata.get('num_labels', 2)}\")\n",
    "    \n",
    "    # Calculate memory footprint\n",
    "    total_bytes = (input_ids.element_size() * input_ids.nelement() + \n",
    "                   attention_mask.element_size() * attention_mask.nelement() + \n",
    "                   labels.element_size() * labels.nelement())\n",
    "    print(f\"  - Memory: {total_bytes / 1024**2:.2f} MB\")\n",
    "    \n",
    "    return input_ids, attention_mask, labels, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Loading with Accurate Quantization\n",
    "\n",
    "Load models in FP32, FP16, or INT8 precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_int8_quantization_gpu(model: nn.Module, verbose: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Apply symmetric INT8 quantization to model weights (for GPU).\n",
    "    \n",
    "    Uses per-tensor symmetric quantization:\n",
    "    - scale = max(|weight|) / 127\n",
    "    - quantized = round(weight / scale)\n",
    "    - dequantized = quantized * scale\n",
    "    \n",
    "    This simulates INT8 precision loss on GPU (true INT8 compute requires special kernels).\n",
    "    \"\"\"\n",
    "    num_quantized = 0\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            with torch.no_grad():\n",
    "                # Quantize weights\n",
    "                weight = module.weight.data\n",
    "                scale = weight.abs().max() / 127.0\n",
    "                \n",
    "                if scale > 0:\n",
    "                    weight_q = torch.round(weight / scale)\n",
    "                    weight_q = torch.clamp(weight_q, -128, 127)\n",
    "                    module.weight.data = weight_q * scale\n",
    "                \n",
    "                # Quantize bias if exists\n",
    "                if module.bias is not None:\n",
    "                    bias = module.bias.data\n",
    "                    scale_bias = bias.abs().max() / 127.0\n",
    "                    \n",
    "                    if scale_bias > 0:\n",
    "                        bias_q = torch.round(bias / scale_bias)\n",
    "                        bias_q = torch.clamp(bias_q, -128, 127)\n",
    "                        module.bias.data = bias_q * scale_bias\n",
    "                \n",
    "                num_quantized += 1\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  ✓ Quantized {num_quantized} Linear layers to INT8 precision\")\n",
    "\n",
    "\n",
    "def load_model(model_name: str, precision: str, device: str = \"cuda\", num_labels: int = 2, verbose: bool = True) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Load model with specified precision.\n",
    "    \n",
    "    Args:\n",
    "        model_name: HuggingFace model identifier\n",
    "        precision: 'fp32', 'fp16', or 'int8'\n",
    "        device: Device to load model to\n",
    "        num_labels: Number of classification labels\n",
    "        verbose: Print loading info\n",
    "    \n",
    "    Returns:\n",
    "        model: Loaded model in eval mode\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\nLoading model: {model_name}\")\n",
    "        print(f\"  Precision: {precision.upper()}\")\n",
    "        print(f\"  Device: {device}\")\n",
    "    \n",
    "    # Load base model in FP32\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        torch_dtype=torch.float32\n",
    "    )\n",
    "    \n",
    "    # Apply precision conversion\n",
    "    if precision == \"fp32\":\n",
    "        model = model.to(device)\n",
    "    \n",
    "    elif precision == \"fp16\":\n",
    "        model = model.half()\n",
    "        model = model.to(device)\n",
    "        if verbose:\n",
    "            print(f\"  ✓ Converted to FP16\")\n",
    "    \n",
    "    elif precision == \"int8\":\n",
    "        if device == \"cuda\":\n",
    "            model = model.to(device)\n",
    "            apply_int8_quantization_gpu(model, verbose=verbose)\n",
    "        else:\n",
    "            # CPU quantization using PyTorch's dynamic quantization\n",
    "            model = torch.quantization.quantize_dynamic(\n",
    "                model, {nn.Linear}, dtype=torch.qint8\n",
    "            )\n",
    "            model = model.to(\"cpu\")\n",
    "            if verbose:\n",
    "                print(f\"  ✓ Applied dynamic INT8 quantization (CPU)\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown precision: {precision}\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Calculate model size\n",
    "    param_size = sum(p.nelement() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.nelement() * b.element_size() for b in model.buffers())\n",
    "    model_size_mb = (param_size + buffer_size) / 1024**2\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  ✓ Model loaded successfully\")\n",
    "        print(f\"  - Parameters: {num_params:,}\")\n",
    "        print(f\"  - Model size: {model_size_mb:.2f} MB\")\n",
    "        first_param = next(model.parameters())\n",
    "        print(f\"  - Param dtype: {first_param.dtype}\")\n",
    "        print(f\"  - Param device: {first_param.device}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_info(model: nn.Module) -> Dict:\n",
    "    \"\"\"Get model metadata.\"\"\"\n",
    "    param_size = sum(p.nelement() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.nelement() * b.element_size() for b in model.buffers())\n",
    "    \n",
    "    return {\n",
    "        \"model_size_mb\": (param_size + buffer_size) / 1024**2,\n",
    "        \"num_parameters\": sum(p.numel() for p in model.parameters()),\n",
    "        \"dtype\": str(next(model.parameters()).dtype),\n",
    "        \"device\": str(next(model.parameters()).device)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GPU Warmup\n",
    "\n",
    "Stabilize GPU clocks and compile CUDA kernels before measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_gpu(model: nn.Module, input_ids: torch.Tensor, attention_mask: torch.Tensor, \n",
    "               num_steps: int = 50, verbose: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Warmup GPU to stabilize clocks and compile kernels.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to warmup\n",
    "        input_ids: Sample input tensor\n",
    "        attention_mask: Sample attention mask\n",
    "        num_steps: Number of warmup iterations\n",
    "        verbose: Print progress\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\nWarming up GPU for {num_steps} iterations...\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_steps):\n",
    "            _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            if verbose and (i + 1) % 10 == 0:\n",
    "                print(f\"  Warmup: {i+1}/{num_steps}\", end='\\r')\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n  ✓ Warmup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Power Logger (Asynchronous)\n",
    "\n",
    "Monitor GPU power draw using nvidia-smi without interfering with timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerLogger:\n",
    "    \"\"\"\n",
    "    Asynchronous power logger using nvidia-smi polling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sample_interval_ms: int = 100, gpu_id: int = 0, verbose: bool = False):\n",
    "        self.sample_interval_ms = sample_interval_ms\n",
    "        self.gpu_id = gpu_id\n",
    "        self.verbose = verbose\n",
    "        self.samples = []\n",
    "        self.is_running = False\n",
    "        self._lock = threading.Lock()\n",
    "        self._reader_thread = None\n",
    "        \n",
    "        # Verify nvidia-smi is available\n",
    "        self._check_nvidia_smi()\n",
    "    \n",
    "    def _check_nvidia_smi(self) -> None:\n",
    "        \"\"\"Check if nvidia-smi is available.\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"nvidia-smi\", \"--query-gpu=power.draw\", \"--format=csv,noheader,nounits\", f\"--id={self.gpu_id}\"],\n",
    "                capture_output=True, text=True, timeout=5\n",
    "            )\n",
    "            if result.returncode != 0:\n",
    "                raise RuntimeError(f\"nvidia-smi failed: {result.stderr}\")\n",
    "            power = float(result.stdout.strip())\n",
    "            if self.verbose:\n",
    "                print(f\"  ✓ nvidia-smi available, current power: {power:.2f} W\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"nvidia-smi not available: {e}\")\n",
    "    \n",
    "    def start(self) -> None:\n",
    "        \"\"\"Start power logging in background thread.\"\"\"\n",
    "        with self._lock:\n",
    "            if self.is_running:\n",
    "                raise RuntimeError(\"PowerLogger already running\")\n",
    "            self.samples = []\n",
    "            self.is_running = True\n",
    "        \n",
    "        self._reader_thread = threading.Thread(target=self._poll_power, daemon=True)\n",
    "        self._reader_thread.start()\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"  ✓ Power logger started (interval: {self.sample_interval_ms} ms)\")\n",
    "    \n",
    "    def _poll_power(self) -> None:\n",
    "        \"\"\"Background thread that polls nvidia-smi.\"\"\"\n",
    "        interval_sec = self.sample_interval_ms / 1000.0\n",
    "        \n",
    "        while self.is_running:\n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [\"nvidia-smi\", \"--query-gpu=power.draw\", \"--format=csv,noheader,nounits\", f\"--id={self.gpu_id}\"],\n",
    "                    capture_output=True, text=True, timeout=2\n",
    "                )\n",
    "                \n",
    "                if result.returncode == 0:\n",
    "                    power = float(result.stdout.strip())\n",
    "                    with self._lock:\n",
    "                        self.samples.append(power)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            time.sleep(interval_sec)\n",
    "    \n",
    "    def stop(self) -> None:\n",
    "        \"\"\"Stop power logging.\"\"\"\n",
    "        with self._lock:\n",
    "            if not self.is_running:\n",
    "                raise RuntimeError(\"PowerLogger not running\")\n",
    "            self.is_running = False\n",
    "        \n",
    "        if self._reader_thread:\n",
    "            self._reader_thread.join(timeout=2)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"  ✓ Power logger stopped ({len(self.samples)} samples)\")\n",
    "    \n",
    "    def get_samples(self) -> List[float]:\n",
    "        \"\"\"Return collected power samples.\"\"\"\n",
    "        with self._lock:\n",
    "            return self.samples.copy()\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.start()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        if self.is_running:\n",
    "            self.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Timed Inference Benchmark\n",
    "\n",
    "Run inference with precise timing and CUDA synchronization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_timed_inference(model: nn.Module, input_ids: torch.Tensor, attention_mask: torch.Tensor,\n",
    "                       num_iters: int, verbose: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Run timed inference loop with CUDA synchronization.\n",
    "    \n",
    "    Args:\n",
    "        model: Model in eval mode\n",
    "        input_ids: Input tensor [batch_size, seq_len]\n",
    "        attention_mask: Attention mask [batch_size, seq_len]\n",
    "        num_iters: Number of iterations\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with timing metrics\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\nRunning {num_iters} timed inference iterations...\")\n",
    "    \n",
    "    model.eval()\n",
    "    batch_size = input_ids.shape[0]\n",
    "    latencies = []\n",
    "    \n",
    "    # Ensure clean state\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_iters):\n",
    "            iter_start = time.perf_counter()\n",
    "            \n",
    "            # Forward pass\n",
    "            _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Synchronize to ensure completion\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            iter_end = time.perf_counter()\n",
    "            latencies.append(iter_end - iter_start)\n",
    "            \n",
    "            if verbose and (i + 1) % 50 == 0:\n",
    "                print(f\"  Progress: {i+1}/{num_iters}\", end='\\r')\n",
    "    \n",
    "    end_time = time.perf_counter()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n  ✓ Inference complete: {total_time:.3f}s\")\n",
    "    \n",
    "    latencies = np.array(latencies)\n",
    "    \n",
    "    return {\n",
    "        \"total_time\": float(total_time),\n",
    "        \"num_iters\": num_iters,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"mean_latency\": float(np.mean(latencies)),\n",
    "        \"std_latency\": float(np.std(latencies)),\n",
    "        \"min_latency\": float(np.min(latencies)),\n",
    "        \"max_latency\": float(np.max(latencies)),\n",
    "        \"median_latency\": float(np.median(latencies)),\n",
    "        \"throughput\": float(batch_size * num_iters / total_time)\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_accuracy(model: nn.Module, input_ids: torch.Tensor, attention_mask: torch.Tensor,\n",
    "                    labels: torch.Tensor, verbose: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Compute model accuracy on dataset.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with accuracy metrics\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\nComputing accuracy...\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        correct = (predictions == labels).sum().item()\n",
    "        total = labels.shape[0]\n",
    "        accuracy = correct / total\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  ✓ Accuracy: {accuracy*100:.2f}% ({correct}/{total})\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"num_correct\": int(correct),\n",
    "        \"num_samples\": int(total)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Energy Computation\n",
    "\n",
    "Compute energy metrics from power samples and timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_energy_metrics(power_samples: List[float], timing_results: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Compute energy consumption from power samples and timing.\n",
    "    \n",
    "    Energy (J) = Power (W) × Time (s)\n",
    "    \n",
    "    Args:\n",
    "        power_samples: List of power measurements in Watts\n",
    "        timing_results: Dictionary from run_timed_inference\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with energy metrics\n",
    "    \"\"\"\n",
    "    if len(power_samples) == 0:\n",
    "        print(\"  ⚠️  Warning: No power samples collected\")\n",
    "        return {\n",
    "            \"mean_power_w\": None,\n",
    "            \"std_power_w\": None,\n",
    "            \"min_power_w\": None,\n",
    "            \"max_power_w\": None,\n",
    "            \"num_power_samples\": 0,\n",
    "            \"total_energy_j\": None,\n",
    "            \"energy_per_inference_j\": None,\n",
    "            \"energy_per_inference_mj\": None,\n",
    "            \"inferences_per_joule\": None\n",
    "        }\n",
    "    \n",
    "    power_array = np.array(power_samples)\n",
    "    \n",
    "    # Power statistics\n",
    "    mean_power = float(np.mean(power_array))\n",
    "    std_power = float(np.std(power_array))\n",
    "    min_power = float(np.min(power_array))\n",
    "    max_power = float(np.max(power_array))\n",
    "    \n",
    "    # Energy computation\n",
    "    total_time = timing_results[\"total_time\"]\n",
    "    num_inferences = timing_results[\"num_iters\"]\n",
    "    \n",
    "    total_energy = mean_power * total_time  # Joules\n",
    "    energy_per_inference = total_energy / num_inferences\n",
    "    energy_per_inference_mj = energy_per_inference * 1000  # millijoules\n",
    "    \n",
    "    return {\n",
    "        \"mean_power_w\": mean_power,\n",
    "        \"std_power_w\": std_power,\n",
    "        \"min_power_w\": min_power,\n",
    "        \"max_power_w\": max_power,\n",
    "        \"num_power_samples\": len(power_samples),\n",
    "        \"total_energy_j\": total_energy,\n",
    "        \"energy_per_inference_j\": energy_per_inference,\n",
    "        \"energy_per_inference_mj\": energy_per_inference_mj,\n",
    "        \"inferences_per_joule\": 1.0 / energy_per_inference if energy_per_inference > 0 else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete Measurement Function\n",
    "\n",
    "Orchestrate the complete measurement pipeline for one trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_trial(model_name: str, precision: str, input_ids: torch.Tensor, \n",
    "                    attention_mask: torch.Tensor, labels: torch.Tensor,\n",
    "                    num_iters: int = 300, warmup_iters: int = 50, \n",
    "                    power_interval_ms: int = 100, device: str = \"cuda\",\n",
    "                    trial_num: int = 1, verbose: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Run a complete measurement trial for one precision level.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Load model\n",
    "    2. Warmup GPU\n",
    "    3. Start power logging\n",
    "    4. Run timed inference\n",
    "    5. Stop power logging\n",
    "    6. Compute accuracy\n",
    "    7. Compute energy metrics\n",
    "    8. Collect memory stats\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with all metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TRIAL {trial_num}: {precision.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Reset GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # 1. Load model\n",
    "    model = load_model(model_name, precision, device, num_labels=2, verbose=verbose)\n",
    "    model_info = get_model_info(model)\n",
    "    \n",
    "    # 2. Warmup\n",
    "    warmup_gpu(model, input_ids, attention_mask, num_steps=warmup_iters, verbose=verbose)\n",
    "    \n",
    "    # 3. Start power logging\n",
    "    print(f\"\\nStarting measurement...\")\n",
    "    power_logger = PowerLogger(sample_interval_ms=power_interval_ms, verbose=verbose)\n",
    "    power_logger.start()\n",
    "    time.sleep(0.5)  # Let logger stabilize\n",
    "    \n",
    "    # 4. Run timed inference\n",
    "    timing_results = run_timed_inference(model, input_ids, attention_mask, num_iters, verbose=verbose)\n",
    "    \n",
    "    # 5. Stop power logging\n",
    "    time.sleep(0.5)  # Capture trailing samples\n",
    "    power_logger.stop()\n",
    "    power_samples = power_logger.get_samples()\n",
    "    \n",
    "    # 6. Compute accuracy\n",
    "    accuracy_results = compute_accuracy(model, input_ids, attention_mask, labels, verbose=verbose)\n",
    "    \n",
    "    # 7. Compute energy\n",
    "    energy_results = compute_energy_metrics(power_samples, timing_results)\n",
    "    \n",
    "    # 8. Memory stats\n",
    "    memory_stats = {}\n",
    "    if torch.cuda.is_available():\n",
    "        memory_stats = {\n",
    "            \"peak_memory_mb\": torch.cuda.max_memory_allocated() / 1024**2,\n",
    "            \"allocated_memory_mb\": torch.cuda.memory_allocated() / 1024**2,\n",
    "            \"reserved_memory_mb\": torch.cuda.memory_reserved() / 1024**2\n",
    "        }\n",
    "    \n",
    "    # Combine all results\n",
    "    results = {\n",
    "        \"trial\": trial_num,\n",
    "        \"precision\": precision,\n",
    "        \"model_name\": model_name,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "    }\n",
    "    results.update(timing_results)\n",
    "    results.update(accuracy_results)\n",
    "    results.update(energy_results)\n",
    "    results.update(memory_stats)\n",
    "    results.update(model_info)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TRIAL {trial_num} SUMMARY: {precision.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Latency:       {timing_results['mean_latency']*1000:.3f} ms (± {timing_results['std_latency']*1000:.3f} ms)\")\n",
    "    print(f\"Throughput:    {timing_results['throughput']:.2f} samples/s\")\n",
    "    print(f\"Accuracy:      {accuracy_results['accuracy']*100:.2f}%\")\n",
    "    if energy_results['mean_power_w'] is not None:\n",
    "        print(f\"Power:         {energy_results['mean_power_w']:.2f} W (± {energy_results['std_power_w']:.2f} W)\")\n",
    "        print(f\"Energy:        {energy_results['energy_per_inference_mj']:.3f} mJ/inference\")\n",
    "        print(f\"Total Energy:  {energy_results['total_energy_j']:.3f} J\")\n",
    "    if memory_stats:\n",
    "        print(f\"Peak Memory:   {memory_stats['peak_memory_mb']:.2f} MB\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Multi-Trial Runner\n",
    "\n",
    "Run multiple trials for statistical significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_trial_experiment(model_name: str, precision: str, dataset_path: str,\n",
    "                              num_trials: int = 5, num_iters: int = 300,\n",
    "                              warmup_iters: int = 50, power_interval_ms: int = 100,\n",
    "                              device: str = \"cuda\") -> Tuple[List[Dict], Dict]:\n",
    "    \"\"\"\n",
    "    Run multiple trials for one precision level and aggregate results.\n",
    "    \n",
    "    Returns:\n",
    "        trial_results: List of per-trial result dictionaries\n",
    "        aggregated: Dictionary with mean/std across trials\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"# MULTI-TRIAL EXPERIMENT: {precision.upper()}\")\n",
    "    print(f\"# Number of trials: {num_trials}\")\n",
    "    print(f\"# Iterations per trial: {num_iters}\")\n",
    "    print(f\"{'#'*70}\")\n",
    "    \n",
    "    # Load dataset once (stays on GPU)\n",
    "    input_ids, attention_mask, labels, metadata = load_pretokenized_dataset(dataset_path, device)\n",
    "    \n",
    "    # Run trials\n",
    "    trial_results = []\n",
    "    for trial in range(1, num_trials + 1):\n",
    "        result = run_single_trial(\n",
    "            model_name=model_name,\n",
    "            precision=precision,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            num_iters=num_iters,\n",
    "            warmup_iters=warmup_iters,\n",
    "            power_interval_ms=power_interval_ms,\n",
    "            device=device,\n",
    "            trial_num=trial,\n",
    "            verbose=True\n",
    "        )\n",
    "        trial_results.append(result)\n",
    "    \n",
    "    # Aggregate results\n",
    "    aggregated = aggregate_trials(trial_results)\n",
    "    print_aggregated_results(aggregated, precision)\n",
    "    \n",
    "    return trial_results, aggregated\n",
    "\n",
    "\n",
    "def aggregate_trials(trial_results: List[Dict]) -> Dict:\n",
    "    \"\"\"Compute mean and std across trials.\"\"\"\n",
    "    numeric_keys = [\n",
    "        \"mean_latency\", \"std_latency\", \"throughput\", \"accuracy\",\n",
    "        \"mean_power_w\", \"std_power_w\", \"total_energy_j\", \n",
    "        \"energy_per_inference_j\", \"energy_per_inference_mj\",\n",
    "        \"peak_memory_mb\"\n",
    "    ]\n",
    "    \n",
    "    aggregated = {\n",
    "        \"precision\": trial_results[0][\"precision\"],\n",
    "        \"model_name\": trial_results[0][\"model_name\"],\n",
    "        \"num_trials\": len(trial_results),\n",
    "        \"batch_size\": trial_results[0][\"batch_size\"],\n",
    "        \"model_size_mb\": trial_results[0][\"model_size_mb\"],\n",
    "        \"num_parameters\": trial_results[0][\"num_parameters\"]\n",
    "    }\n",
    "    \n",
    "    for key in numeric_keys:\n",
    "        if key in trial_results[0] and trial_results[0][key] is not None:\n",
    "            values = [r[key] for r in trial_results if key in r and r[key] is not None]\n",
    "            if values:\n",
    "                aggregated[f\"{key}_mean\"] = float(np.mean(values))\n",
    "                aggregated[f\"{key}_std\"] = float(np.std(values))\n",
    "                aggregated[f\"{key}_min\"] = float(np.min(values))\n",
    "                aggregated[f\"{key}_max\"] = float(np.max(values))\n",
    "    \n",
    "    return aggregated\n",
    "\n",
    "\n",
    "def print_aggregated_results(agg: Dict, precision: str) -> None:\n",
    "    \"\"\"Pretty print aggregated results.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"AGGREGATED RESULTS: {precision.upper()}\")\n",
    "    print(f\"Trials: {agg['num_trials']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if 'mean_latency_mean' in agg:\n",
    "        print(f\"\\nLatency:\")\n",
    "        print(f\"  {agg['mean_latency_mean']*1000:.3f} ± {agg['mean_latency_std']*1000:.3f} ms\")\n",
    "    \n",
    "    if 'throughput_mean' in agg:\n",
    "        print(f\"\\nThroughput:\")\n",
    "        print(f\"  {agg['throughput_mean']:.2f} ± {agg['throughput_std']:.2f} samples/s\")\n",
    "    \n",
    "    if 'accuracy_mean' in agg:\n",
    "        print(f\"\\nAccuracy:\")\n",
    "        print(f\"  {agg['accuracy_mean']*100:.2f} ± {agg['accuracy_std']*100:.2f}%\")\n",
    "    \n",
    "    if 'mean_power_w_mean' in agg:\n",
    "        print(f\"\\nPower:\")\n",
    "        print(f\"  {agg['mean_power_w_mean']:.2f} ± {agg['mean_power_w_std']:.2f} W\")\n",
    "    \n",
    "    if 'energy_per_inference_mj_mean' in agg:\n",
    "        print(f\"\\nEnergy per Inference:\")\n",
    "        print(f\"  {agg['energy_per_inference_mj_mean']:.3f} ± {agg['energy_per_inference_mj_std']:.3f} mJ\")\n",
    "    \n",
    "    if 'peak_memory_mb_mean' in agg:\n",
    "        print(f\"\\nMemory:\")\n",
    "        print(f\"  Model size: {agg['model_size_mb']:.2f} MB\")\n",
    "        print(f\"  Peak GPU: {agg['peak_memory_mb_mean']:.2f} ± {agg['peak_memory_mb_std']:.2f} MB\")\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Results Saving\n",
    "\n",
    "Save results in CSV and JSON formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(trial_results: List[Dict], aggregated: Dict, \n",
    "                precision: str, output_dir: str = \"./results\") -> None:\n",
    "    \"\"\"\n",
    "    Save trial and aggregated results.\n",
    "    \n",
    "    Creates:\n",
    "    - results/{precision}_trials.csv: Per-trial results\n",
    "    - results/{precision}_aggregated.json: Aggregated statistics\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save detailed trial results\n",
    "    trials_df = pd.DataFrame(trial_results)\n",
    "    trials_path = output_dir / f\"{precision}_trials.csv\"\n",
    "    trials_df.to_csv(trials_path, index=False)\n",
    "    print(f\"\\n✓ Saved trial results: {trials_path}\")\n",
    "    \n",
    "    # Save aggregated results\n",
    "    agg_path = output_dir / f\"{precision}_aggregated.json\"\n",
    "    with open(agg_path, 'w') as f:\n",
    "        json.dump(aggregated, f, indent=2)\n",
    "    print(f\"✓ Saved aggregated results: {agg_path}\")\n",
    "\n",
    "\n",
    "def save_comparison_table(all_aggregated: List[Dict], output_dir: str = \"./results\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create and save comparison table across all precisions.\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    \n",
    "    comparison_df = pd.DataFrame(all_aggregated)\n",
    "    comparison_path = output_dir / \"comparison_all_precisions.csv\"\n",
    "    comparison_df.to_csv(comparison_path, index=False)\n",
    "    print(f\"\\n✓ Saved comparison table: {comparison_path}\")\n",
    "    \n",
    "    return comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Main Experiment Runner\n",
    "\n",
    "Run experiments for all precision levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"model_name\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"dataset_path\": \"../datasets/tokenized_data\",\n",
    "    \"precisions\": [\"fp32\", \"fp16\", \"int8\"],  # Precision levels to test\n",
    "    \"num_trials\": 5,  # Number of trials per precision\n",
    "    \"num_iters\": 300,  # Inference iterations per trial\n",
    "    \"warmup_iters\": 50,  # Warmup iterations\n",
    "    \"power_interval_ms\": 100,  # Power sampling interval\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"output_dir\": \"../results\"\n",
    "}\n",
    "\n",
    "print(\"\\nExperiment Configuration:\")\n",
    "print(json.dumps(CONFIG, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments for all precisions\n",
    "all_trial_results = {}\n",
    "all_aggregated = []\n",
    "\n",
    "for precision in CONFIG[\"precisions\"]:\n",
    "    trial_results, aggregated = run_multi_trial_experiment(\n",
    "        model_name=CONFIG[\"model_name\"],\n",
    "        precision=precision,\n",
    "        dataset_path=CONFIG[\"dataset_path\"],\n",
    "        num_trials=CONFIG[\"num_trials\"],\n",
    "        num_iters=CONFIG[\"num_iters\"],\n",
    "        warmup_iters=CONFIG[\"warmup_iters\"],\n",
    "        power_interval_ms=CONFIG[\"power_interval_ms\"],\n",
    "        device=CONFIG[\"device\"]\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    save_results(trial_results, aggregated, precision, CONFIG[\"output_dir\"])\n",
    "    \n",
    "    # Store for comparison\n",
    "    all_trial_results[precision] = trial_results\n",
    "    all_aggregated.append(aggregated)\n",
    "\n",
    "# Save comparison table\n",
    "comparison_df = save_comparison_table(all_aggregated, CONFIG[\"output_dir\"])\n",
    "\n",
    "print(\"\\n\" + \"#\"*70)\n",
    "print(\"# ALL EXPERIMENTS COMPLETE\")\n",
    "print(\"#\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle(\"Energy-Aware Quantization: Performance Comparison\", fontsize=16, fontweight='bold')\n",
    "\n",
    "df = pd.DataFrame(all_aggregated)\n",
    "\n",
    "# 1. Latency\n",
    "ax = axes[0, 0]\n",
    "if 'mean_latency_mean' in df.columns:\n",
    "    x = range(len(df))\n",
    "    ax.bar(x, df['mean_latency_mean']*1000, yerr=df['mean_latency_std']*1000, capsize=5, alpha=0.7)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df['precision'].str.upper())\n",
    "    ax.set_ylabel('Latency (ms)', fontsize=11)\n",
    "    ax.set_title('Mean Latency per Inference', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Throughput\n",
    "ax = axes[0, 1]\n",
    "if 'throughput_mean' in df.columns:\n",
    "    x = range(len(df))\n",
    "    ax.bar(x, df['throughput_mean'], yerr=df['throughput_std'], capsize=5, alpha=0.7, color='green')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df['precision'].str.upper())\n",
    "    ax.set_ylabel('Throughput (samples/s)', fontsize=11)\n",
    "    ax.set_title('Inference Throughput', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Energy per Inference\n",
    "ax = axes[0, 2]\n",
    "if 'energy_per_inference_mj_mean' in df.columns:\n",
    "    x = range(len(df))\n",
    "    ax.bar(x, df['energy_per_inference_mj_mean'], yerr=df['energy_per_inference_mj_std'], \n",
    "           capsize=5, alpha=0.7, color='orange')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df['precision'].str.upper())\n",
    "    ax.set_ylabel('Energy (mJ)', fontsize=11)\n",
    "    ax.set_title('Energy per Inference', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Power Draw\n",
    "ax = axes[1, 0]\n",
    "if 'mean_power_w_mean' in df.columns:\n",
    "    x = range(len(df))\n",
    "    ax.bar(x, df['mean_power_w_mean'], yerr=df['mean_power_w_std'], capsize=5, alpha=0.7, color='red')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df['precision'].str.upper())\n",
    "    ax.set_ylabel('Power (W)', fontsize=11)\n",
    "    ax.set_title('Mean GPU Power Draw', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 5. Accuracy\n",
    "ax = axes[1, 1]\n",
    "if 'accuracy_mean' in df.columns:\n",
    "    x = range(len(df))\n",
    "    ax.bar(x, df['accuracy_mean']*100, yerr=df['accuracy_std']*100, capsize=5, alpha=0.7, color='purple')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df['precision'].str.upper())\n",
    "    ax.set_ylabel('Accuracy (%)', fontsize=11)\n",
    "    ax.set_title('Classification Accuracy', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylim([0, 100])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 6. Model Size\n",
    "ax = axes[1, 2]\n",
    "if 'model_size_mb' in df.columns:\n",
    "    x = range(len(df))\n",
    "    ax.bar(x, df['model_size_mb'], alpha=0.7, color='teal')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df['precision'].str.upper())\n",
    "    ax.set_ylabel('Size (MB)', fontsize=11)\n",
    "    ax.set_title('Model Size', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = Path(CONFIG[\"output_dir\"]) / \"comparison_plots.png\"\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n✓ Saved plots: {plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create formatted summary table\n",
    "summary_data = []\n",
    "\n",
    "for agg in all_aggregated:\n",
    "    row = {\n",
    "        \"Precision\": agg[\"precision\"].upper(),\n",
    "        \"Latency (ms)\": f\"{agg.get('mean_latency_mean', 0)*1000:.3f} ± {agg.get('mean_latency_std', 0)*1000:.3f}\",\n",
    "        \"Throughput (samples/s)\": f\"{agg.get('throughput_mean', 0):.2f} ± {agg.get('throughput_std', 0):.2f}\",\n",
    "        \"Accuracy (%)\": f\"{agg.get('accuracy_mean', 0)*100:.2f} ± {agg.get('accuracy_std', 0)*100:.2f}\",\n",
    "        \"Power (W)\": f\"{agg.get('mean_power_w_mean', 0):.2f} ± {agg.get('mean_power_w_std', 0):.2f}\" if agg.get('mean_power_w_mean') else \"N/A\",\n",
    "        \"Energy (mJ)\": f\"{agg.get('energy_per_inference_mj_mean', 0):.3f} ± {agg.get('energy_per_inference_mj_std', 0):.3f}\" if agg.get('energy_per_inference_mj_mean') else \"N/A\",\n",
    "        \"Model Size (MB)\": f\"{agg.get('model_size_mb', 0):.2f}\",\n",
    "        \"Peak Memory (MB)\": f\"{agg.get('peak_memory_mb_mean', 0):.2f}\" if agg.get('peak_memory_mb_mean') else \"N/A\"\n",
    "    }\n",
    "    summary_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"FINAL SUMMARY TABLE\")\n",
    "print(\"=\"*120)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Save summary table\n",
    "summary_path = Path(CONFIG[\"output_dir\"]) / \"summary_table.csv\"\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print(f\"\\n✓ Saved summary table: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Relative Improvements\n",
    "\n",
    "Compare FP16 and INT8 against FP32 baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_aggregated) >= 2:\n",
    "    # Use FP32 as baseline\n",
    "    baseline = next((a for a in all_aggregated if a['precision'] == 'fp32'), all_aggregated[0])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RELATIVE IMPROVEMENTS vs FP32 BASELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for agg in all_aggregated:\n",
    "        if agg['precision'] == 'fp32':\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{agg['precision'].upper()} vs FP32:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Latency speedup\n",
    "        if 'mean_latency_mean' in baseline and 'mean_latency_mean' in agg:\n",
    "            speedup = baseline['mean_latency_mean'] / agg['mean_latency_mean']\n",
    "            reduction_pct = (1 - 1/speedup) * 100\n",
    "            print(f\"Latency:     {speedup:.2f}x faster ({reduction_pct:.1f}% reduction)\")\n",
    "        \n",
    "        # Energy savings\n",
    "        if 'energy_per_inference_mj_mean' in baseline and 'energy_per_inference_mj_mean' in agg:\n",
    "            if baseline['energy_per_inference_mj_mean'] and agg['energy_per_inference_mj_mean']:\n",
    "                energy_ratio = agg['energy_per_inference_mj_mean'] / baseline['energy_per_inference_mj_mean']\n",
    "                energy_savings_pct = (1 - energy_ratio) * 100\n",
    "                print(f\"Energy:      {energy_savings_pct:+.1f}% change\")\n",
    "        \n",
    "        # Accuracy delta\n",
    "        if 'accuracy_mean' in baseline and 'accuracy_mean' in agg:\n",
    "            accuracy_delta = (agg['accuracy_mean'] - baseline['accuracy_mean']) * 100\n",
    "            print(f\"Accuracy:    {accuracy_delta:+.2f} percentage points\")\n",
    "        \n",
    "        # Model size reduction\n",
    "        if 'model_size_mb' in baseline and 'model_size_mb' in agg:\n",
    "            size_ratio = agg['model_size_mb'] / baseline['model_size_mb']\n",
    "            size_reduction_pct = (1 - size_ratio) * 100\n",
    "            print(f\"Model Size:  {size_reduction_pct:.1f}% reduction\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### Key Features:\n",
    "1. **Zero-I/O Design**: All data loaded to GPU before measurements\n",
    "2. **Accurate Quantization**: Symmetric INT8 quantization for weights\n",
    "3. **Asynchronous Power Monitoring**: nvidia-smi polling in background thread\n",
    "4. **CUDA Synchronization**: Precise timing with GPU sync after each forward pass\n",
    "5. **Statistical Significance**: Multiple trials with aggregation\n",
    "6. **Comprehensive Metrics**: Latency, throughput, energy, accuracy, memory\n",
    "\n",
    "### Checklist:\n",
    "- ✅ Dataset loads from .pt files directly to GPU\n",
    "- ✅ No I/O operations during measurement loop\n",
    "- ✅ Model loaded once per trial with correct precision\n",
    "- ✅ Warmup phase stabilizes GPU before measurement\n",
    "- ✅ Power logger runs asynchronously without blocking\n",
    "- ✅ CUDA synchronization ensures accurate timing\n",
    "- ✅ Energy computed as Power × Time\n",
    "- ✅ Results saved in CSV and JSON formats\n",
    "- ✅ Multiple trials for statistical confidence\n",
    "- ✅ Comparison plots and summary tables generated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Energy-Aware Quantization Measurement Harness\n## ESE 5390 Final Project: Accurate Energy Measurement for Quantized LLMs\n\nThis notebook implements a zero-I/O measurement harness for measuring:\n- **Energy consumption** (per inference)\n- **Latency** (per sample and per batch)\n- **Throughput** (samples/second)\n- **Accuracy** (classification accuracy)\n- **Memory usage** (GPU memory)\n\nAcross three precision levels:\n- **FP32**: Full precision baseline\n- **FP16**: Half precision\n- **INT8**: 8-bit quantized (simulated on GPU)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!git clone https://github.com/krishkc5/energy_aware_quantization.git",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-03T08:51:29.304159Z",
     "iopub.execute_input": "2025-12-03T08:51:29.304989Z",
     "iopub.status.idle": "2025-12-03T08:51:29.436887Z",
     "shell.execute_reply.started": "2025-12-03T08:51:29.304964Z",
     "shell.execute_reply": "2025-12-03T08:51:29.436013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "fatal: destination path 'energy_aware_quantization' already exists and is not an empty directory.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Setup and Imports",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn as nn\nfrom transformers import AutoModelForSequenceClassification\nimport numpy as np\nimport pandas as pd\nimport json\nimport time\nimport subprocess\nimport threading\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n    print(f\"CUDA version: {torch.version.cuda}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-03T08:51:29.438612Z",
     "iopub.execute_input": "2025-12-03T08:51:29.438865Z",
     "iopub.status.idle": "2025-12-03T08:51:29.445228Z",
     "shell.execute_reply.started": "2025-12-03T08:51:29.438842Z",
     "shell.execute_reply": "2025-12-03T08:51:29.444438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "PyTorch version: 2.6.0+cu124\nCUDA available: True\nCUDA device: Tesla P100-PCIE-16GB\nCUDA version: 12.4\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Dataset Loading (Zero-I/O Design)\n\nLoad pre-tokenized tensors directly to GPU before any measurements.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def load_pretokenized_dataset(dataset_path: str, device: str = \"cuda\") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Dict]:\n    \"\"\"\n    Load pre-tokenized dataset from separate .pt files.\n    \n    Args:\n        dataset_path: Path to directory containing input_ids.pt, attention_mask.pt, labels.pt, metadata.json\n        device: Device to load tensors to ('cuda' or 'cpu')\n    \n    Returns:\n        input_ids: [N, seq_len] tensor on device\n        attention_mask: [N, seq_len] tensor on device\n        labels: [N] tensor on device\n        metadata: Dictionary with dataset info\n    \"\"\"\n    dataset_path = Path(dataset_path)\n    print(f\"\\nLoading dataset from: {dataset_path}\")\n    \n    # Load tensors\n    input_ids = torch.load(dataset_path / \"input_ids.pt\", map_location=device)\n    attention_mask = torch.load(dataset_path / \"attention_mask.pt\", map_location=device)\n    labels = torch.load(dataset_path / \"labels.pt\", map_location=device)\n    \n    # Load metadata\n    with open(dataset_path / \"metadata.json\", 'r') as f:\n        metadata = json.load(f)\n    \n    print(f\"✓ Loaded {input_ids.shape[0]} samples\")\n    print(f\"  - Sequence length: {input_ids.shape[1]}\")\n    print(f\"  - Device: {input_ids.device}\")\n    print(f\"  - Dataset: {metadata.get('dataset_name', 'unknown')}\")\n    print(f\"  - Labels: {metadata.get('num_labels', 2)}\")\n    \n    # Calculate memory footprint\n    total_bytes = (input_ids.element_size() * input_ids.nelement() + \n                   attention_mask.element_size() * attention_mask.nelement() + \n                   labels.element_size() * labels.nelement())\n    print(f\"  - Memory: {total_bytes / 1024**2:.2f} MB\")\n    \n    return input_ids, attention_mask, labels, metadata",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-03T08:51:29.446145Z",
     "iopub.execute_input": "2025-12-03T08:51:29.446366Z",
     "iopub.status.idle": "2025-12-03T08:51:29.461830Z",
     "shell.execute_reply.started": "2025-12-03T08:51:29.446350Z",
     "shell.execute_reply": "2025-12-03T08:51:29.461164Z"
    }
   },
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Model Loading with Accurate Quantization\n\nLoad models in FP32, FP16, or INT8 precision.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def apply_int8_quantization_gpu(model: nn.Module, verbose: bool = True) -> None:\n    \"\"\"\n    Apply symmetric INT8 quantization to model weights (for GPU).\n    \n    Uses per-tensor symmetric quantization:\n    - scale = max(|weight|) / 127\n    - quantized = round(weight / scale)\n    - dequantized = quantized * scale\n    \n    This simulates INT8 precision loss on GPU (true INT8 compute requires special kernels).\n    \"\"\"\n    num_quantized = 0\n    \n    for name, module in model.named_modules():\n        if isinstance(module, nn.Linear):\n            with torch.no_grad():\n                # Quantize weights\n                weight = module.weight.data\n                scale = weight.abs().max() / 127.0\n                \n                if scale > 0:\n                    weight_q = torch.round(weight / scale)\n                    weight_q = torch.clamp(weight_q, -128, 127)\n                    module.weight.data = weight_q * scale\n                \n                # Quantize bias if exists\n                if module.bias is not None:\n                    bias = module.bias.data\n                    scale_bias = bias.abs().max() / 127.0\n                    \n                    if scale_bias > 0:\n                        bias_q = torch.round(bias / scale_bias)\n                        bias_q = torch.clamp(bias_q, -128, 127)\n                        module.bias.data = bias_q * scale_bias\n                \n                num_quantized += 1\n    \n    if verbose:\n        print(f\"  ✓ Quantized {num_quantized} Linear layers to INT8 precision\")\n\n\ndef load_model(model_name: str, precision: str, device: str = \"cuda\", num_labels: int = 2, verbose: bool = True) -> nn.Module:\n    \"\"\"\n    Load model with specified precision.\n    \n    Args:\n        model_name: HuggingFace model identifier\n        precision: 'fp32', 'fp16', or 'int8'\n        device: Device to load model to\n        num_labels: Number of classification labels\n        verbose: Print loading info\n    \n    Returns:\n        model: Loaded model in eval mode\n    \"\"\"\n    if verbose:\n        print(f\"\\nLoading model: {model_name}\")\n        print(f\"  Precision: {precision.upper()}\")\n        print(f\"  Device: {device}\")\n    \n    # Load base model in FP32\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_name,\n        num_labels=num_labels,\n        torch_dtype=torch.float32\n    )\n    \n    # Apply precision conversion\n    if precision == \"fp32\":\n        model = model.to(device)\n    \n    elif precision == \"fp16\":\n        model = model.half()\n        model = model.to(device)\n        if verbose:\n            print(f\"  ✓ Converted to FP16\")\n    \n    elif precision == \"int8\":\n        if device == \"cuda\":\n            model = model.to(device)\n            apply_int8_quantization_gpu(model, verbose=verbose)\n        else:\n            # CPU quantization using PyTorch's dynamic quantization\n            model = torch.quantization.quantize_dynamic(\n                model, {nn.Linear}, dtype=torch.qint8\n            )\n            model = model.to(\"cpu\")\n            if verbose:\n                print(f\"  ✓ Applied dynamic INT8 quantization (CPU)\")\n    else:\n        raise ValueError(f\"Unknown precision: {precision}\")\n    \n    model.eval()\n    \n    # Calculate model size\n    param_size = sum(p.nelement() * p.element_size() for p in model.parameters())\n    buffer_size = sum(b.nelement() * b.element_size() for b in model.buffers())\n    model_size_mb = (param_size + buffer_size) / 1024**2\n    num_params = sum(p.numel() for p in model.parameters())\n    \n    if verbose:\n        print(f\"  ✓ Model loaded successfully\")\n        print(f\"  - Parameters: {num_params:,}\")\n        print(f\"  - Model size: {model_size_mb:.2f} MB\")\n        first_param = next(model.parameters())\n        print(f\"  - Param dtype: {first_param.dtype}\")\n        print(f\"  - Param device: {first_param.device}\")\n    \n    return model\n\n\ndef get_model_info(model: nn.Module) -> Dict:\n    \"\"\"Get model metadata.\"\"\"\n    param_size = sum(p.nelement() * p.element_size() for p in model.parameters())\n    buffer_size = sum(b.nelement() * b.element_size() for b in model.buffers())\n    \n    return {\n        \"model_size_mb\": (param_size + buffer_size) / 1024**2,\n        \"num_parameters\": sum(p.numel() for p in model.parameters()),\n        \"dtype\": str(next(model.parameters()).dtype),\n        \"device\": str(next(model.parameters()).device)\n    }",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-03T08:51:29.463404Z",
     "iopub.execute_input": "2025-12-03T08:51:29.463924Z",
     "iopub.status.idle": "2025-12-03T08:51:29.481936Z",
     "shell.execute_reply.started": "2025-12-03T08:51:29.463906Z",
     "shell.execute_reply": "2025-12-03T08:51:29.481293Z"
    }
   },
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": "## 4. GPU Warmup\n\nStabilize GPU clocks and compile CUDA kernels before measurement.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def warmup_gpu(model: nn.Module, input_ids: torch.Tensor, attention_mask: torch.Tensor, \n               num_steps: int = 50, verbose: bool = True) -> None:\n    \"\"\"\n    Warmup GPU to stabilize clocks and compile kernels.\n    \n    Args:\n        model: Model to warmup\n        input_ids: Sample input tensor\n        attention_mask: Sample attention mask\n        num_steps: Number of warmup iterations\n        verbose: Print progress\n    \"\"\"\n    if verbose:\n        print(f\"\\nWarming up GPU for {num_steps} iterations...\")\n    \n    model.eval()\n    \n    with torch.no_grad():\n        for i in range(num_steps):\n            _ = model(input_ids=input_ids, attention_mask=attention_mask)\n            \n            if verbose and (i + 1) % 10 == 0:\n                print(f\"  Warmup: {i+1}/{num_steps}\", end='\\r')\n    \n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    \n    if verbose:\n        print(f\"\\n  ✓ Warmup complete\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-03T08:51:29.482642Z",
     "iopub.execute_input": "2025-12-03T08:51:29.482839Z",
     "iopub.status.idle": "2025-12-03T08:51:29.499637Z",
     "shell.execute_reply.started": "2025-12-03T08:51:29.482825Z",
     "shell.execute_reply": "2025-12-03T08:51:29.498879Z"
    }
   },
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Power Logger (Asynchronous)\n\nMonitor GPU power draw using nvidia-smi without interfering with timing.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class PowerLogger:\n    \"\"\"\n    Asynchronous power logger using nvidia-smi polling.\n    \"\"\"\n    \n    def __init__(self, sample_interval_ms: int = 100, gpu_id: int = 0, verbose: bool = False):\n        self.sample_interval_ms = sample_interval_ms\n        self.gpu_id = gpu_id\n        self.verbose = verbose\n        self.samples = []\n        self.is_running = False\n        self._lock = threading.Lock()\n        self._reader_thread = None\n        \n        # Verify nvidia-smi is available\n        self._check_nvidia_smi()\n    \n    def _check_nvidia_smi(self) -> None:\n        \"\"\"Check if nvidia-smi is available.\"\"\"\n        try:\n            result = subprocess.run(\n                [\"nvidia-smi\", \"--query-gpu=power.draw\", \"--format=csv,noheader,nounits\", f\"--id={self.gpu_id}\"],\n                capture_output=True, text=True, timeout=5\n            )\n            if result.returncode != 0:\n                raise RuntimeError(f\"nvidia-smi failed: {result.stderr}\")\n            power = float(result.stdout.strip())\n            if self.verbose:\n                print(f\"  ✓ nvidia-smi available, current power: {power:.2f} W\")\n        except Exception as e:\n            raise RuntimeError(f\"nvidia-smi not available: {e}\")\n    \n    def start(self) -> None:\n        \"\"\"Start power logging in background thread.\"\"\"\n        with self._lock:\n            if self.is_running:\n                raise RuntimeError(\"PowerLogger already running\")\n            self.samples = []\n            self.is_running = True\n        \n        self._reader_thread = threading.Thread(target=self._poll_power, daemon=True)\n        self._reader_thread.start()\n        \n        if self.verbose:\n            print(f\"  ✓ Power logger started (interval: {self.sample_interval_ms} ms)\")\n    \n    def _poll_power(self) -> None:\n        \"\"\"Background thread that polls nvidia-smi.\"\"\"\n        interval_sec = self.sample_interval_ms / 1000.0\n        \n        while self.is_running:\n            try:\n                result = subprocess.run(\n                    [\"nvidia-smi\", \"--query-gpu=power.draw\", \"--format=csv,noheader,nounits\", f\"--id={self.gpu_id}\"],\n                    capture_output=True, text=True, timeout=2\n                )\n                \n                if result.returncode == 0:\n                    power = float(result.stdout.strip())\n                    with self._lock:\n                        self.samples.append(power)\n            except:\n                pass\n            \n            time.sleep(interval_sec)\n    \n    def stop(self) -> None:\n        \"\"\"Stop power logging.\"\"\"\n        with self._lock:\n            if not self.is_running:\n                raise RuntimeError(\"PowerLogger not running\")\n            self.is_running = False\n        \n        if self._reader_thread:\n            self._reader_thread.join(timeout=2)\n        \n        if self.verbose:\n            print(f\"  ✓ Power logger stopped ({len(self.samples)} samples)\")\n    \n    def get_samples(self) -> List[float]:\n        \"\"\"Return collected power samples.\"\"\"\n        with self._lock:\n            return self.samples.copy()\n    \n    def __enter__(self):\n        self.start()\n        return self\n    \n    def __exit__(self, *args):\n        if self.is_running:\n            self.stop()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-03T08:51:29.500359Z",
     "iopub.execute_input": "2025-12-03T08:51:29.500646Z",
     "iopub.status.idle": "2025-12-03T08:51:29.521966Z",
     "shell.execute_reply.started": "2025-12-03T08:51:29.500622Z",
     "shell.execute_reply": "2025-12-03T08:51:29.521299Z"
    }
   },
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Timed Inference Benchmark\n\nRun inference with precise timing and CUDA synchronization.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def run_timed_inference(model: nn.Module, input_ids: torch.Tensor, attention_mask: torch.Tensor,\n                       num_iters: int, verbose: bool = True) -> Dict:\n    \"\"\"\n    Run timed inference loop with CUDA synchronization.\n    \n    Args:\n        model: Model in eval mode\n        input_ids: Input tensor [batch_size, seq_len]\n        attention_mask: Attention mask [batch_size, seq_len]\n        num_iters: Number of iterations\n        verbose: Print progress\n    \n    Returns:\n        Dictionary with timing metrics\n    \"\"\"\n    if verbose:\n        print(f\"\\nRunning {num_iters} timed inference iterations...\")\n    \n    model.eval()\n    batch_size = input_ids.shape[0]\n    latencies = []\n    \n    # Ensure clean state\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    \n    start_time = time.perf_counter()\n    \n    with torch.no_grad():\n        for i in range(num_iters):\n            iter_start = time.perf_counter()\n            \n            # Forward pass\n            _ = model(input_ids=input_ids, attention_mask=attention_mask)\n            \n            # Synchronize to ensure completion\n            if torch.cuda.is_available():\n                torch.cuda.synchronize()\n            \n            iter_end = time.perf_counter()\n            latencies.append(iter_end - iter_start)\n            \n            if verbose and (i + 1) % 50 == 0:\n                print(f\"  Progress: {i+1}/{num_iters}\", end='\\r')\n    \n    end_time = time.perf_counter()\n    total_time = end_time - start_time\n    \n    if verbose:\n        print(f\"\\n  ✓ Inference complete: {total_time:.3f}s\")\n    \n    latencies = np.array(latencies)\n    \n    return {\n        \"total_time\": float(total_time),\n        \"num_iters\": num_iters,\n        \"batch_size\": batch_size,\n        \"mean_latency\": float(np.mean(latencies)),\n        \"std_latency\": float(np.std(latencies)),\n        \"min_latency\": float(np.min(latencies)),\n        \"max_latency\": float(np.max(latencies)),\n        \"median_latency\": float(np.median(latencies)),\n        \"throughput\": float(batch_size * num_iters / total_time)\n    }\n\n\ndef compute_accuracy(model: nn.Module, input_ids: torch.Tensor, attention_mask: torch.Tensor,\n                    labels: torch.Tensor, verbose: bool = True) -> Dict:\n    \"\"\"\n    Compute model accuracy on dataset.\n    \n    Returns:\n        Dictionary with accuracy metrics\n    \"\"\"\n    if verbose:\n        print(f\"\\nComputing accuracy...\")\n    \n    model.eval()\n    \n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        predictions = torch.argmax(outputs.logits, dim=-1)\n        correct = (predictions == labels).sum().item()\n        total = labels.shape[0]\n        accuracy = correct / total\n    \n    if verbose:\n        print(f\"  ✓ Accuracy: {accuracy*100:.2f}% ({correct}/{total})\")\n    \n    return {\n        \"accuracy\": float(accuracy),\n        \"num_correct\": int(correct),\n        \"num_samples\": int(total)\n    }",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-03T08:51:29.522923Z",
     "iopub.execute_input": "2025-12-03T08:51:29.523141Z",
     "iopub.status.idle": "2025-12-03T08:51:29.542755Z",
     "shell.execute_reply.started": "2025-12-03T08:51:29.523124Z",
     "shell.execute_reply": "2025-12-03T08:51:29.541972Z"
    }
   },
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Energy Computation\n\nCompute energy metrics from power samples and timing.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_energy_metrics(power_samples: List[float], timing_results: Dict) -> Dict:\n    \"\"\"\n    Compute energy consumption from power samples and timing.\n    \n    Energy (J) = Power (W) × Time (s)\n    \n    IMPORTANT: This computes energy PER SAMPLE, not per batch iteration.\n    - Each iteration processes batch_size samples\n    - Energy per sample = Total energy / (num_iters × batch_size)\n    \n    Args:\n        power_samples: List of power measurements in Watts\n        timing_results: Dictionary from run_timed_inference\n    \n    Returns:\n        Dictionary with energy metrics\n    \"\"\"\n    if len(power_samples) == 0:\n        print(\"  ⚠️  Warning: No power samples collected\")\n        return {\n            \"mean_power_w\": None,\n            \"std_power_w\": None,\n            \"min_power_w\": None,\n            \"max_power_w\": None,\n            \"num_power_samples\": 0,\n            \"total_energy_j\": None,\n            \"energy_per_batch_j\": None,\n            \"energy_per_batch_mj\": None,\n            \"energy_per_sample_j\": None,\n            \"energy_per_sample_mj\": None,\n            \"samples_per_joule\": None\n        }\n    \n    power_array = np.array(power_samples)\n    \n    # Power statistics\n    mean_power = float(np.mean(power_array))\n    std_power = float(np.std(power_array))\n    min_power = float(np.min(power_array))\n    max_power = float(np.max(power_array))\n    \n    # Energy computation\n    total_time = timing_results[\"total_time\"]\n    num_iters = timing_results[\"num_iters\"]\n    batch_size = timing_results[\"batch_size\"]\n    total_samples = num_iters * batch_size\n    \n    # Total energy for all iterations\n    total_energy = mean_power * total_time  # Joules\n    \n    # Energy per batch (per iteration)\n    energy_per_batch = total_energy / num_iters\n    energy_per_batch_mj = energy_per_batch * 1000  # millijoules\n    \n    # Energy per sample (correct metric!)\n    energy_per_sample = total_energy / total_samples\n    energy_per_sample_mj = energy_per_sample * 1000  # millijoules\n    \n    return {\n        \"mean_power_w\": mean_power,\n        \"std_power_w\": std_power,\n        \"min_power_w\": min_power,\n        \"max_power_w\": max_power,\n        \"num_power_samples\": len(power_samples),\n        \"total_energy_j\": total_energy,\n        \"total_samples\": total_samples,\n        # Per-batch metrics (for reference)\n        \"energy_per_batch_j\": energy_per_batch,\n        \"energy_per_batch_mj\": energy_per_batch_mj,\n        # Per-sample metrics (the correct ones!)\n        \"energy_per_sample_j\": energy_per_sample,\n        \"energy_per_sample_mj\": energy_per_sample_mj,\n        \"samples_per_joule\": 1.0 / energy_per_sample if energy_per_sample > 0 else None\n    }",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-03T08:51:29.543575Z",
     "iopub.execute_input": "2025-12-03T08:51:29.543881Z",
     "iopub.status.idle": "2025-12-03T08:51:29.562135Z",
     "shell.execute_reply.started": "2025-12-03T08:51:29.543864Z",
     "shell.execute_reply": "2025-12-03T08:51:29.561398Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Complete Measurement Function\n\nOrchestrate the complete measurement pipeline for one trial.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def run_single_trial(model_name: str, precision: str, input_ids: torch.Tensor, \n                    attention_mask: torch.Tensor, labels: torch.Tensor,\n                    num_iters: int = 300, warmup_iters: int = 50, \n                    power_interval_ms: int = 100, device: str = \"cuda\",\n                    trial_num: int = 1, verbose: bool = True) -> Dict:\n    \"\"\"\n    Run a complete measurement trial for one precision level.\n    \n    Pipeline:\n    1. Load model\n    2. Warmup GPU\n    3. Start power logging\n    4. Run timed inference\n    5. Stop power logging\n    6. Compute accuracy\n    7. Compute energy metrics\n    8. Collect memory stats\n    \n    Returns:\n        Dictionary with all metrics\n    \"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"TRIAL {trial_num}: {precision.upper()}\")\n    print(f\"{'='*70}\")\n    \n    # Reset GPU memory\n    if torch.cuda.is_available():\n        torch.cuda.reset_peak_memory_stats()\n        torch.cuda.empty_cache()\n    \n    # 1. Load model\n    model = load_model(model_name, precision, device, num_labels=2, verbose=verbose)\n    model_info = get_model_info(model)\n    \n    # 2. Warmup\n    warmup_gpu(model, input_ids, attention_mask, num_steps=warmup_iters, verbose=verbose)\n    \n    # 3. Start power logging\n    print(f\"\\nStarting measurement...\")\n    power_logger = PowerLogger(sample_interval_ms=power_interval_ms, verbose=verbose)\n    power_logger.start()\n    time.sleep(0.5)  # Let logger stabilize\n    \n    # 4. Run timed inference\n    timing_results = run_timed_inference(model, input_ids, attention_mask, num_iters, verbose=verbose)\n    \n    # 5. Stop power logging\n    time.sleep(0.5)  # Capture trailing samples\n    power_logger.stop()\n    power_samples = power_logger.get_samples()\n    \n    # 6. Compute accuracy\n    accuracy_results = compute_accuracy(model, input_ids, attention_mask, labels, verbose=verbose)\n    \n    # 7. Compute energy\n    energy_results = compute_energy_metrics(power_samples, timing_results)\n    \n    # 8. Memory stats\n    memory_stats = {}\n    if torch.cuda.is_available():\n        memory_stats = {\n            \"peak_memory_mb\": torch.cuda.max_memory_allocated() / 1024**2,\n            \"allocated_memory_mb\": torch.cuda.memory_allocated() / 1024**2,\n            \"reserved_memory_mb\": torch.cuda.memory_reserved() / 1024**2\n        }\n    \n    # Combine all results\n    results = {\n        \"trial\": trial_num,\n        \"precision\": precision,\n        \"model_name\": model_name,\n        \"timestamp\": datetime.now().isoformat(),\n    }\n    results.update(timing_results)\n    results.update(accuracy_results)\n    results.update(energy_results)\n    results.update(memory_stats)\n    results.update(model_info)\n    \n    # Print summary\n    print(f\"\\n{'='*70}\")\n    print(f\"TRIAL {trial_num} SUMMARY: {precision.upper()}\")\n    print(f\"{'='*70}\")\n    print(f\"Batch size:    {timing_results['batch_size']} samples\")\n    print(f\"Iterations:    {timing_results['num_iters']}\")\n    print(f\"Total samples: {energy_results.get('total_samples', timing_results['batch_size'] * timing_results['num_iters'])}\")\n    print(f\"Latency:       {timing_results['mean_latency']*1000:.3f} ms/batch (± {timing_results['std_latency']*1000:.3f} ms)\")\n    print(f\"Throughput:    {timing_results['throughput']:.2f} samples/s\")\n    print(f\"Accuracy:      {accuracy_results['accuracy']*100:.2f}%\")\n    if energy_results['mean_power_w'] is not None:\n        print(f\"Power:         {energy_results['mean_power_w']:.2f} W (± {energy_results['std_power_w']:.2f} W)\")\n        print(f\"Total Energy:  {energy_results['total_energy_j']:.3f} J\")\n        print(f\"Energy/batch:  {energy_results['energy_per_batch_mj']:.3f} mJ\")\n        print(f\"Energy/sample: {energy_results['energy_per_sample_mj']:.3f} mJ  ← (correct per-sample metric)\")\n    if memory_stats:\n        print(f\"Peak Memory:   {memory_stats['peak_memory_mb']:.2f} MB\")\n    print(f\"{'='*70}\")\n    \n    # Cleanup\n    del model\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    return results",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-03T08:51:29.562957Z",
     "iopub.execute_input": "2025-12-03T08:51:29.563955Z",
     "iopub.status.idle": "2025-12-03T08:51:29.579669Z",
     "shell.execute_reply.started": "2025-12-03T08:51:29.563931Z",
     "shell.execute_reply": "2025-12-03T08:51:29.579012Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Multi-Trial Runner\n\nRun multiple trials for statistical significance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def run_multi_trial_experiment(model_name: str, precision: str, dataset_path: str,\n                              num_trials: int = 5, num_iters: int = 300,\n                              warmup_iters: int = 50, power_interval_ms: int = 100,\n                              device: str = \"cuda\") -> Tuple[List[Dict], Dict]:\n    \"\"\"\n    Run multiple trials for one precision level and aggregate results.\n    \n    Returns:\n        trial_results: List of per-trial result dictionaries\n        aggregated: Dictionary with mean/std across trials\n    \"\"\"\n    print(f\"\\n{'#'*70}\")\n    print(f\"# MULTI-TRIAL EXPERIMENT: {precision.upper()}\")\n    print(f\"# Number of trials: {num_trials}\")\n    print(f\"# Iterations per trial: {num_iters}\")\n    print(f\"{'#'*70}\")\n    \n    # Load dataset once (stays on GPU)\n    input_ids, attention_mask, labels, metadata = load_pretokenized_dataset(dataset_path, device)\n    \n    # Run trials\n    trial_results = []\n    for trial in range(1, num_trials + 1):\n        result = run_single_trial(\n            model_name=model_name,\n            precision=precision,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels,\n            num_iters=num_iters,\n            warmup_iters=warmup_iters,\n            power_interval_ms=power_interval_ms,\n            device=device,\n            trial_num=trial,\n            verbose=True\n        )\n        trial_results.append(result)\n    \n    # Aggregate results\n    aggregated = aggregate_trials(trial_results)\n    print_aggregated_results(aggregated, precision)\n    \n    return trial_results, aggregated\n\n\ndef aggregate_trials(trial_results: List[Dict]) -> Dict:\n    \"\"\"Compute mean and std across trials.\"\"\"\n    numeric_keys = [\n        \"mean_latency\", \"std_latency\", \"throughput\", \"accuracy\",\n        \"mean_power_w\", \"std_power_w\", \"total_energy_j\", \n        \"energy_per_batch_j\", \"energy_per_batch_mj\",\n        \"energy_per_sample_j\", \"energy_per_sample_mj\",\n        \"peak_memory_mb\"\n    ]\n    \n    aggregated = {\n        \"precision\": trial_results[0][\"precision\"],\n        \"model_name\": trial_results[0][\"model_name\"],\n        \"num_trials\": len(trial_results),\n        \"batch_size\": trial_results[0][\"batch_size\"],\n        \"total_samples\": trial_results[0].get(\"total_samples\", trial_results[0][\"batch_size\"] * trial_results[0][\"num_iters\"]),\n        \"model_size_mb\": trial_results[0][\"model_size_mb\"],\n        \"num_parameters\": trial_results[0][\"num_parameters\"]\n    }\n    \n    for key in numeric_keys:\n        if key in trial_results[0] and trial_results[0][key] is not None:\n            values = [r[key] for r in trial_results if key in r and r[key] is not None]\n            if values:\n                aggregated[f\"{key}_mean\"] = float(np.mean(values))\n                aggregated[f\"{key}_std\"] = float(np.std(values))\n                aggregated[f\"{key}_min\"] = float(np.min(values))\n                aggregated[f\"{key}_max\"] = float(np.max(values))\n    \n    return aggregated\n\n\ndef print_aggregated_results(agg: Dict, precision: str) -> None:\n    \"\"\"Pretty print aggregated results.\"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"AGGREGATED RESULTS: {precision.upper()}\")\n    print(f\"Trials: {agg['num_trials']}\")\n    print(f\"Batch size: {agg['batch_size']} samples\")\n    print(f\"Total samples per trial: {agg.get('total_samples', 'N/A')}\")\n    print(f\"{'='*70}\")\n    \n    if 'mean_latency_mean' in agg:\n        print(f\"\\nLatency (per batch):\")\n        print(f\"  {agg['mean_latency_mean']*1000:.3f} ± {agg['mean_latency_std']*1000:.3f} ms\")\n    \n    if 'throughput_mean' in agg:\n        print(f\"\\nThroughput:\")\n        print(f\"  {agg['throughput_mean']:.2f} ± {agg['throughput_std']:.2f} samples/s\")\n    \n    if 'accuracy_mean' in agg:\n        print(f\"\\nAccuracy:\")\n        print(f\"  {agg['accuracy_mean']*100:.2f} ± {agg['accuracy_std']*100:.2f}%\")\n    \n    if 'mean_power_w_mean' in agg:\n        print(f\"\\nPower:\")\n        print(f\"  {agg['mean_power_w_mean']:.2f} ± {agg['mean_power_w_std']:.2f} W\")\n    \n    if 'energy_per_batch_mj_mean' in agg:\n        print(f\"\\nEnergy per Batch:\")\n        print(f\"  {agg['energy_per_batch_mj_mean']:.3f} ± {agg['energy_per_batch_mj_std']:.3f} mJ\")\n    \n    if 'energy_per_sample_mj_mean' in agg:\n        print(f\"\\nEnergy per Sample (CORRECT METRIC):\")\n        print(f\"  {agg['energy_per_sample_mj_mean']:.3f} ± {agg['energy_per_sample_mj_std']:.3f} mJ\")\n    \n    if 'peak_memory_mb_mean' in agg:\n        print(f\"\\nMemory:\")\n        print(f\"  Model size: {agg['model_size_mb']:.2f} MB\")\n        print(f\"  Peak GPU: {agg['peak_memory_mb_mean']:.2f} ± {agg['peak_memory_mb_std']:.2f} MB\")\n    \n    print(f\"{'='*70}\\n\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-03T08:51:29.581806Z",
     "iopub.execute_input": "2025-12-03T08:51:29.581994Z",
     "iopub.status.idle": "2025-12-03T08:51:29.597587Z",
     "shell.execute_reply.started": "2025-12-03T08:51:29.581979Z",
     "shell.execute_reply": "2025-12-03T08:51:29.596859Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Results Saving\n\nSave results in CSV and JSON formats.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def save_results(trial_results: List[Dict], aggregated: Dict, \n                precision: str, output_dir: str = \"./results\") -> None:\n    \"\"\"\n    Save trial and aggregated results.\n    \n    Creates:\n    - results/{precision}_trials.csv: Per-trial results\n    - results/{precision}_aggregated.json: Aggregated statistics\n    \"\"\"\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Save detailed trial results\n    trials_df = pd.DataFrame(trial_results)\n    trials_path = output_dir / f\"{precision}_trials.csv\"\n    trials_df.to_csv(trials_path, index=False)\n    print(f\"\\n✓ Saved trial results: {trials_path}\")\n    \n    # Save aggregated results\n    agg_path = output_dir / f\"{precision}_aggregated.json\"\n    with open(agg_path, 'w') as f:\n        json.dump(aggregated, f, indent=2)\n    print(f\"✓ Saved aggregated results: {agg_path}\")\n\n\ndef save_comparison_table(all_aggregated: List[Dict], output_dir: str = \"./results\") -> pd.DataFrame:\n    \"\"\"\n    Create and save comparison table across all precisions.\n    \"\"\"\n    output_dir = Path(output_dir)\n    \n    comparison_df = pd.DataFrame(all_aggregated)\n    comparison_path = output_dir / \"comparison_all_precisions.csv\"\n    comparison_df.to_csv(comparison_path, index=False)\n    print(f\"\\n✓ Saved comparison table: {comparison_path}\")\n    \n    return comparison_df",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-03T08:51:29.598479Z",
     "iopub.execute_input": "2025-12-03T08:51:29.598993Z",
     "iopub.status.idle": "2025-12-03T08:51:29.614110Z",
     "shell.execute_reply.started": "2025-12-03T08:51:29.598971Z",
     "shell.execute_reply": "2025-12-03T08:51:29.613483Z"
    }
   },
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Main Experiment Runner\n\nRun experiments for all precision levels.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Configuration - Use current working directory to find dataset\nimport os\n\n# Get current working directory and find dataset\ncwd = os.getcwd()\nprint(f\"Current working directory: {cwd}\")\n\n# Try multiple paths relative to cwd\npossible_paths = [\n    Path(cwd) / \"..\" / \"datasets\" / \"tokenized_data\",  # From notebooks/\n    Path(cwd) / \"datasets\" / \"tokenized_data\",          # From repo root\n    Path(cwd) / \"..\" / \"..\" / \"datasets\" / \"tokenized_data\",  # From deeper nesting\n    Path(\"/kaggle/working/energy_aware_quantization/datasets/tokenized_data\"),  # Kaggle path\n]\n\ndataset_path = None\nfor path in possible_paths:\n    abs_path = path.resolve()\n    print(f\"Trying: {abs_path}\")\n    if abs_path.exists() and (abs_path / \"input_ids.pt\").exists():\n        dataset_path = str(abs_path)\n        break\n\nif dataset_path is None:\n    # Last resort: search upward from cwd\n    current = Path(cwd)\n    for _ in range(5):  # Search up to 5 levels up\n        test_path = current / \"datasets\" / \"tokenized_data\"\n        print(f\"Trying: {test_path.resolve()}\")\n        if test_path.exists() and (test_path / \"input_ids.pt\").exists():\n            dataset_path = str(test_path.resolve())\n            break\n        current = current.parent\n    \nif dataset_path is None:\n    raise FileNotFoundError(\n        f\"Could not find datasets/tokenized_data directory.\\n\"\n        f\"Searched from: {cwd}\\n\"\n        f\"Please ensure the dataset exists or update the dataset_path manually.\"\n    )\n\nprint(f\"✓ Found dataset at: {dataset_path}\")\n\n# Set output directory\noutput_path = Path(cwd) / \"..\" / \"results\"\nif not output_path.parent.exists():\n    output_path = Path(cwd) / \"results\"\noutput_dir = str(output_path.resolve())\n\nCONFIG = {\n    \"model_name\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n    \"dataset_path\": dataset_path,\n    \"precisions\": [\"fp32\", \"fp16\", \"int8\"],  # Precision levels to test\n    \"num_trials\": 5,  # Number of trials per precision\n    \"num_iters\": 300,  # Inference iterations per trial\n    \"warmup_iters\": 50,  # Warmup iterations\n    \"power_interval_ms\": 100,  # Power sampling interval\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    \"output_dir\": output_dir\n}\n\nprint(\"\\nExperiment Configuration:\")\nprint(json.dumps(CONFIG, indent=2))",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-03T08:51:29.614911Z",
     "iopub.execute_input": "2025-12-03T08:51:29.615178Z",
     "iopub.status.idle": "2025-12-03T08:51:29.632846Z",
     "shell.execute_reply.started": "2025-12-03T08:51:29.615156Z",
     "shell.execute_reply": "2025-12-03T08:51:29.632283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Current working directory: /kaggle/working\nTrying: /kaggle/datasets/tokenized_data\nTrying: /kaggle/working/datasets/tokenized_data\nTrying: /datasets/tokenized_data\nTrying: /kaggle/working/energy_aware_quantization/datasets/tokenized_data\n✓ Found dataset at: /kaggle/working/energy_aware_quantization/datasets/tokenized_data\n\nExperiment Configuration:\n{\n  \"model_name\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n  \"dataset_path\": \"/kaggle/working/energy_aware_quantization/datasets/tokenized_data\",\n  \"precisions\": [\n    \"fp32\",\n    \"fp16\",\n    \"int8\"\n  ],\n  \"num_trials\": 5,\n  \"num_iters\": 300,\n  \"warmup_iters\": 50,\n  \"power_interval_ms\": 100,\n  \"device\": \"cuda\",\n  \"output_dir\": \"/kaggle/results\"\n}\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "source": "# Run experiments for all precisions\nall_trial_results = {}\nall_aggregated = []\n\nfor precision in CONFIG[\"precisions\"]:\n    trial_results, aggregated = run_multi_trial_experiment(\n        model_name=CONFIG[\"model_name\"],\n        precision=precision,\n        dataset_path=CONFIG[\"dataset_path\"],\n        num_trials=CONFIG[\"num_trials\"],\n        num_iters=CONFIG[\"num_iters\"],\n        warmup_iters=CONFIG[\"warmup_iters\"],\n        power_interval_ms=CONFIG[\"power_interval_ms\"],\n        device=CONFIG[\"device\"]\n    )\n    \n    # Save results\n    save_results(trial_results, aggregated, precision, CONFIG[\"output_dir\"])\n    \n    # Store for comparison\n    all_trial_results[precision] = trial_results\n    all_aggregated.append(aggregated)\n\n# Save comparison table\ncomparison_df = save_comparison_table(all_aggregated, CONFIG[\"output_dir\"])\n\nprint(\"\\n\" + \"#\"*70)\nprint(\"# ALL EXPERIMENTS COMPLETE\")\nprint(\"#\"*70)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-03T08:51:29.633549Z",
     "iopub.execute_input": "2025-12-03T08:51:29.634064Z",
     "iopub.status.idle": "2025-12-03T08:59:45.020474Z",
     "shell.execute_reply.started": "2025-12-03T08:51:29.634039Z",
     "shell.execute_reply": "2025-12-03T08:59:45.019678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "\n######################################################################\n# MULTI-TRIAL EXPERIMENT: FP32\n# Number of trials: 5\n# Iterations per trial: 300\n######################################################################\n\nLoading dataset from: /kaggle/working/energy_aware_quantization/datasets/tokenized_data\n✓ Loaded 50 samples\n  - Sequence length: 128\n  - Device: cuda:0\n  - Dataset: sst2\n  - Labels: 2\n  - Memory: 0.10 MB\n\n======================================================================\nTRIAL 1: FP32\n======================================================================\n\nLoading model: distilbert-base-uncased-finetuned-sst-2-english\n  Precision: FP32\n  Device: cuda\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "048d353d878340debe7970b864597ba9"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "2025-12-03 08:51:33.281995: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764751893.458113      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764751893.507547      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ],
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ],
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ],
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ],
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ],
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76f69fa7c88c43798ff1066b58bd545a"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "  ✓ Model loaded successfully\n  - Parameters: 66,955,010\n  - Model size: 255.42 MB\n  - Param dtype: torch.float32\n  - Param device: cuda:0\n\nWarming up GPU for 50 iterations...\n  Warmup: 50/50\n  ✓ Warmup complete\n\nStarting measurement...\n  ✓ nvidia-smi available, current power: 75.53 W\n  ✓ Power logger started (interval: 100 ms)\n\nRunning 300 timed inference iterations...\n  Progress: 300/300\n  ✓ Inference complete: 25.936s\n  ✓ Power logger stopped (214 samples)\n\nComputing accuracy...\n  ✓ Accuracy: 86.00% (43/50)\n\n======================================================================\nTRIAL 1 SUMMARY: FP32\n======================================================================\nLatency:       86.447 ms (± 0.166 ms)\nThroughput:    578.35 samples/s\nAccuracy:      86.00%\nPower:         221.43 W (± 35.55 W)\nEnergy:        19143.190 mJ/inference\nTotal Energy:  5742.957 J\nPeak Memory:   476.35 MB\n======================================================================\n\n======================================================================\nTRIAL 2: FP32\n======================================================================\n\nLoading model: distilbert-base-uncased-finetuned-sst-2-english\n  Precision: FP32\n  Device: cuda\n  ✓ Model loaded successfully\n  - Parameters: 66,955,010\n  - Model size: 255.42 MB\n  - Param dtype: torch.float32\n  - Param device: cuda:0\n\nWarming up GPU for 50 iterations...\n  Warmup: 50/50\n  ✓ Warmup complete\n\nStarting measurement...\n  ✓ nvidia-smi available, current power: 85.13 W\n  ✓ Power logger started (interval: 100 ms)\n\nRunning 300 timed inference iterations...\n  Progress: 300/300\n  ✓ Inference complete: 26.042s\n  ✓ Power logger stopped (214 samples)\n\nComputing accuracy...\n  ✓ Accuracy: 86.00% (43/50)\n\n======================================================================\nTRIAL 2 SUMMARY: FP32\n======================================================================\nLatency:       86.801 ms (± 0.350 ms)\nThroughput:    575.99 samples/s\nAccuracy:      86.00%\nPower:         227.56 W (± 35.57 W)\nEnergy:        19753.682 mJ/inference\nTotal Energy:  5926.105 J\nPeak Memory:   475.85 MB\n======================================================================\n\n======================================================================\nTRIAL 3: FP32\n======================================================================\n\nLoading model: distilbert-base-uncased-finetuned-sst-2-english\n  Precision: FP32\n  Device: cuda\n  ✓ Model loaded successfully\n  - Parameters: 66,955,010\n  - Model size: 255.42 MB\n  - Param dtype: torch.float32\n  - Param device: cuda:0\n\nWarming up GPU for 50 iterations...\n  Warmup: 50/50\n  ✓ Warmup complete\n\nStarting measurement...\n  ✓ nvidia-smi available, current power: 92.99 W\n  ✓ Power logger started (interval: 100 ms)\n\nRunning 300 timed inference iterations...\n  Progress: 300/300\n  ✓ Inference complete: 26.278s\n  ✓ Power logger stopped (215 samples)\n\nComputing accuracy...\n  ✓ Accuracy: 86.00% (43/50)\n\n======================================================================\nTRIAL 3 SUMMARY: FP32\n======================================================================\nLatency:       87.587 ms (± 0.679 ms)\nThroughput:    570.81 samples/s\nAccuracy:      86.00%\nPower:         229.24 W (± 35.08 W)\nEnergy:        20079.795 mJ/inference\nTotal Energy:  6023.938 J\nPeak Memory:   475.85 MB\n======================================================================\n\n======================================================================\nTRIAL 4: FP32\n======================================================================\n\nLoading model: distilbert-base-uncased-finetuned-sst-2-english\n  Precision: FP32\n  Device: cuda\n  ✓ Model loaded successfully\n  - Parameters: 66,955,010\n  - Model size: 255.42 MB\n  - Param dtype: torch.float32\n  - Param device: cuda:0\n\nWarming up GPU for 50 iterations...\n  Warmup: 50/50\n  ✓ Warmup complete\n\nStarting measurement...\n  ✓ nvidia-smi available, current power: 78.75 W\n  ✓ Power logger started (interval: 100 ms)\n\nRunning 300 timed inference iterations...\n  Progress: 300/300\n  ✓ Inference complete: 26.334s\n  ✓ Power logger stopped (215 samples)\n\nComputing accuracy...\n  ✓ Accuracy: 86.00% (43/50)\n\n======================================================================\nTRIAL 4 SUMMARY: FP32\n======================================================================\nLatency:       87.774 ms (± 0.606 ms)\nThroughput:    569.60 samples/s\nAccuracy:      86.00%\nPower:         228.19 W (± 34.78 W)\nEnergy:        20030.893 mJ/inference\nTotal Energy:  6009.268 J\nPeak Memory:   475.85 MB\n======================================================================\n\n======================================================================\nTRIAL 5: FP32\n======================================================================\n\nLoading model: distilbert-base-uncased-finetuned-sst-2-english\n  Precision: FP32\n  Device: cuda\n  ✓ Model loaded successfully\n  - Parameters: 66,955,010\n  - Model size: 255.42 MB\n  - Param dtype: torch.float32\n  - Param device: cuda:0\n\nWarming up GPU for 50 iterations...\n  Warmup: 50/50\n  ✓ Warmup complete\n\nStarting measurement...\n  ✓ nvidia-smi available, current power: 94.92 W\n  ✓ Power logger started (interval: 100 ms)\n\nRunning 300 timed inference iterations...\n  Progress: 300/300\n  ✓ Inference complete: 26.278s\n  ✓ Power logger stopped (214 samples)\n\nComputing accuracy...\n  ✓ Accuracy: 86.00% (43/50)\n\n======================================================================\nTRIAL 5 SUMMARY: FP32\n======================================================================\nLatency:       87.587 ms (± 0.639 ms)\nThroughput:    570.81 samples/s\nAccuracy:      86.00%\nPower:         228.72 W (± 34.82 W)\nEnergy:        20034.890 mJ/inference\nTotal Energy:  6010.467 J\nPeak Memory:   475.85 MB\n======================================================================\n\n======================================================================\nAGGREGATED RESULTS: FP32\nTrials: 5\n======================================================================\n\nLatency:\n  87.239 ± 0.519 ms\n\nThroughput:\n  573.11 ± 3.43 samples/s\n\nAccuracy:\n  86.00 ± 0.00%\n\nPower:\n  227.03 ± 2.85 W\n\nEnergy per Inference:\n  19808.490 ± 352.123 mJ\n\nMemory:\n  Model size: 255.42 MB\n  Peak GPU: 475.95 ± 0.20 MB\n======================================================================\n\n\n✓ Saved trial results: /kaggle/results/fp32_trials.csv\n✓ Saved aggregated results: /kaggle/results/fp32_aggregated.json\n\n######################################################################\n# MULTI-TRIAL EXPERIMENT: FP16\n# Number of trials: 5\n# Iterations per trial: 300\n######################################################################\n\nLoading dataset from: /kaggle/working/energy_aware_quantization/datasets/tokenized_data\n✓ Loaded 50 samples\n  - Sequence length: 128\n  - Device: cuda:0\n  - Dataset: sst2\n  - Labels: 2\n  - Memory: 0.10 MB\n\n======================================================================\nTRIAL 1: FP16\n======================================================================\n\nLoading model: distilbert-base-uncased-finetuned-sst-2-english\n  Precision: FP16\n  Device: cuda\n  ✓ Converted to FP16\n  ✓ Model loaded successfully\n  - Parameters: 66,955,010\n  - Model size: 127.71 MB\n  - Param dtype: torch.float16\n  - Param device: cuda:0\n\nWarming up GPU for 50 iterations...\n  Warmup: 50/50\n  ✓ Warmup complete\n\nStarting measurement...\n  ✓ nvidia-smi available, current power: 94.24 W\n  ✓ Power logger started (interval: 100 ms)\n\nRunning 300 timed inference iterations...\n  Progress: 300/300\n  ✓ Inference complete: 24.983s\n  ✓ Power logger stopped (206 samples)\n\nComputing accuracy...\n  ✓ Accuracy: 86.00% (43/50)\n\n======================================================================\nTRIAL 1 SUMMARY: FP16\n======================================================================\nLatency:       83.269 ms (± 0.145 ms)\nThroughput:    600.42 samples/s\nAccuracy:      86.00%\nPower:         227.42 W (± 34.82 W)\nEnergy:        18938.655 mJ/inference\nTotal Energy:  5681.597 J\nPeak Memory:   245.04 MB\n======================================================================\n\n======================================================================\nTRIAL 2: FP16\n======================================================================\n\nLoading model: distilbert-base-uncased-finetuned-sst-2-english\n  Precision: FP16\n  Device: cuda\n  ✓ Converted to FP16\n  ✓ Model loaded successfully\n  - Parameters: 66,955,010\n  - Model size: 127.71 MB\n  - Param dtype: torch.float16\n  - Param device: cuda:0\n\nWarming up GPU for 50 iterations...\n  Warmup: 50/50\n  ✓ Warmup complete\n\nStarting measurement...\n  ✓ nvidia-smi available, current power: 95.03 W\n  ✓ Power logger started (interval: 100 ms)\n\nRunning 300 timed inference iterations...\n  Progress: 300/300\n  ✓ Inference complete: 24.980s\n  ✓ Power logger stopped (206 samples)\n\nComputing accuracy...\n  ✓ Accuracy: 86.00% (43/50)\n\n======================================================================\nTRIAL 2 SUMMARY: FP16\n======================================================================\nLatency:       83.260 ms (± 0.135 ms)\nThroughput:    600.47 samples/s\nAccuracy:      86.00%\nPower:         227.04 W (± 34.84 W)\nEnergy:        18905.239 mJ/inference\nTotal Energy:  5671.572 J\nPeak Memory:   245.04 MB\n======================================================================\n\n======================================================================\nTRIAL 3: FP16\n======================================================================\n\nLoading model: distilbert-base-uncased-finetuned-sst-2-english\n  Precision: FP16\n  Device: cuda\n  ✓ Converted to FP16\n  ✓ Model loaded successfully\n  - Parameters: 66,955,010\n  - Model size: 127.71 MB\n  - Param dtype: torch.float16\n  - Param device: cuda:0\n\nWarming up GPU for 50 iterations...\n  Warmup: 50/50\n  ✓ Warmup complete\n\nStarting measurement...\n  ✓ nvidia-smi available, current power: 93.05 W\n  ✓ Power logger started (interval: 100 ms)\n\nRunning 300 timed inference iterations...\n  Progress: 300/300\n  ✓ Inference complete: 24.981s\n  ✓ Power logger stopped (206 samples)\n\nComputing accuracy...\n  ✓ Accuracy: 86.00% (43/50)\n\n======================================================================\nTRIAL 3 SUMMARY: FP16\n======================================================================\nLatency:       83.262 ms (± 0.135 ms)\nThroughput:    600.46 samples/s\nAccuracy:      86.00%\nPower:         227.26 W (± 34.78 W)\nEnergy:        18923.826 mJ/inference\nTotal Energy:  5677.148 J\nPeak Memory:   245.04 MB\n======================================================================\n\n======================================================================\nTRIAL 4: FP16\n======================================================================\n\nLoading model: distilbert-base-uncased-finetuned-sst-2-english\n  Precision: FP16\n  Device: cuda\n  ✓ Converted to FP16\n  ✓ Model loaded successfully\n  - Parameters: 66,955,010\n  - Model size: 127.71 MB\n  - Param dtype: torch.float16\n  - Param device: cuda:0\n\nWarming up GPU for 50 iterations...\n  Warmup: 50/50\n  ✓ Warmup complete\n\nStarting measurement...\n  ✓ nvidia-smi available, current power: 94.79 W\n  ✓ Power logger started (interval: 100 ms)\n\nRunning 300 timed inference iterations...\n  Progress: 300/300\n  ✓ Inference complete: 24.975s\n  ✓ Power logger stopped (206 samples)\n\nComputing accuracy...\n  ✓ Accuracy: 86.00% (43/50)\n\n======================================================================\nTRIAL 4 SUMMARY: FP16\n======================================================================\nLatency:       83.242 ms (± 0.138 ms)\nThroughput:    600.61 samples/s\nAccuracy:      86.00%\nPower:         226.89 W (± 34.57 W)\nEnergy:        18888.277 mJ/inference\nTotal Energy:  5666.483 J\nPeak Memory:   245.04 MB\n======================================================================\n\n======================================================================\nTRIAL 5: FP16\n======================================================================\n\nLoading model: distilbert-base-uncased-finetuned-sst-2-english\n  Precision: FP16\n  Device: cuda\n  ✓ Converted to FP16\n  ✓ Model loaded successfully\n  - Parameters: 66,955,010\n  - Model size: 127.71 MB\n  - Param dtype: torch.float16\n  - Param device: cuda:0\n\nWarming up GPU for 50 iterations...\n  Warmup: 50/50\n  ✓ Warmup complete\n\nStarting measurement...\n  ✓ nvidia-smi available, current power: 94.29 W\n  ✓ Power logger started (interval: 100 ms)\n\nRunning 300 timed inference iterations...\n  Progress: 300/300\n  ✓ Inference complete: 24.972s\n  ✓ Power logger stopped (206 samples)\n\nComputing accuracy...\n  ✓ Accuracy: 86.00% (43/50)\n\n======================================================================\nTRIAL 5 SUMMARY: FP16\n======================================================================\nLatency:       83.234 ms (± 0.136 ms)\nThroughput:    600.67 samples/s\nAccuracy:      86.00%\nPower:         227.13 W (± 34.74 W)\nEnergy:        18906.808 mJ/inference\nTotal Energy:  5672.042 J\nPeak Memory:   245.04 MB\n======================================================================\n\n======================================================================\nAGGREGATED RESULTS: FP16\nTrials: 5\n======================================================================\n\nLatency:\n  83.253 ± 0.013 ms\n\nThroughput:\n  600.53 ± 0.09 samples/s\n\nAccuracy:\n  86.00 ± 0.00%\n\nPower:\n  227.15 ± 0.18 W\n\nEnergy per Inference:\n  18912.561 ± 17.229 mJ\n\nMemory:\n  Model size: 127.71 MB\n  Peak GPU: 245.04 ± 0.00 MB\n======================================================================\n\n\n✓ Saved trial results: /kaggle/results/fp16_trials.csv\n✓ Saved aggregated results: /kaggle/results/fp16_aggregated.json\n\n######################################################################\n# MULTI-TRIAL EXPERIMENT: INT8\n# Number of trials: 5\n# Iterations per trial: 300\n######################################################################\n\nLoading dataset from: /kaggle/working/energy_aware_quantization/datasets/tokenized_data\n✓ Loaded 50 samples\n  - Sequence length: 128\n  - Device: cuda:0\n  - Dataset: sst2\n  - Labels: 2\n  - Memory: 0.10 MB\n\n======================================================================\nTRIAL 1: INT8\n======================================================================\n\nLoading model: distilbert-base-uncased-finetuned-sst-2-english\n  Precision: INT8\n  Device: cuda\n  ✓ Quantized 38 Linear layers to INT8 precision\n  ✓ Model loaded successfully\n  - Parameters: 66,955,010\n  - Model size: 255.42 MB\n  - Param dtype: torch.float32\n  - Param device: cuda:0\n\nWarming up GPU for 50 iterations...\n  Warmup: 50/50\n  ✓ Warmup complete\n\nStarting measurement...\n  ✓ nvidia-smi available, current power: 94.60 W\n  ✓ Power logger started (interval: 100 ms)\n\nRunning 300 timed inference iterations...\n  Progress: 300/300\n  ✓ Inference complete: 26.127s\n  ✓ Power logger stopped (214 samples)\n\nComputing accuracy...\n  ✓ Accuracy: 88.00% (44/50)\n\n======================================================================\nTRIAL 1 SUMMARY: INT8\n======================================================================\nLatency:       87.083 ms (± 0.645 ms)\nThroughput:    574.12 samples/s\nAccuracy:      88.00%\nPower:         229.99 W (± 35.29 W)\nEnergy:        20029.545 mJ/inference\nTotal Energy:  6008.864 J\nPeak Memory:   475.85 MB\n======================================================================\n\n======================================================================\nTRIAL 2: INT8\n======================================================================\n\nLoading model: distilbert-base-uncased-finetuned-sst-2-english\n  Precision: INT8\n  Device: cuda\n  ✓ Quantized 38 Linear layers to INT8 precision\n  ✓ Model loaded successfully\n  - Parameters: 66,955,010\n  - Model size: 255.42 MB\n  - Param dtype: torch.float32\n  - Param device: cuda:0\n\nWarming up GPU for 50 iterations...\n  Warmup: 50/50\n  ✓ Warmup complete\n\nStarting measurement...\n  ✓ nvidia-smi available, current power: 92.07 W\n  ✓ Power logger started (interval: 100 ms)\n\nRunning 300 timed inference iterations...\n  Progress: 300/300\n  ✓ Inference complete: 26.150s\n  ✓ Power logger stopped (214 samples)\n\nComputing accuracy...\n  ✓ Accuracy: 88.00% (44/50)\n\n======================================================================\nTRIAL 2 SUMMARY: INT8\n======================================================================\nLatency:       87.160 ms (± 0.589 ms)\nThroughput:    573.61 samples/s\nAccuracy:      88.00%\nPower:         229.08 W (± 35.45 W)\nEnergy:        19968.120 mJ/inference\nTotal Energy:  5990.436 J\nPeak Memory:   475.85 MB\n======================================================================\n\n======================================================================\nTRIAL 3: INT8\n======================================================================\n\nLoading model: distilbert-base-uncased-finetuned-sst-2-english\n  Precision: INT8\n  Device: cuda\n  ✓ Quantized 38 Linear layers to INT8 precision\n  ✓ Model loaded successfully\n  - Parameters: 66,955,010\n  - Model size: 255.42 MB\n  - Param dtype: torch.float32\n  - Param device: cuda:0\n\nWarming up GPU for 50 iterations...\n  Warmup: 50/50\n  ✓ Warmup complete\n\nStarting measurement...\n  ✓ nvidia-smi available, current power: 78.29 W\n  ✓ Power logger started (interval: 100 ms)\n\nRunning 300 timed inference iterations...\n  Progress: 300/300\n  ✓ Inference complete: 26.138s\n  ✓ Power logger stopped (214 samples)\n\nComputing accuracy...\n  ✓ Accuracy: 88.00% (44/50)\n\n======================================================================\nTRIAL 3 SUMMARY: INT8\n======================================================================\nLatency:       87.119 ms (± 0.660 ms)\nThroughput:    573.88 samples/s\nAccuracy:      88.00%\nPower:         229.27 W (± 35.17 W)\nEnergy:        19975.607 mJ/inference\nTotal Energy:  5992.682 J\nPeak Memory:   475.85 MB\n======================================================================\n\n======================================================================\nTRIAL 4: INT8\n======================================================================\n\nLoading model: distilbert-base-uncased-finetuned-sst-2-english\n  Precision: INT8\n  Device: cuda\n  ✓ Quantized 38 Linear layers to INT8 precision\n  ✓ Model loaded successfully\n  - Parameters: 66,955,010\n  - Model size: 255.42 MB\n  - Param dtype: torch.float32\n  - Param device: cuda:0\n\nWarming up GPU for 50 iterations...\n  Warmup: 50/50\n  ✓ Warmup complete\n\nStarting measurement...\n  ✓ nvidia-smi available, current power: 92.07 W\n  ✓ Power logger started (interval: 100 ms)\n\nRunning 300 timed inference iterations...\n  Progress: 300/300\n  ✓ Inference complete: 26.140s\n  ✓ Power logger stopped (214 samples)\n\nComputing accuracy...\n  ✓ Accuracy: 88.00% (44/50)\n\n======================================================================\nTRIAL 4 SUMMARY: INT8\n======================================================================\nLatency:       87.126 ms (± 0.599 ms)\nThroughput:    573.84 samples/s\nAccuracy:      88.00%\nPower:         229.71 W (± 35.14 W)\nEnergy:        20015.458 mJ/inference\nTotal Energy:  6004.637 J\nPeak Memory:   475.85 MB\n======================================================================\n\n======================================================================\nTRIAL 5: INT8\n======================================================================\n\nLoading model: distilbert-base-uncased-finetuned-sst-2-english\n  Precision: INT8\n  Device: cuda\n  ✓ Quantized 38 Linear layers to INT8 precision\n  ✓ Model loaded successfully\n  - Parameters: 66,955,010\n  - Model size: 255.42 MB\n  - Param dtype: torch.float32\n  - Param device: cuda:0\n\nWarming up GPU for 50 iterations...\n  Warmup: 50/50\n  ✓ Warmup complete\n\nStarting measurement...\n  ✓ nvidia-smi available, current power: 94.79 W\n  ✓ Power logger started (interval: 100 ms)\n\nRunning 300 timed inference iterations...\n  Progress: 300/300\n  ✓ Inference complete: 26.114s\n  ✓ Power logger stopped (214 samples)\n\nComputing accuracy...\n  ✓ Accuracy: 88.00% (44/50)\n\n======================================================================\nTRIAL 5 SUMMARY: INT8\n======================================================================\nLatency:       87.041 ms (± 0.599 ms)\nThroughput:    574.39 samples/s\nAccuracy:      88.00%\nPower:         230.08 W (± 35.22 W)\nEnergy:        20028.363 mJ/inference\nTotal Energy:  6008.509 J\nPeak Memory:   475.85 MB\n======================================================================\n\n======================================================================\nAGGREGATED RESULTS: INT8\nTrials: 5\n======================================================================\n\nLatency:\n  87.106 ± 0.040 ms\n\nThroughput:\n  573.97 ± 0.27 samples/s\n\nAccuracy:\n  88.00 ± 0.00%\n\nPower:\n  229.63 ± 0.39 W\n\nEnergy per Inference:\n  20003.418 ± 26.341 mJ\n\nMemory:\n  Model size: 255.42 MB\n  Peak GPU: 475.85 ± 0.00 MB\n======================================================================\n\n\n✓ Saved trial results: /kaggle/results/int8_trials.csv\n✓ Saved aggregated results: /kaggle/results/int8_aggregated.json\n\n✓ Saved comparison table: /kaggle/results/comparison_all_precisions.csv\n\n######################################################################\n# ALL EXPERIMENTS COMPLETE\n######################################################################\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Results Analysis and Visualization",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\nsns.set_palette(\"husl\")\n\n# Create comparison plots\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\nfig.suptitle(\"Energy-Aware Quantization: Performance Comparison (Per-Sample Metrics)\", fontsize=16, fontweight='bold')\n\ndf = pd.DataFrame(all_aggregated)\n\n# 1. Latency per batch\nax = axes[0, 0]\nif 'mean_latency_mean' in df.columns:\n    x = range(len(df))\n    ax.bar(x, df['mean_latency_mean']*1000, yerr=df['mean_latency_std']*1000, capsize=5, alpha=0.7)\n    ax.set_xticks(x)\n    ax.set_xticklabels(df['precision'].str.upper())\n    ax.set_ylabel('Latency (ms)', fontsize=11)\n    ax.set_title('Mean Latency per Batch', fontsize=12, fontweight='bold')\n    ax.grid(axis='y', alpha=0.3)\n\n# 2. Throughput\nax = axes[0, 1]\nif 'throughput_mean' in df.columns:\n    x = range(len(df))\n    ax.bar(x, df['throughput_mean'], yerr=df['throughput_std'], capsize=5, alpha=0.7, color='green')\n    ax.set_xticks(x)\n    ax.set_xticklabels(df['precision'].str.upper())\n    ax.set_ylabel('Throughput (samples/s)', fontsize=11)\n    ax.set_title('Inference Throughput', fontsize=12, fontweight='bold')\n    ax.grid(axis='y', alpha=0.3)\n\n# 3. Energy per Sample (CORRECT METRIC)\nax = axes[0, 2]\nif 'energy_per_sample_mj_mean' in df.columns:\n    x = range(len(df))\n    ax.bar(x, df['energy_per_sample_mj_mean'], yerr=df['energy_per_sample_mj_std'], \n           capsize=5, alpha=0.7, color='orange')\n    ax.set_xticks(x)\n    ax.set_xticklabels(df['precision'].str.upper())\n    ax.set_ylabel('Energy (mJ)', fontsize=11)\n    ax.set_title('Energy per Sample', fontsize=12, fontweight='bold')\n    ax.grid(axis='y', alpha=0.3)\n\n# 4. Power Draw\nax = axes[1, 0]\nif 'mean_power_w_mean' in df.columns:\n    x = range(len(df))\n    ax.bar(x, df['mean_power_w_mean'], yerr=df['mean_power_w_std'], capsize=5, alpha=0.7, color='red')\n    ax.set_xticks(x)\n    ax.set_xticklabels(df['precision'].str.upper())\n    ax.set_ylabel('Power (W)', fontsize=11)\n    ax.set_title('Mean GPU Power Draw', fontsize=12, fontweight='bold')\n    ax.grid(axis='y', alpha=0.3)\n\n# 5. Accuracy\nax = axes[1, 1]\nif 'accuracy_mean' in df.columns:\n    x = range(len(df))\n    ax.bar(x, df['accuracy_mean']*100, yerr=df['accuracy_std']*100, capsize=5, alpha=0.7, color='purple')\n    ax.set_xticks(x)\n    ax.set_xticklabels(df['precision'].str.upper())\n    ax.set_ylabel('Accuracy (%)', fontsize=11)\n    ax.set_title('Classification Accuracy', fontsize=12, fontweight='bold')\n    ax.set_ylim([0, 100])\n    ax.grid(axis='y', alpha=0.3)\n\n# 6. Model Size\nax = axes[1, 2]\nif 'model_size_mb' in df.columns:\n    x = range(len(df))\n    ax.bar(x, df['model_size_mb'], alpha=0.7, color='teal')\n    ax.set_xticks(x)\n    ax.set_xticklabels(df['precision'].str.upper())\n    ax.set_ylabel('Size (MB)', fontsize=11)\n    ax.set_title('Model Size', fontsize=12, fontweight='bold')\n    ax.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplot_path = Path(CONFIG[\"output_dir\"]) / \"comparison_plots.png\"\nplt.savefig(plot_path, dpi=300, bbox_inches='tight')\nprint(f\"\\n✓ Saved plots: {plot_path}\")\nplt.show()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-03T08:59:45.021163Z",
     "iopub.execute_input": "2025-12-03T08:59:45.021388Z",
     "iopub.status.idle": "2025-12-03T08:59:47.886449Z",
     "shell.execute_reply.started": "2025-12-03T08:59:45.021362Z",
     "shell.execute_reply": "2025-12-03T08:59:47.885754Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 13. Summary Table",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create formatted summary table\nsummary_data = []\n\nfor agg in all_aggregated:\n    row = {\n        \"Precision\": agg[\"precision\"].upper(),\n        \"Batch Size\": agg.get(\"batch_size\", \"N/A\"),\n        \"Latency/batch (ms)\": f\"{agg.get('mean_latency_mean', 0)*1000:.3f} ± {agg.get('mean_latency_std', 0)*1000:.3f}\",\n        \"Throughput (samples/s)\": f\"{agg.get('throughput_mean', 0):.2f} ± {agg.get('throughput_std', 0):.2f}\",\n        \"Accuracy (%)\": f\"{agg.get('accuracy_mean', 0)*100:.2f} ± {agg.get('accuracy_std', 0)*100:.2f}\",\n        \"Power (W)\": f\"{agg.get('mean_power_w_mean', 0):.2f} ± {agg.get('mean_power_w_std', 0):.2f}\" if agg.get('mean_power_w_mean') else \"N/A\",\n        \"Energy/sample (mJ)\": f\"{agg.get('energy_per_sample_mj_mean', 0):.3f} ± {agg.get('energy_per_sample_mj_std', 0):.3f}\" if agg.get('energy_per_sample_mj_mean') else \"N/A\",\n        \"Model Size (MB)\": f\"{agg.get('model_size_mb', 0):.2f}\",\n        \"Peak Memory (MB)\": f\"{agg.get('peak_memory_mb_mean', 0):.2f}\" if agg.get('peak_memory_mb_mean') else \"N/A\"\n    }\n    summary_data.append(row)\n\nsummary_df = pd.DataFrame(summary_data)\n\nprint(\"\\n\" + \"=\"*140)\nprint(\"FINAL SUMMARY TABLE (Per-Sample Energy)\")\nprint(\"=\"*140)\nprint(summary_df.to_string(index=False))\nprint(\"=\"*140)\nprint(\"\\nNOTE: Energy/sample = Total Energy / (num_iters × batch_size)\")\nprint(f\"      With batch_size={all_aggregated[0].get('batch_size', 50)} and num_iters=300:\")\nprint(f\"      Energy/sample ≈ Energy/batch / {all_aggregated[0].get('batch_size', 50)}\")\nprint(\"=\"*140)\n\n# Save summary table\nsummary_path = Path(CONFIG[\"output_dir\"]) / \"summary_table.csv\"\nsummary_df.to_csv(summary_path, index=False)\nprint(f\"\\n✓ Saved summary table: {summary_path}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-03T08:59:47.887233Z",
     "iopub.execute_input": "2025-12-03T08:59:47.888165Z",
     "iopub.status.idle": "2025-12-03T08:59:47.902688Z",
     "shell.execute_reply.started": "2025-12-03T08:59:47.888145Z",
     "shell.execute_reply": "2025-12-03T08:59:47.902050Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 14. Relative Improvements\n\nCompare FP16 and INT8 against FP32 baseline.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "if len(all_aggregated) >= 2:\n    # Use FP32 as baseline\n    baseline = next((a for a in all_aggregated if a['precision'] == 'fp32'), all_aggregated[0])\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"RELATIVE IMPROVEMENTS vs FP32 BASELINE\")\n    print(\"=\"*70)\n    \n    for agg in all_aggregated:\n        if agg['precision'] == 'fp32':\n            continue\n        \n        print(f\"\\n{agg['precision'].upper()} vs FP32:\")\n        print(\"-\" * 40)\n        \n        # Latency speedup\n        if 'mean_latency_mean' in baseline and 'mean_latency_mean' in agg:\n            speedup = baseline['mean_latency_mean'] / agg['mean_latency_mean']\n            reduction_pct = (1 - 1/speedup) * 100\n            print(f\"Latency:     {speedup:.2f}x faster ({reduction_pct:.1f}% reduction)\")\n        \n        # Energy savings\n        if 'energy_per_inference_mj_mean' in baseline and 'energy_per_inference_mj_mean' in agg:\n            if baseline['energy_per_inference_mj_mean'] and agg['energy_per_inference_mj_mean']:\n                energy_ratio = agg['energy_per_inference_mj_mean'] / baseline['energy_per_inference_mj_mean']\n                energy_savings_pct = (1 - energy_ratio) * 100\n                print(f\"Energy:      {energy_savings_pct:+.1f}% change\")\n        \n        # Accuracy delta\n        if 'accuracy_mean' in baseline and 'accuracy_mean' in agg:\n            accuracy_delta = (agg['accuracy_mean'] - baseline['accuracy_mean']) * 100\n            print(f\"Accuracy:    {accuracy_delta:+.2f} percentage points\")\n        \n        # Model size reduction\n        if 'model_size_mb' in baseline and 'model_size_mb' in agg:\n            size_ratio = agg['model_size_mb'] / baseline['model_size_mb']\n            size_reduction_pct = (1 - size_ratio) * 100\n            print(f\"Model Size:  {size_reduction_pct:.1f}% reduction\")\n    \n    print(\"\\n\" + \"=\"*70)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-03T08:59:47.903441Z",
     "iopub.execute_input": "2025-12-03T08:59:47.903733Z",
     "iopub.status.idle": "2025-12-03T08:59:47.918349Z",
     "shell.execute_reply.started": "2025-12-03T08:59:47.903708Z",
     "shell.execute_reply": "2025-12-03T08:59:47.917615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "\n======================================================================\nRELATIVE IMPROVEMENTS vs FP32 BASELINE\n======================================================================\n\nFP16 vs FP32:\n----------------------------------------\nLatency:     1.05x faster (4.6% reduction)\nEnergy:      +4.5% change\nAccuracy:    +0.00 percentage points\nModel Size:  50.0% reduction\n\nINT8 vs FP32:\n----------------------------------------\nLatency:     1.00x faster (0.2% reduction)\nEnergy:      -1.0% change\nAccuracy:    +2.00 percentage points\nModel Size:  0.0% reduction\n\n======================================================================\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "source": "## Notes\n\n### Key Features:\n1. **Zero-I/O Design**: All data loaded to GPU before measurements\n2. **Accurate Quantization**: Symmetric INT8 quantization for weights\n3. **Asynchronous Power Monitoring**: nvidia-smi polling in background thread\n4. **CUDA Synchronization**: Precise timing with GPU sync after each forward pass\n5. **Statistical Significance**: Multiple trials with aggregation\n6. **Comprehensive Metrics**: Latency, throughput, energy, accuracy, memory\n\n### Checklist:\n- ✅ Dataset loads from .pt files directly to GPU\n- ✅ No I/O operations during measurement loop\n- ✅ Model loaded once per trial with correct precision\n- ✅ Warmup phase stabilizes GPU before measurement\n- ✅ Power logger runs asynchronously without blocking\n- ✅ CUDA synchronization ensures accurate timing\n- ✅ Energy computed as Power × Time\n- ✅ Results saved in CSV and JSON formats\n- ✅ Multiple trials for statistical confidence\n- ✅ Comparison plots and summary tables generated",
   "metadata": {}
  }
 ]
}
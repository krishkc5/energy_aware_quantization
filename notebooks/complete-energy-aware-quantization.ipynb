{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Energy-Aware Quantization Pipeline\n",
    "## ESE 5390 Final Project: DistilBERT & GPT-2 Small\n",
    "\n",
    "This notebook provides a complete end-to-end pipeline for energy-aware quantization analysis:\n",
    "\n",
    "**Part 1: DistilBERT (67M parameters)**\n",
    "1. Dataset Preparation (SST-2 sentiment classification)\n",
    "2. Quantization Benchmarking (FP32 vs FP16)\n",
    "3. Per-Layer Energy Profiling\n",
    "\n",
    "**Part 2: GPT-2 Small (124M parameters)**\n",
    "1. Dataset Preparation (WikiText-2 language modeling)\n",
    "2. Quantization Benchmarking (FP32 vs FP16)\n",
    "3. Per-Layer Energy Profiling\n",
    "\n",
    "**Key Features:**\n",
    "- Zero I/O design (pre-tokenized datasets)\n",
    "- GPU power monitoring via nvidia-smi\n",
    "- CUDA-synchronized timing\n",
    "- Comprehensive visualizations\n",
    "- Layer-wise energy attribution\n",
    "- Quantization candidate identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:43:18.406777Z",
     "iopub.status.busy": "2025-12-07T17:43:18.406558Z",
     "iopub.status.idle": "2025-12-07T17:43:18.427423Z",
     "shell.execute_reply": "2025-12-07T17:43:18.426908Z",
     "shell.execute_reply.started": "2025-12-07T17:43:18.406751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, \n",
    "    DistilBertForSequenceClassification,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "from transformers.activations import NewGELUActivation, GELUActivation\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import time\n",
    "import threading\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"Total GPU memory: {total_memory:.2f} GB\")\n",
    "    device = \"cuda\"\n",
    "    print(\"\\n GPU is ready!\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"\\n Running on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Classes and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:43:18.428295Z",
     "iopub.status.busy": "2025-12-07T17:43:18.428088Z",
     "iopub.status.idle": "2025-12-07T17:43:19.485431Z",
     "shell.execute_reply": "2025-12-07T17:43:19.484580Z",
     "shell.execute_reply.started": "2025-12-07T17:43:18.428275Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PowerLogger:\n",
    "    \"\"\"Asynchronous GPU power monitoring using nvidia-smi\"\"\"\n",
    "    \n",
    "    def __init__(self, poll_interval_ms=50, gpu_id=0):\n",
    "        self.poll_interval_ms = poll_interval_ms\n",
    "        self.gpu_id = gpu_id\n",
    "        self.power_samples = []\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "    \n",
    "    def _monitor_power(self):\n",
    "        \"\"\"Background monitoring loop\"\"\"\n",
    "        while self.running:\n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    ['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits', f'--id={self.gpu_id}'],\n",
    "                    capture_output=True,\n",
    "                    text=True,\n",
    "                    timeout=1.0\n",
    "                )\n",
    "                if result.returncode == 0:\n",
    "                    power_str = result.stdout.strip().split('\\n')[0].strip()\n",
    "                    power_w = float(power_str)\n",
    "                    self.power_samples.append(power_w)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            time.sleep(self.poll_interval_ms / 1000.0)\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"Start power monitoring in background thread\"\"\"\n",
    "        self.power_samples = []\n",
    "        self.running = True\n",
    "        self.thread = threading.Thread(target=self._monitor_power, daemon=True)\n",
    "        self.thread.start()\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop power monitoring and return statistics\"\"\"\n",
    "        self.running = False\n",
    "        if self.thread:\n",
    "            self.thread.join(timeout=2.0)\n",
    "        \n",
    "        if len(self.power_samples) == 0:\n",
    "            return {'mean_power_w': 0, 'std_power_w': 0, 'num_samples': 0}\n",
    "        \n",
    "        return {\n",
    "            'mean_power_w': np.mean(self.power_samples),\n",
    "            'std_power_w': np.std(self.power_samples),\n",
    "            'num_samples': len(self.power_samples)\n",
    "        }\n",
    "\n",
    "# Test power logger\n",
    "if device == \"cuda\":\n",
    "    print(\"Testing power logger\")\n",
    "    logger = PowerLogger(poll_interval_ms=50)\n",
    "    logger.start()\n",
    "    time.sleep(1.0)\n",
    "    stats = logger.stop()\n",
    "    print(f\" Power logger working: {stats['mean_power_w']:.2f}W (n={stats['num_samples']} samples)\")\n",
    "else:\n",
    "    print(\"Power monitoring disabled (CPU mode)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Profiler (for Per-Layer Energy Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:43:19.486544Z",
     "iopub.status.busy": "2025-12-07T17:43:19.486302Z",
     "iopub.status.idle": "2025-12-07T17:43:19.498732Z",
     "shell.execute_reply": "2025-12-07T17:43:19.497881Z",
     "shell.execute_reply.started": "2025-12-07T17:43:19.486526Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LayerProfiler:\n",
    "    \"\"\"Profile execution time of each layer using forward hooks.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, device: str = \"cuda\"):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.hooks = []\n",
    "        self.layer_times = defaultdict(list)\n",
    "        self.layer_names = []\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        \"\"\"Register forward pre/post hooks on all layers.\"\"\"\n",
    "        \n",
    "        def make_pre_hook(name):\n",
    "            def pre_hook(module, input):\n",
    "                if self.device == \"cuda\":\n",
    "                    torch.cuda.synchronize()\n",
    "                self.layer_times[name + \"_start\"].append(time.perf_counter())\n",
    "            return pre_hook\n",
    "        \n",
    "        def make_post_hook(name):\n",
    "            def post_hook(module, input, output):\n",
    "                if self.device == \"cuda\":\n",
    "                    torch.cuda.synchronize()\n",
    "                self.layer_times[name + \"_end\"].append(time.perf_counter())\n",
    "            return post_hook\n",
    "        \n",
    "        for name, module in self.model.named_modules():\n",
    "            if len(list(module.children())) == 0:  # Leaf modules only\n",
    "                if isinstance(module, (nn.Linear, nn.LayerNorm, nn.Dropout, nn.GELU, nn.Embedding, nn.Conv1d, NewGELUActivation, GELUActivation)):\n",
    "                    self.layer_names.append(name)\n",
    "                    hook_pre = module.register_forward_pre_hook(make_pre_hook(name))\n",
    "                    hook_post = module.register_forward_hook(make_post_hook(name))\n",
    "                    self.hooks.append(hook_pre)\n",
    "                    self.hooks.append(hook_post)\n",
    "        \n",
    "        print(f\" Registered hooks on {len(self.layer_names)} layers\")\n",
    "    \n",
    "    def reset(self):\n",
    "        self.layer_times.clear()\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks.clear()\n",
    "    \n",
    "    def get_layer_stats(self) -> pd.DataFrame:\n",
    "        \"\"\"Get detailed statistics for each layer.\"\"\"\n",
    "        stats = []\n",
    "        \n",
    "        for name in self.layer_names:\n",
    "            start_times = self.layer_times.get(name + \"_start\", [])\n",
    "            end_times = self.layer_times.get(name + \"_end\", [])\n",
    "            \n",
    "            if len(start_times) == len(end_times) and len(start_times) > 0:\n",
    "                durations = [end - start for start, end in zip(start_times, end_times)]\n",
    "                \n",
    "                stats.append({\n",
    "                    \"layer_name\": name,\n",
    "                    \"mean_time_ms\": np.mean(durations) * 1000,\n",
    "                    \"std_time_ms\": np.std(durations) * 1000,\n",
    "                    \"total_time_s\": np.sum(durations),\n",
    "                    \"num_calls\": len(durations)\n",
    "                })\n",
    "        \n",
    "        df = pd.DataFrame(stats)\n",
    "        \n",
    "        if len(df) > 0:\n",
    "            total_time = df[\"total_time_s\"].sum()\n",
    "            df[\"percent_total\"] = (df[\"total_time_s\"] / total_time) * 100\n",
    "        \n",
    "        return df.sort_values(\"total_time_s\", ascending=False)\n",
    "\n",
    "print(\"LayerProfiler class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:43:19.499618Z",
     "iopub.status.busy": "2025-12-07T17:43:19.499394Z",
     "iopub.status.idle": "2025-12-07T17:43:19.520418Z",
     "shell.execute_reply": "2025-12-07T17:43:19.519640Z",
     "shell.execute_reply.started": "2025-12-07T17:43:19.499570Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_layer_type_distilbert(name: str) -> str:\n",
    "    \"\"\"Extract layer type from DistilBERT layer name.\"\"\"\n",
    "    if \"attention.q_lin\" in name or \"attention.k_lin\" in name or \"attention.v_lin\" in name:\n",
    "        return \"Attention Projection\"\n",
    "    elif \"attention.out_lin\" in name:\n",
    "        return \"Attention Output\"\n",
    "    elif \"ffn.lin1\" in name:\n",
    "        return \"FFN Layer 1\"\n",
    "    elif \"ffn.lin2\" in name:\n",
    "        return \"FFN Layer 2\"\n",
    "    elif \"sa_layer_norm\" in name or \"output_layer_norm\" in name:\n",
    "        return \"LayerNorm\"\n",
    "    elif \"embeddings\" in name:\n",
    "        return \"Embeddings\"\n",
    "    elif \"pre_classifier\" in name or \"classifier\" in name:\n",
    "        return \"Classifier\"\n",
    "    elif \"dropout\" in name:\n",
    "        return \"Dropout\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "def get_layer_type_gpt2(name: str) -> str:\n",
    "    \"\"\"Extract layer type from GPT-2 layer name.\"\"\"\n",
    "    if \"attn.c_attn\" in name:\n",
    "        return \"Attention QKV Projection\"\n",
    "    elif \"attn.c_proj\" in name:\n",
    "        return \"Attention Output\"\n",
    "    elif \"mlp.c_fc\" in name:\n",
    "        return \"MLP Layer 1 (Expansion)\"\n",
    "    elif \"mlp.c_proj\" in name:\n",
    "        return \"MLP Layer 2 (Projection)\"\n",
    "    elif \"ln_\" in name:\n",
    "        return \"LayerNorm\"\n",
    "    elif \"wte\" in name or \"wpe\" in name:\n",
    "        return \"Embeddings\"\n",
    "    elif \"lm_head\" in name:\n",
    "        return \"LM Head\"\n",
    "    elif \"dropout\" in name:\n",
    "        return \"Dropout\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "def create_output_directories():\n",
    "    \"\"\"Create output directories for results and figures.\"\"\"\n",
    "    base_dir = Path(\"../results\")\n",
    "    dirs = {\n",
    "        'base': base_dir,\n",
    "        'distilbert': base_dir / \"distilbert\",\n",
    "        'gpt2': base_dir / \"gpt2\",\n",
    "        'datasets': base_dir / \"datasets\"\n",
    "    }\n",
    "    \n",
    "    for dir_path in dirs.values():\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    return dirs\n",
    "\n",
    "output_dirs = create_output_directories()\n",
    "print(\"Output directories created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 1: DistilBERT Pipeline\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 DistilBERT: Dataset Preparation (SST-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:43:19.522102Z",
     "iopub.status.busy": "2025-12-07T17:43:19.521916Z",
     "iopub.status.idle": "2025-12-07T17:43:21.362968Z",
     "shell.execute_reply": "2025-12-07T17:43:21.362334Z",
     "shell.execute_reply.started": "2025-12-07T17:43:19.522088Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"\\n[1/4] Loading DistilBERT tokenizer\")\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Load SST-2 dataset\n",
    "print(\"[2/4] Loading SST-2 validation set\")\n",
    "sst2_dataset = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
    "print(f\"  Dataset size: {len(sst2_dataset)} samples\")\n",
    "\n",
    "# Tokenize\n",
    "print(\"[3/4] Tokenizing all samples (max_length=128)\")\n",
    "sst2_dataset = sst2_dataset.shuffle(seed=42)\n",
    "texts = [example['sentence'] for example in sst2_dataset]\n",
    "labels = [example['label'] for example in sst2_dataset]\n",
    "\n",
    "distilbert_encodings = distilbert_tokenizer(\n",
    "    texts,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "distilbert_labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Save dataset\n",
    "print(\"[4/4] Saving tokenized dataset\")\n",
    "distilbert_data_dir = output_dirs['datasets'] / \"distilbert_sst2\"\n",
    "distilbert_data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "torch.save(distilbert_encodings['input_ids'], distilbert_data_dir / 'input_ids.pt')\n",
    "torch.save(distilbert_encodings['attention_mask'], distilbert_data_dir / 'attention_mask.pt')\n",
    "torch.save(distilbert_labels, distilbert_data_dir / 'labels.pt')\n",
    "\n",
    "# Save metadata\n",
    "distilbert_metadata = {\n",
    "    'num_samples': len(texts),\n",
    "    'max_length': 128,\n",
    "    'dataset_name': 'sst2',\n",
    "    'num_labels': 2,\n",
    "    'model': 'distilbert-base-uncased',\n",
    "    'task': 'sentiment_classification'\n",
    "}\n",
    "\n",
    "with open(distilbert_data_dir / 'metadata.json', 'w') as f:\n",
    "    json.dump(distilbert_metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nDistilBERT dataset prepared: {len(texts)} samples\")\n",
    "print(f\"  Saved to: {distilbert_data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 DistilBERT: Quantization Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:43:21.364005Z",
     "iopub.status.busy": "2025-12-07T17:43:21.363755Z",
     "iopub.status.idle": "2025-12-07T17:43:33.622848Z",
     "shell.execute_reply": "2025-12-07T17:43:33.621910Z",
     "shell.execute_reply.started": "2025-12-07T17:43:21.363977Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def benchmark_distilbert(precision: str, input_ids, attention_mask, labels, device, num_batches=None, batch_size=32):\n",
    "    \"\"\"Benchmark DistilBERT with specified precision.\"\"\"\n",
    "    \n",
    "    print(f\"Benchmarking DistilBERT: {precision.upper()}\")\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"[1/4] Loading {precision.upper()} model\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        'distilbert-base-uncased-finetuned-sst-2-english',\n",
    "        num_labels=2\n",
    "    )\n",
    "    \n",
    "    if precision == 'fp16':\n",
    "        model = model.half()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    model_size_mb = sum(p.element_size() * p.nelement() for p in model.parameters()) / (1024 ** 2)\n",
    "    print(f\"  Model size: {model_size_mb:.2f} MB\")\n",
    "    \n",
    "    # Move data to device\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    dataset_size = input_ids.shape[0]\n",
    "    num_batches = num_batches or (dataset_size + batch_size - 1) // batch_size\n",
    "    \n",
    "    # Warmup\n",
    "    print(\"[2/4] Warmup (10 batches)\")\n",
    "    with torch.no_grad():\n",
    "        for i in range(10):\n",
    "            start_idx = 0\n",
    "            end_idx = min(batch_size, dataset_size)\n",
    "            _ = model(input_ids=input_ids[start_idx:end_idx], attention_mask=attention_mask[start_idx:end_idx])\n",
    "    \n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Start power logger\n",
    "    print(\"[3/4] Running benchmark\")\n",
    "    power_logger = None\n",
    "    if device == \"cuda\":\n",
    "        power_logger = PowerLogger(poll_interval_ms=50)\n",
    "        power_logger.start()\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # Benchmark\n",
    "    latencies = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    benchmark_start = time.perf_counter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_batches):\n",
    "            start_idx = (i * batch_size) % dataset_size\n",
    "            end_idx = min(start_idx + batch_size, dataset_size)\n",
    "            \n",
    "            if start_idx == 0 and i > 0:\n",
    "                break\n",
    "            \n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            batch_start = time.perf_counter()\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids[start_idx:end_idx],\n",
    "                attention_mask=attention_mask[start_idx:end_idx]\n",
    "            )\n",
    "            \n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            batch_end = time.perf_counter()\n",
    "            latencies.append(batch_end - batch_start)\n",
    "            \n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            correct += (predictions == labels[start_idx:end_idx]).sum().item()\n",
    "            total += (end_idx - start_idx)\n",
    "    \n",
    "    benchmark_end = time.perf_counter()\n",
    "    \n",
    "    # Stop power logger\n",
    "    power_stats = None\n",
    "    if power_logger:\n",
    "        time.sleep(0.5)\n",
    "        power_stats = power_logger.stop()\n",
    "    \n",
    "    # Compute metrics\n",
    "    print(\"[4/4] Computing metrics\")\n",
    "    total_time_s = benchmark_end - benchmark_start\n",
    "    mean_latency_ms = np.mean(latencies) * 1000\n",
    "    std_latency_ms = np.std(latencies) * 1000\n",
    "    accuracy = 100.0 * correct / total\n",
    "    throughput = total / total_time_s\n",
    "    \n",
    "    mean_power_w = 0\n",
    "    energy_per_sample_mj = 0\n",
    "    if power_stats and power_stats['num_samples'] > 0:\n",
    "        mean_power_w = power_stats['mean_power_w']\n",
    "        total_energy_j = mean_power_w * total_time_s\n",
    "        energy_per_sample_mj = (total_energy_j / total) * 1000\n",
    "    \n",
    "    results = {\n",
    "        'name': precision.upper(),\n",
    "        'mean_latency_ms': mean_latency_ms,\n",
    "        'std_latency_ms': std_latency_ms,\n",
    "        'accuracy': accuracy,\n",
    "        'throughput': throughput,\n",
    "        'mean_power_w': mean_power_w,\n",
    "        'energy_per_sample_mj': energy_per_sample_mj,\n",
    "        'model_size_mb': model_size_mb,\n",
    "        'total_samples': total\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n Results:\")\n",
    "    print(f\"  Latency:    {mean_latency_ms:.3f} ± {std_latency_ms:.3f} ms/batch\")\n",
    "    print(f\"  Throughput: {throughput:.1f} samples/sec\")\n",
    "    print(f\"  Accuracy:   {accuracy:.2f}% ({correct}/{total})\")\n",
    "    if power_stats:\n",
    "        print(f\"  Power:      {mean_power_w:.2f} W\")\n",
    "        print(f\"  Energy:     {energy_per_sample_mj:.2f} mJ/sample\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmarks\n",
    "distilbert_results = []\n",
    "\n",
    "for precision in ['fp32', 'fp16']:\n",
    "    results = benchmark_distilbert(\n",
    "        precision=precision,\n",
    "        input_ids=distilbert_encodings['input_ids'],\n",
    "        attention_mask=distilbert_encodings['attention_mask'],\n",
    "        labels=distilbert_labels,\n",
    "        device=device,\n",
    "        batch_size=32\n",
    "    )\n",
    "    distilbert_results.append(results)\n",
    "    time.sleep(2)\n",
    "\n",
    "# Create results dataframe\n",
    "df_distilbert = pd.DataFrame(distilbert_results)\n",
    "\n",
    "# Calculate speedup\n",
    "fp32_latency = df_distilbert[df_distilbert['name'] == 'FP32']['mean_latency_ms'].values[0]\n",
    "df_distilbert['speedup_vs_fp32'] = fp32_latency / df_distilbert['mean_latency_ms']\n",
    "\n",
    "fp32_energy = df_distilbert[df_distilbert['name'] == 'FP32']['energy_per_sample_mj'].values[0]\n",
    "if fp32_energy > 0:\n",
    "    df_distilbert['energy_reduction_vs_fp32'] = fp32_energy / df_distilbert['energy_per_sample_mj']\n",
    "else:\n",
    "    df_distilbert['energy_reduction_vs_fp32'] = 1.0\n",
    "\n",
    "fp32_size = df_distilbert[df_distilbert['name'] == 'FP32']['model_size_mb'].values[0]\n",
    "df_distilbert['size_reduction_vs_fp32'] = fp32_size / df_distilbert['model_size_mb']\n",
    "\n",
    "print(df_distilbert.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "df_distilbert.to_csv(output_dirs['distilbert'] / 'benchmark_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 DistilBERT: Per-Layer Energy Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:43:33.624210Z",
     "iopub.status.busy": "2025-12-07T17:43:33.623911Z",
     "iopub.status.idle": "2025-12-07T17:43:36.146414Z",
     "shell.execute_reply": "2025-12-07T17:43:36.145628Z",
     "shell.execute_reply.started": "2025-12-07T17:43:33.624186Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load model for profiling\n",
    "print(\"\\n[1/5] Loading DistilBERT model\")\n",
    "distilbert_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased-finetuned-sst-2-english',\n",
    "    num_labels=2\n",
    ")\n",
    "distilbert_model = distilbert_model.to(device)\n",
    "distilbert_model.eval()\n",
    "print(\" Model loaded\")\n",
    "\n",
    "# Create profiler\n",
    "print(\"[2/5] Creating layer profiler\")\n",
    "distilbert_profiler = LayerProfiler(distilbert_model, device=device)\n",
    "\n",
    "# Move data to GPU\n",
    "distilbert_input_ids = distilbert_encodings['input_ids'].to(device)\n",
    "distilbert_attention_mask = distilbert_encodings['attention_mask'].to(device)\n",
    "\n",
    "# Warmup\n",
    "print(\"[3/5] Warmup\")\n",
    "with torch.no_grad():\n",
    "    for i in range(20):\n",
    "        idx = i % distilbert_input_ids.shape[0]\n",
    "        _ = distilbert_model(\n",
    "            input_ids=distilbert_input_ids[idx].unsqueeze(0),\n",
    "            attention_mask=distilbert_attention_mask[idx].unsqueeze(0)\n",
    "        )\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# Reset profiler\n",
    "distilbert_profiler.reset()\n",
    "\n",
    "# Run profiling\n",
    "print(\"[4/5] Running profiling (100 iterations)\")\n",
    "power_logger = PowerLogger(poll_interval_ms=50)\n",
    "power_logger.start()\n",
    "time.sleep(0.5)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(100):\n",
    "        idx = i % distilbert_input_ids.shape[0]\n",
    "        _ = distilbert_model(\n",
    "            input_ids=distilbert_input_ids[idx].unsqueeze(0),\n",
    "            attention_mask=distilbert_attention_mask[idx].unsqueeze(0)\n",
    "        )\n",
    "\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "time.sleep(0.5)\n",
    "power_stats = power_logger.stop()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "mean_power = power_stats['mean_power_w']\n",
    "total_energy = mean_power * total_time\n",
    "\n",
    "print(f\" Total time: {total_time:.3f}s\")\n",
    "print(f\" Mean power: {mean_power:.2f}W\")\n",
    "print(f\" Total energy: {total_energy:.3f}J\")\n",
    "\n",
    "# Get layer statistics\n",
    "print(\"[5/5] Analyzing layer statistics\")\n",
    "distilbert_layer_stats = distilbert_profiler.get_layer_stats()\n",
    "\n",
    "# Compute per-layer energy\n",
    "total_measured_time = distilbert_layer_stats[\"total_time_s\"].sum()\n",
    "distilbert_layer_stats[\"energy_j\"] = (distilbert_layer_stats[\"total_time_s\"] / total_measured_time) * total_energy\n",
    "distilbert_layer_stats[\"energy_mj\"] = distilbert_layer_stats[\"energy_j\"] * 1000\n",
    "distilbert_layer_stats[\"layer_type\"] = distilbert_layer_stats[\"layer_name\"].apply(get_layer_type_distilbert)\n",
    "\n",
    "# Sort by energy\n",
    "distilbert_layer_stats = distilbert_layer_stats.sort_values(\"energy_j\", ascending=False)\n",
    "\n",
    "print(f\"\\n Per-layer profiling complete\")\n",
    "print(f\"  Profiled {len(distilbert_layer_stats)} layers\")\n",
    "print(f\"\\nTop 10 energy-consuming layers:\")\n",
    "print(distilbert_layer_stats[[\"layer_name\", \"energy_mj\", \"percent_total\", \"layer_type\"]].head(10).to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "distilbert_layer_stats.to_csv(output_dirs['distilbert'] / 'per_layer_energy.csv', index=False)\n",
    "\n",
    "# Group by layer type\n",
    "distilbert_type_energy = distilbert_layer_stats.groupby(\"layer_type\").agg({\n",
    "    \"energy_j\": \"sum\",\n",
    "    \"energy_mj\": \"sum\",\n",
    "    \"layer_name\": \"count\"\n",
    "}).rename(columns={\"layer_name\": \"num_layers\"})\n",
    "distilbert_type_energy[\"percent_energy\"] = (distilbert_type_energy[\"energy_j\"] / distilbert_type_energy[\"energy_j\"].sum()) * 100\n",
    "distilbert_type_energy = distilbert_type_energy.sort_values(\"energy_j\", ascending=False)\n",
    "\n",
    "print(f\"\\nEnergy by layer type:\")\n",
    "print(distilbert_type_energy.to_string())\n",
    "\n",
    "# Cleanup\n",
    "distilbert_profiler.remove_hooks()\n",
    "del distilbert_model\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 DistilBERT: Prediction Impact Analysis\n",
    "\n",
    "Analyze which layers contribute most to model predictions using layer ablation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:43:36.147597Z",
     "iopub.status.busy": "2025-12-07T17:43:36.147365Z",
     "iopub.status.idle": "2025-12-07T17:44:04.330172Z",
     "shell.execute_reply": "2025-12-07T17:44:04.329552Z",
     "shell.execute_reply.started": "2025-12-07T17:43:36.147580Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LayerAblationAnalyzer:\n",
    "    \"\"\"Analyze prediction impact by ablating (zeroing) layers\"\"\"\n",
    "    \n",
    "    def __init__(self, model, device=\"cuda\"):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.layer_names = []\n",
    "        \n",
    "        for name, module in model.named_modules():\n",
    "            if len(list(module.children())) == 0:\n",
    "                if isinstance(module, (nn.Linear, nn.LayerNorm, nn.Dropout, nn.GELU, nn.Embedding, NewGELUActivation, GELUActivation)):\n",
    "                    self.layer_names.append(name)\n",
    "        \n",
    "        print(f\"Found {len(self.layer_names)} layers for analysis\")\n",
    "    \n",
    "    def get_baseline(self, input_ids, attention_mask, labels, num_samples=100):\n",
    "        \"\"\"Get baseline predictions\"\"\"\n",
    "        self.model.eval()\n",
    "        indices = torch.randperm(input_ids.shape[0])[:num_samples]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids[indices],\n",
    "                attention_mask=attention_mask[indices]\n",
    "            )\n",
    "            logits = outputs.logits.cpu()\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            accuracy = (preds == labels[indices].cpu()).float().mean().item()\n",
    "        \n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'probs': probs,\n",
    "            'accuracy': accuracy,\n",
    "            'indices': indices\n",
    "        }\n",
    "    \n",
    "    def measure_impact(self, input_ids, attention_mask, labels, layer_name, baseline):\n",
    "        \"\"\"Measure impact of ablating one layer\"\"\"\n",
    "        def ablation_hook(module, input, output):\n",
    "            if isinstance(output, torch.Tensor):\n",
    "                return torch.zeros_like(output)\n",
    "            return output\n",
    "        \n",
    "        module = dict(self.model.named_modules())[layer_name]\n",
    "        hook = module.register_forward_hook(ablation_hook)\n",
    "        \n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids[baseline['indices']],\n",
    "                    attention_mask=attention_mask[baseline['indices']]\n",
    "                )\n",
    "                ablated_logits = outputs.logits.cpu()\n",
    "                ablated_probs = torch.softmax(ablated_logits, dim=-1)\n",
    "            \n",
    "            # Compute metrics\n",
    "            kl_div = nn.functional.kl_div(\n",
    "                torch.log(ablated_probs + 1e-8),\n",
    "                baseline['probs'],\n",
    "                reduction='batchmean'\n",
    "            ).item()\n",
    "            \n",
    "            logit_l2 = nn.functional.mse_loss(ablated_logits, baseline['logits']).item()\n",
    "            \n",
    "            ablated_preds = torch.argmax(ablated_logits, dim=-1)\n",
    "            ablated_acc = (ablated_preds == labels[baseline['indices']].cpu()).float().mean().item()\n",
    "            accuracy_drop = baseline['accuracy'] - ablated_acc\n",
    "            \n",
    "            return {\n",
    "                'layer_name': layer_name,\n",
    "                'kl_divergence': kl_div,\n",
    "                'logit_l2': logit_l2,\n",
    "                'accuracy_drop': accuracy_drop,\n",
    "                'impact_score': kl_div + logit_l2\n",
    "            }\n",
    "        finally:\n",
    "            hook.remove()\n",
    "\n",
    "# Load model for ablation\n",
    "print(\"\\n[1/3] Loading model for ablation analysis\")\n",
    "distilbert_ablation_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased-finetuned-sst-2-english',\n",
    "    num_labels=2\n",
    ").to(device)\n",
    "distilbert_ablation_model.eval()\n",
    "\n",
    "analyzer = LayerAblationAnalyzer(distilbert_ablation_model, device)\n",
    "\n",
    "# Get baseline\n",
    "print(\"\\n[2/3] Computing baseline predictions\")\n",
    "distilbert_baseline = analyzer.get_baseline(\n",
    "    distilbert_encodings['input_ids'].to(device),\n",
    "    distilbert_encodings['attention_mask'].to(device),\n",
    "    distilbert_labels.to(device),\n",
    "    num_samples=100\n",
    ")\n",
    "print(f\" Baseline accuracy: {distilbert_baseline['accuracy']:.4f}\")\n",
    "\n",
    "# Analyze all layers\n",
    "print(\"\\n[3/3] Analyzing layer impacts\")\n",
    "distilbert_impact_results = []\n",
    "\n",
    "for i, layer_name in enumerate(analyzer.layer_names):\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Progress: {i+1}/{len(analyzer.layer_names)}\", end='\\r')\n",
    "    \n",
    "    impact = analyzer.measure_impact(\n",
    "        distilbert_encodings['input_ids'].to(device),\n",
    "        distilbert_encodings['attention_mask'].to(device),\n",
    "        distilbert_labels.to(device),\n",
    "        layer_name,\n",
    "        distilbert_baseline\n",
    "    )\n",
    "    distilbert_impact_results.append(impact)\n",
    "\n",
    "print(f\"\\n Analysis complete\")\n",
    "\n",
    "# Create dataframe\n",
    "distilbert_impact_df = pd.DataFrame(distilbert_impact_results)\n",
    "distilbert_impact_df['layer_type'] = distilbert_impact_df['layer_name'].apply(get_layer_type_distilbert)\n",
    "distilbert_impact_df = distilbert_impact_df.sort_values('impact_score', ascending=False)\n",
    "\n",
    "# Group by type\n",
    "distilbert_type_impact = distilbert_impact_df.groupby('layer_type').agg({\n",
    "    'kl_divergence': 'mean',\n",
    "    'logit_l2': 'mean',\n",
    "    'accuracy_drop': 'mean',\n",
    "    'impact_score': 'mean',\n",
    "    'layer_name': 'count'\n",
    "}).rename(columns={'layer_name': 'num_layers'})\n",
    "distilbert_type_impact = distilbert_type_impact.sort_values('impact_score', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 high-impact layers:\")\n",
    "print(distilbert_impact_df[['layer_name', 'impact_score', 'accuracy_drop', 'layer_type']].head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nImpact by layer type:\")\n",
    "print(distilbert_type_impact.to_string())\n",
    "\n",
    "# Save\n",
    "distilbert_impact_df.to_csv(output_dirs['distilbert'] / 'prediction_impact.csv', index=False)\n",
    "print(f\"\\nSaved to: {output_dirs['distilbert'] / 'prediction_impact.csv'}\")\n",
    "\n",
    "# Cleanup\n",
    "del distilbert_ablation_model\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 2: GPT-2 Small Pipeline\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 GPT-2: Dataset Preparation (WikiText-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:44:04.331185Z",
     "iopub.status.busy": "2025-12-07T17:44:04.330946Z",
     "iopub.status.idle": "2025-12-07T17:44:07.827009Z",
     "shell.execute_reply": "2025-12-07T17:44:07.826300Z",
     "shell.execute_reply.started": "2025-12-07T17:44:04.331163Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"\\n[1/4] Loading GPT-2 tokenizer\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "# Load WikiText-2\n",
    "print(\"[2/4] Loading WikiText-2 test set\")\n",
    "wikitext_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "\n",
    "# Filter valid texts\n",
    "def is_valid_text(example):\n",
    "    text = example['text'].strip()\n",
    "    return len(text) > 50\n",
    "\n",
    "wikitext_dataset = wikitext_dataset.filter(is_valid_text)\n",
    "print(f\"  Valid sequences: {len(wikitext_dataset)}\")\n",
    "\n",
    "# Shuffle and tokenize\n",
    "print(\"[3/4] Tokenizing (max_length=128)\")\n",
    "wikitext_dataset = wikitext_dataset.shuffle(seed=42)\n",
    "texts = [example['text'] for example in wikitext_dataset]\n",
    "\n",
    "gpt2_encodings = gpt2_tokenizer(\n",
    "    texts,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "gpt2_labels = gpt2_encodings['input_ids'].clone()\n",
    "\n",
    "# Save dataset\n",
    "print(\"[4/4] Saving tokenized dataset\")\n",
    "gpt2_data_dir = output_dirs['datasets'] / \"gpt2_wikitext2\"\n",
    "gpt2_data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "torch.save(gpt2_encodings['input_ids'], gpt2_data_dir / 'input_ids.pt')\n",
    "torch.save(gpt2_encodings['attention_mask'], gpt2_data_dir / 'attention_mask.pt')\n",
    "torch.save(gpt2_labels, gpt2_data_dir / 'labels.pt')\n",
    "\n",
    "# Save metadata\n",
    "gpt2_metadata = {\n",
    "    'num_samples': len(texts),\n",
    "    'max_length': 128,\n",
    "    'dataset_name': 'wikitext-2',\n",
    "    'model': 'gpt2',\n",
    "    'task': 'language_modeling'\n",
    "}\n",
    "\n",
    "with open(gpt2_data_dir / 'metadata.json', 'w') as f:\n",
    "    json.dump(gpt2_metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nGPT-2 dataset prepared: {len(texts)} samples\")\n",
    "print(f\"  Saved to: {gpt2_data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 GPT-2: Quantization Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:05:20.452859Z",
     "iopub.status.busy": "2025-12-07T18:05:20.452137Z",
     "iopub.status.idle": "2025-12-07T18:05:30.761015Z",
     "shell.execute_reply": "2025-12-07T18:05:30.760166Z",
     "shell.execute_reply.started": "2025-12-07T18:05:20.452829Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def benchmark_gpt2(precision: str, input_ids, attention_mask, labels, device, num_iters=100):\n",
    "    \"\"\"Benchmark GPT-2 with specified precision.\"\"\"\n",
    "    \n",
    "    print(f\"Benchmarking GPT-2: {precision.upper()}\")\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"[1/4] Loading {precision.upper()} model\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(\n",
    "        'gpt2',\n",
    "        torch_dtype=torch.float32 if precision == 'fp32' else torch.float16\n",
    "    )\n",
    "    \n",
    "    if precision == 'fp16':\n",
    "        model = model.half()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    model_size_mb = sum(p.element_size() * p.nelement() for p in model.parameters()) / (1024 ** 2)\n",
    "    print(f\"  Model size: {model_size_mb:.2f} MB\")\n",
    "    \n",
    "    # Move data to device\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    num_samples = input_ids.shape[0]\n",
    "    \n",
    "    # Warmup\n",
    "    print(\"[2/4] Warmup (10 iterations)\")\n",
    "    with torch.no_grad():\n",
    "        for i in range(10):\n",
    "            idx = i % num_samples\n",
    "            _ = model(\n",
    "                input_ids=input_ids[idx].unsqueeze(0),\n",
    "                attention_mask=attention_mask[idx].unsqueeze(0)\n",
    "            )\n",
    "    \n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Start power logger\n",
    "    print(\"[3/4] Running benchmark\")\n",
    "    power_logger = None\n",
    "    if device == \"cuda\":\n",
    "        power_logger = PowerLogger(poll_interval_ms=50)\n",
    "        power_logger.start()\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # Benchmark\n",
    "    latencies = []\n",
    "    total_loss = 0\n",
    "    num_tokens = 0\n",
    "    \n",
    "    benchmark_start = time.perf_counter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_iters):\n",
    "            idx = i % num_samples\n",
    "            \n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            iter_start = time.perf_counter()\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids[idx].unsqueeze(0),\n",
    "                attention_mask=attention_mask[idx].unsqueeze(0),\n",
    "                labels=labels[idx].unsqueeze(0)\n",
    "            )\n",
    "            \n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            iter_end = time.perf_counter()\n",
    "            latencies.append(iter_end - iter_start)\n",
    "            \n",
    "            total_loss += outputs.loss.item()\n",
    "            num_tokens += attention_mask[idx].sum().item()\n",
    "    \n",
    "    benchmark_end = time.perf_counter()\n",
    "    \n",
    "    # Stop power logger\n",
    "    power_stats = None\n",
    "    if power_logger:\n",
    "        time.sleep(0.5)\n",
    "        power_stats = power_logger.stop()\n",
    "    \n",
    "    # Compute metrics\n",
    "    print(\"[4/4] Computing metrics\")\n",
    "    total_time_s = benchmark_end - benchmark_start\n",
    "    mean_latency_ms = np.mean(latencies) * 1000\n",
    "    std_latency_ms = np.std(latencies) * 1000\n",
    "    avg_loss = total_loss / num_iters\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    throughput = num_iters / total_time_s\n",
    "    tokens_per_sec = num_tokens / total_time_s\n",
    "    \n",
    "    mean_power_w = 0\n",
    "    energy_per_sample_mj = 0\n",
    "    if power_stats and power_stats['num_samples'] > 0:\n",
    "        mean_power_w = power_stats['mean_power_w']\n",
    "        total_energy_j = mean_power_w * total_time_s\n",
    "        energy_per_sample_mj = (total_energy_j / num_iters) * 1000\n",
    "    \n",
    "    results = {\n",
    "        'name': precision.upper(),\n",
    "        'mean_latency_ms': mean_latency_ms,\n",
    "        'std_latency_ms': std_latency_ms,\n",
    "        'perplexity': perplexity,\n",
    "        'throughput': throughput,\n",
    "        'tokens_per_sec': tokens_per_sec,\n",
    "        'mean_power_w': mean_power_w,\n",
    "        'energy_per_sample_mj': energy_per_sample_mj,\n",
    "        'model_size_mb': model_size_mb,\n",
    "        'total_iters': num_iters\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n Results:\")\n",
    "    print(f\"  Latency:     {mean_latency_ms:.3f} ± {std_latency_ms:.3f} ms/sample\")\n",
    "    print(f\"  Throughput:  {throughput:.1f} samples/sec\")\n",
    "    print(f\"  Tokens/sec:  {tokens_per_sec:.1f}\")\n",
    "    print(f\"  Perplexity:  {perplexity:.2f}\")\n",
    "    if power_stats:\n",
    "        print(f\"  Power:       {mean_power_w:.2f} W\")\n",
    "        print(f\"  Energy:      {energy_per_sample_mj:.2f} mJ/sample\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmarks\n",
    "gpt2_results = []\n",
    "\n",
    "for precision in ['fp32', 'fp16']:\n",
    "    results = benchmark_gpt2(\n",
    "        precision=precision,\n",
    "        input_ids=gpt2_encodings['input_ids'],\n",
    "        attention_mask=gpt2_encodings['attention_mask'],\n",
    "        labels=gpt2_labels,\n",
    "        device=device,\n",
    "        num_iters=100\n",
    "    )\n",
    "    gpt2_results.append(results)\n",
    "    time.sleep(2)\n",
    "\n",
    "# Create results dataframe\n",
    "df_gpt2 = pd.DataFrame(gpt2_results)\n",
    "\n",
    "# Calculate speedup\n",
    "fp32_latency = df_gpt2[df_gpt2['name'] == 'FP32']['mean_latency_ms'].values[0]\n",
    "df_gpt2['speedup_vs_fp32'] = fp32_latency / df_gpt2['mean_latency_ms']\n",
    "\n",
    "fp32_energy = df_gpt2[df_gpt2['name'] == 'FP32']['energy_per_sample_mj'].values[0]\n",
    "if fp32_energy > 0:\n",
    "    df_gpt2['energy_reduction_vs_fp32'] = fp32_energy / df_gpt2['energy_per_sample_mj']\n",
    "else:\n",
    "    df_gpt2['energy_reduction_vs_fp32'] = 1.0\n",
    "\n",
    "fp32_size = df_gpt2[df_gpt2['name'] == 'FP32']['model_size_mb'].values[0]\n",
    "df_gpt2['size_reduction_vs_fp32'] = fp32_size / df_gpt2['model_size_mb']\n",
    "\n",
    "print(df_gpt2.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "df_gpt2.to_csv(output_dirs['gpt2'] / 'benchmark_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 GPT-2: Per-Layer Energy Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:44:17.686458Z",
     "iopub.status.busy": "2025-12-07T17:44:17.686112Z",
     "iopub.status.idle": "2025-12-07T17:44:21.250266Z",
     "shell.execute_reply": "2025-12-07T17:44:21.249489Z",
     "shell.execute_reply.started": "2025-12-07T17:44:17.686434Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load model for profiling\n",
    "print(\"\\n[1/5] Loading GPT-2 model\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "gpt2_model = gpt2_model.to(device)\n",
    "gpt2_model.eval()\n",
    "print(\" Model loaded\")\n",
    "\n",
    "# Create profiler\n",
    "print(\"[2/5] Creating layer profiler\")\n",
    "gpt2_profiler = LayerProfiler(gpt2_model, device=device)\n",
    "\n",
    "# Move data to GPU\n",
    "gpt2_input_ids = gpt2_encodings['input_ids'].to(device)\n",
    "gpt2_attention_mask = gpt2_encodings['attention_mask'].to(device)\n",
    "\n",
    "# Warmup\n",
    "print(\"[3/5] Warmup\")\n",
    "with torch.no_grad():\n",
    "    for i in range(20):\n",
    "        idx = i % gpt2_input_ids.shape[0]\n",
    "        _ = gpt2_model(\n",
    "            input_ids=gpt2_input_ids[idx].unsqueeze(0),\n",
    "            attention_mask=gpt2_attention_mask[idx].unsqueeze(0)\n",
    "        )\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# Reset profiler\n",
    "gpt2_profiler.reset()\n",
    "\n",
    "# Run profiling\n",
    "print(\"[4/5] Running profiling (100 iterations)\")\n",
    "power_logger = PowerLogger(poll_interval_ms=50)\n",
    "power_logger.start()\n",
    "time.sleep(0.5)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(100):\n",
    "        idx = i % gpt2_input_ids.shape[0]\n",
    "        _ = gpt2_model(\n",
    "            input_ids=gpt2_input_ids[idx].unsqueeze(0),\n",
    "            attention_mask=gpt2_attention_mask[idx].unsqueeze(0)\n",
    "        )\n",
    "\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "time.sleep(0.5)\n",
    "power_stats = power_logger.stop()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "mean_power = power_stats['mean_power_w']\n",
    "total_energy = mean_power * total_time\n",
    "\n",
    "print(f\" Total time: {total_time:.3f}s\")\n",
    "print(f\" Mean power: {mean_power:.2f}W\")\n",
    "print(f\" Total energy: {total_energy:.3f}J\")\n",
    "\n",
    "# Get layer statistics\n",
    "print(\"[5/5] Analyzing layer statistics\")\n",
    "gpt2_layer_stats = gpt2_profiler.get_layer_stats()\n",
    "\n",
    "# Compute per-layer energy\n",
    "total_measured_time = gpt2_layer_stats[\"total_time_s\"].sum()\n",
    "gpt2_layer_stats[\"energy_j\"] = (gpt2_layer_stats[\"total_time_s\"] / total_measured_time) * total_energy\n",
    "gpt2_layer_stats[\"energy_mj\"] = gpt2_layer_stats[\"energy_j\"] * 1000\n",
    "gpt2_layer_stats[\"layer_type\"] = gpt2_layer_stats[\"layer_name\"].apply(get_layer_type_gpt2)\n",
    "\n",
    "# Sort by energy\n",
    "gpt2_layer_stats = gpt2_layer_stats.sort_values(\"energy_j\", ascending=False)\n",
    "\n",
    "print(f\"\\nPer-layer profiling complete\")\n",
    "print(f\"  Profiled {len(gpt2_layer_stats)} layers\")\n",
    "print(f\"\\nTop 10 energy-consuming layers:\")\n",
    "print(gpt2_layer_stats[[\"layer_name\", \"energy_mj\", \"percent_total\", \"layer_type\"]].head(10).to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "gpt2_layer_stats.to_csv(output_dirs['gpt2'] / 'per_layer_energy.csv', index=False)\n",
    "\n",
    "# Group by layer type\n",
    "gpt2_type_energy = gpt2_layer_stats.groupby(\"layer_type\").agg({\n",
    "    \"energy_j\": \"sum\",\n",
    "    \"energy_mj\": \"sum\",\n",
    "    \"layer_name\": \"count\"\n",
    "}).rename(columns={\"layer_name\": \"num_layers\"})\n",
    "gpt2_type_energy[\"percent_energy\"] = (gpt2_type_energy[\"energy_j\"] / gpt2_type_energy[\"energy_j\"].sum()) * 100\n",
    "gpt2_type_energy = gpt2_type_energy.sort_values(\"energy_j\", ascending=False)\n",
    "\n",
    "print(f\"\\nEnergy by layer type:\")\n",
    "print(gpt2_type_energy.to_string())\n",
    "\n",
    "# Cleanup\n",
    "gpt2_profiler.remove_hooks()\n",
    "del gpt2_model\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 GPT-2: Prediction Impact Analysis\n",
    "\n",
    "Analyze which layers contribute most to GPT-2 predictions using layer ablation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:44:21.251412Z",
     "iopub.status.busy": "2025-12-07T17:44:21.251158Z",
     "iopub.status.idle": "2025-12-07T17:53:36.590833Z",
     "shell.execute_reply": "2025-12-07T17:53:36.590044Z",
     "shell.execute_reply.started": "2025-12-07T17:44:21.251395Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load model for ablation\n",
    "print(\"\\n[1/3] Loading GPT-2 for ablation analysis\")\n",
    "gpt2_ablation_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "gpt2_ablation_model.eval()\n",
    "\n",
    "gpt2_analyzer = LayerAblationAnalyzer(gpt2_ablation_model, device)\n",
    "\n",
    "# Get baseline (using perplexity for language modeling)\n",
    "print(\"\\n[2/3] Computing baseline predictions\")\n",
    "\n",
    "def get_gpt2_baseline(model, input_ids, attention_mask, labels, num_samples=50):\n",
    "    \"\"\"Get baseline for GPT-2 (language modeling)\"\"\"\n",
    "    model.eval()\n",
    "    indices = torch.randperm(input_ids.shape[0])[:num_samples]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids[indices],\n",
    "            attention_mask=attention_mask[indices]\n",
    "        )\n",
    "        logits = outputs.logits.cpu()\n",
    "        \n",
    "        # Compute perplexity\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[indices].cpu()[..., 1:].contiguous()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        perplexity = torch.exp(loss).item()\n",
    "    \n",
    "    return {\n",
    "        'logits': logits,\n",
    "        'probs': torch.softmax(logits.cpu(), dim=-1),\n",
    "        'perplexity': perplexity,\n",
    "        'indices': indices\n",
    "    }\n",
    "\n",
    "gpt2_baseline = get_gpt2_baseline(\n",
    "    gpt2_ablation_model,\n",
    "    gpt2_encodings['input_ids'].to(device),\n",
    "    gpt2_encodings['attention_mask'].to(device),\n",
    "    gpt2_labels.to(device),\n",
    "    num_samples=50\n",
    ")\n",
    "print(f\" Baseline perplexity: {gpt2_baseline['perplexity']:.2f}\")\n",
    "\n",
    "# Analyze all layers\n",
    "print(\"\\n[3/3] Analyzing layer impacts\")\n",
    "gpt2_impact_results = []\n",
    "\n",
    "for i, layer_name in enumerate(gpt2_analyzer.layer_names):\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Progress: {i+1}/{len(gpt2_analyzer.layer_names)}\", end='\\r')\n",
    "    \n",
    "    def ablation_hook(module, input, output):\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            return torch.zeros_like(output)\n",
    "        return output\n",
    "    \n",
    "    module = dict(gpt2_ablation_model.named_modules())[layer_name]\n",
    "    hook = module.register_forward_hook(ablation_hook)\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = gpt2_ablation_model(\n",
    "                input_ids=gpt2_encodings['input_ids'][gpt2_baseline['indices']].to(device),\n",
    "                attention_mask=gpt2_encodings['attention_mask'][gpt2_baseline['indices']].to(device)\n",
    "            )\n",
    "            ablated_logits = outputs.logits.cpu()\n",
    "            ablated_probs = torch.softmax(ablated_logits, dim=-1)\n",
    "            \n",
    "            # Compute perplexity\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            shift_logits = ablated_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = gpt2_labels[gpt2_baseline['indices']].cpu()[..., 1:].contiguous()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            ablated_perplexity = torch.exp(loss).item()\n",
    "        \n",
    "        # Compute impact metrics\n",
    "        kl_div = nn.functional.kl_div(\n",
    "            torch.log(ablated_probs + 1e-8),\n",
    "            gpt2_baseline['probs'],\n",
    "            reduction='batchmean'\n",
    "        ).item()\n",
    "        \n",
    "        logit_l2 = nn.functional.mse_loss(ablated_logits, gpt2_baseline['logits']).item()\n",
    "        perplexity_change = ablated_perplexity - gpt2_baseline['perplexity']\n",
    "        \n",
    "        gpt2_impact_results.append({\n",
    "            'layer_name': layer_name,\n",
    "            'kl_divergence': kl_div,\n",
    "            'logit_l2': logit_l2,\n",
    "            'perplexity_change': perplexity_change,\n",
    "            'impact_score': kl_div + logit_l2\n",
    "        })\n",
    "    finally:\n",
    "        hook.remove()\n",
    "\n",
    "print(f\"\\n Analysis complete\")\n",
    "\n",
    "# Create dataframe\n",
    "gpt2_impact_df = pd.DataFrame(gpt2_impact_results)\n",
    "gpt2_impact_df['layer_type'] = gpt2_impact_df['layer_name'].apply(get_layer_type_gpt2)\n",
    "gpt2_impact_df = gpt2_impact_df.sort_values('impact_score', ascending=False)\n",
    "\n",
    "# Group by type\n",
    "gpt2_type_impact = gpt2_impact_df.groupby('layer_type').agg({\n",
    "    'kl_divergence': 'mean',\n",
    "    'logit_l2': 'mean',\n",
    "    'perplexity_change': 'mean',\n",
    "    'impact_score': 'mean',\n",
    "    'layer_name': 'count'\n",
    "}).rename(columns={'layer_name': 'num_layers'})\n",
    "gpt2_type_impact = gpt2_type_impact.sort_values('impact_score', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 high-impact layers:\")\n",
    "print(gpt2_impact_df[['layer_name', 'impact_score', 'perplexity_change', 'layer_type']].head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nImpact by layer type:\")\n",
    "print(gpt2_type_impact.to_string())\n",
    "\n",
    "# Save\n",
    "gpt2_impact_df.to_csv(output_dirs['gpt2'] / 'prediction_impact.csv', index=False)\n",
    "print(f\"\\n Saved to: {output_dirs['gpt2'] / 'prediction_impact.csv'}\")\n",
    "\n",
    "# Cleanup\n",
    "del gpt2_ablation_model\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 3: Comprehensive Visualizations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 DistilBERT Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:53:36.592910Z",
     "iopub.status.busy": "2025-12-07T17:53:36.592586Z",
     "iopub.status.idle": "2025-12-07T17:53:39.071797Z",
     "shell.execute_reply": "2025-12-07T17:53:39.071091Z",
     "shell.execute_reply.started": "2025-12-07T17:53:36.592879Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create comprehensive visualization for DistilBERT benchmarks\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('DistilBERT Quantization Benchmark Results (SST-2)', fontsize=16, fontweight='bold')\n",
    "\n",
    "colors = sns.color_palette('husl', n_colors=len(df_distilbert))\n",
    "\n",
    "# 1. Latency comparison\n",
    "ax = axes[0, 0]\n",
    "ax.bar(df_distilbert['name'], df_distilbert['mean_latency_ms'], color=colors, alpha=0.8, edgecolor='black')\n",
    "ax.errorbar(df_distilbert['name'], df_distilbert['mean_latency_ms'], \n",
    "            yerr=df_distilbert['std_latency_ms'], fmt='none', color='black', capsize=5)\n",
    "ax.set_ylabel('Latency (ms/batch)', fontweight='bold')\n",
    "ax.set_title('Inference Latency', fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Speedup vs FP32\n",
    "ax = axes[0, 1]\n",
    "ax.bar(df_distilbert['name'], df_distilbert['speedup_vs_fp32'], color=colors, alpha=0.8, edgecolor='black')\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', linewidth=2, label='FP32 Baseline')\n",
    "ax.set_ylabel('Speedup (x)', fontweight='bold')\n",
    "ax.set_title('Speedup vs FP32', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Energy per sample\n",
    "ax = axes[0, 2]\n",
    "if df_distilbert['energy_per_sample_mj'].max() > 0:\n",
    "    ax.bar(df_distilbert['name'], df_distilbert['energy_per_sample_mj'], color=colors, alpha=0.8, edgecolor='black')\n",
    "    ax.set_ylabel('Energy (mJ/sample)', fontweight='bold')\n",
    "    ax.set_title('Energy per Sample', fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Throughput\n",
    "ax = axes[1, 0]\n",
    "ax.bar(df_distilbert['name'], df_distilbert['throughput'], color=colors, alpha=0.8, edgecolor='black')\n",
    "ax.set_ylabel('Throughput (samples/sec)', fontweight='bold')\n",
    "ax.set_title('Inference Throughput', fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 5. Accuracy\n",
    "ax = axes[1, 1]\n",
    "ax.bar(df_distilbert['name'], df_distilbert['accuracy'], color=colors, alpha=0.8, edgecolor='black')\n",
    "ax.set_ylabel('Accuracy (%)', fontweight='bold')\n",
    "ax.set_title('Classification Accuracy', fontweight='bold')\n",
    "ax.set_ylim([min(df_distilbert['accuracy']) - 1, 100])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 6. Model size\n",
    "ax = axes[1, 2]\n",
    "ax.bar(df_distilbert['name'], df_distilbert['model_size_mb'], color=colors, alpha=0.8, edgecolor='black')\n",
    "ax.set_ylabel('Size (MB)', fontweight='bold')\n",
    "ax.set_title('Model Size', fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dirs['distilbert'] / 'benchmark_overview.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"DistilBERT benchmark visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:53:39.075987Z",
     "iopub.status.busy": "2025-12-07T17:53:39.075539Z",
     "iopub.status.idle": "2025-12-07T17:53:42.161432Z",
     "shell.execute_reply": "2025-12-07T17:53:42.160789Z",
     "shell.execute_reply.started": "2025-12-07T17:53:39.075968Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# DistilBERT Per-Layer Energy Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('DistilBERT Per-Layer Energy Analysis (FP32)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Top 15 layers by energy\n",
    "ax = axes[0, 0]\n",
    "top_layers = distilbert_layer_stats.head(15)\n",
    "y_pos = range(len(top_layers))\n",
    "ax.barh(y_pos, top_layers[\"energy_mj\"], alpha=0.7)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels([name.split('.')[-1] if len(name.split('.')) > 1 else name for name in top_layers[\"layer_name\"]], fontsize=8)\n",
    "ax.set_xlabel(\"Energy (mJ)\", fontsize=10)\n",
    "ax.set_title(\"Top 15 Layers by Energy Consumption\", fontsize=12, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 2. Energy by layer type\n",
    "ax = axes[0, 1]\n",
    "type_energy_sorted = distilbert_type_energy.sort_values(\"energy_mj\", ascending=False)\n",
    "y_pos = range(len(type_energy_sorted))\n",
    "colors_type = plt.cm.Set3(range(len(type_energy_sorted)))\n",
    "ax.barh(y_pos, type_energy_sorted[\"energy_mj\"], alpha=0.7, color=colors_type)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(type_energy_sorted.index, fontsize=10)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel(\"Energy (mJ)\", fontsize=10)\n",
    "ax.set_title(\"Energy Consumption by Layer Type\", fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 3. Pie chart of energy distribution\n",
    "ax = axes[1, 0]\n",
    "type_energy_pie = distilbert_type_energy[distilbert_type_energy[\"percent_energy\"] > 1.0]\n",
    "ax.pie(type_energy_pie[\"percent_energy\"], labels=type_energy_pie.index, autopct='%1.1f%%', startangle=90)\n",
    "ax.set_title(\"Energy Distribution by Layer Type\", fontsize=12, fontweight='bold')\n",
    "\n",
    "# 4. Cumulative energy contribution\n",
    "ax = axes[1, 1]\n",
    "cumulative = distilbert_layer_stats[\"energy_mj\"].cumsum() / distilbert_layer_stats[\"energy_mj\"].sum() * 100\n",
    "ax.plot(range(len(cumulative)), cumulative, linewidth=2)\n",
    "ax.axhline(y=80, color='r', linestyle='--', alpha=0.7, label='80% threshold')\n",
    "ax.axhline(y=90, color='orange', linestyle='--', alpha=0.7, label='90% threshold')\n",
    "ax.set_xlabel(\"Number of Layers\", fontsize=10)\n",
    "ax.set_ylabel(\"Cumulative Energy (%)\", fontsize=10)\n",
    "ax.set_title(\"Cumulative Energy Contribution\", fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dirs['distilbert'] / 'per_layer_energy_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"DistilBERT per-layer energy visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3: Speed-Accuracy Trade-off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:53:42.162225Z",
     "iopub.status.busy": "2025-12-07T17:53:42.162044Z",
     "iopub.status.idle": "2025-12-07T17:53:43.217403Z",
     "shell.execute_reply": "2025-12-07T17:53:43.216797Z",
     "shell.execute_reply.started": "2025-12-07T17:53:42.162211Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Figure 3: Speed-Accuracy Trade-off\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "for _, row in df_distilbert.iterrows():\n",
    "    ax.scatter(row['mean_latency_ms'], row['accuracy'], s=200, alpha=0.7, label=row['name'])\n",
    "    ax.annotate(row['name'], (row['mean_latency_ms'], row['accuracy']),\n",
    "                xytext=(10, 10), textcoords='offset points', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Latency (ms/batch) - Lower is Better', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy (%) - Higher is Better', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Speed vs Accuracy Trade-off', fontsize=16, fontweight='bold')\n",
    "\n",
    "best_latency = df_distilbert['mean_latency_ms'].min()\n",
    "best_accuracy = df_distilbert['accuracy'].max()\n",
    "ax.axvline(x=best_latency, color='gray', linestyle='--', alpha=0.5, label='Best Latency')\n",
    "ax.axhline(y=best_accuracy, color='gray', linestyle='--', alpha=0.5, label='Best Accuracy')\n",
    "\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dirs['distilbert'] / 'speed_accuracy_tradeoff.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Speed-Accuracy Trade-off saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 6: Prediction Impact by Layer Type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:53:43.218397Z",
     "iopub.status.busy": "2025-12-07T17:53:43.218142Z",
     "iopub.status.idle": "2025-12-07T17:53:44.136653Z",
     "shell.execute_reply": "2025-12-07T17:53:44.135848Z",
     "shell.execute_reply.started": "2025-12-07T17:53:43.218372Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Figure 6: Prediction Impact by Layer Type\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "type_impact_sorted = distilbert_type_impact.sort_values(\"impact_score\", ascending=True)\n",
    "y_pos = range(len(type_impact_sorted))\n",
    "colors_type = plt.cm.Set3(range(len(type_impact_sorted)))\n",
    "\n",
    "bars = ax.barh(y_pos, type_impact_sorted[\"impact_score\"], alpha=0.7, color=colors_type)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(type_impact_sorted.index, fontsize=12)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel(\"Prediction Impact Score\", fontsize=14, fontweight='bold')\n",
    "ax.set_title(\"Model Prediction Impact by Layer Type\", fontsize=16, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "for i, (idx, row) in enumerate(type_impact_sorted.iterrows()):\n",
    "    ax.text(row[\"impact_score\"] * 0.02, i, f'{row[\"impact_score\"]:.3f}',\n",
    "            va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dirs['distilbert'] / 'prediction_impact_by_type.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Prediction Impact by Layer Type saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 7 & 8: Energy-Impact Correlation Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:53:44.138214Z",
     "iopub.status.busy": "2025-12-07T17:53:44.137550Z",
     "iopub.status.idle": "2025-12-07T17:53:46.774065Z",
     "shell.execute_reply": "2025-12-07T17:53:46.773430Z",
     "shell.execute_reply.started": "2025-12-07T17:53:44.138193Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Merge energy and impact data\n",
    "merged_distilbert = distilbert_layer_stats[[\"layer_name\", \"energy_j\", \"energy_mj\", \"layer_type\"]].merge(\n",
    "    distilbert_impact_df[[\"layer_name\", \"kl_divergence\", \"logit_l2\", \"accuracy_drop\", \"impact_score\"]],\n",
    "    on=\"layer_name\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Normalize for correlation\n",
    "merged_distilbert[\"energy_normalized\"] = (merged_distilbert[\"energy_mj\"] - merged_distilbert[\"energy_mj\"].min()) / \\\n",
    "                                          (merged_distilbert[\"energy_mj\"].max() - merged_distilbert[\"energy_mj\"].min())\n",
    "merged_distilbert[\"impact_normalized\"] = (merged_distilbert[\"impact_score\"] - merged_distilbert[\"impact_score\"].min()) / \\\n",
    "                                           (merged_distilbert[\"impact_score\"].max() - merged_distilbert[\"impact_score\"].min())\n",
    "\n",
    "# Figure 7: Scatter plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "layer_types = merged_distilbert[\"layer_type\"].unique()\n",
    "colors_map = dict(zip(layer_types, plt.cm.Set3(range(len(layer_types)))))\n",
    "\n",
    "for layer_type in layer_types:\n",
    "    mask = merged_distilbert[\"layer_type\"] == layer_type\n",
    "    ax.scatter(\n",
    "        merged_distilbert.loc[mask, \"energy_mj\"],\n",
    "        merged_distilbert.loc[mask, \"impact_score\"],\n",
    "        label=layer_type,\n",
    "        alpha=0.6,\n",
    "        s=100,\n",
    "        color=colors_map[layer_type]\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Energy Consumption (mJ)\", fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel(\"Prediction Impact Score\", fontsize=14, fontweight='bold')\n",
    "ax.set_title(\"Energy Consumption vs Prediction Impact\\n(All {} DistilBERT Layers)\".format(len(merged_distilbert)),\n",
    "             fontsize=16, fontweight='bold')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "corr_coef = merged_distilbert[\"energy_mj\"].corr(merged_distilbert[\"impact_score\"])\n",
    "ax.text(0.05, 0.95, f'Correlation: {corr_coef:.3f}',\n",
    "        transform=ax.transAxes, fontsize=12, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "        verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dirs['distilbert'] / 'energy_vs_impact_scatter.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Energy vs Impact Scatter (correlation={corr_coef:.3f})\")\n",
    "\n",
    "# Figure 8: Correlation Matrix\n",
    "corr_columns = [\"energy_j\", \"energy_mj\", \"kl_divergence\", \"logit_l2\", \"accuracy_drop\", \"impact_score\",\n",
    "                \"energy_normalized\", \"impact_normalized\"]\n",
    "corr_matrix = merged_distilbert[corr_columns].corr()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.8},\n",
    "    mask=mask,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title(\"Correlation Matrix: Energy Consumption vs Prediction Impact\\n(All DistilBERT Layers)\",\n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dirs['distilbert'] / 'energy_impact_correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Correlation Matrix saved\")\n",
    "\n",
    "# Save merged data\n",
    "merged_distilbert.to_csv(output_dirs['distilbert'] / 'energy_impact_merged.csv', index=False)\n",
    "print(f\"\\n Merged data saved ({len(merged_distilbert)} layers)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 GPT-2 Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:53:46.775087Z",
     "iopub.status.busy": "2025-12-07T17:53:46.774760Z",
     "iopub.status.idle": "2025-12-07T17:53:49.276049Z",
     "shell.execute_reply": "2025-12-07T17:53:49.275270Z",
     "shell.execute_reply.started": "2025-12-07T17:53:46.775069Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create comprehensive visualization for GPT-2 benchmarks\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('GPT-2 Small Quantization Benchmark Results (WikiText-2)', fontsize=16, fontweight='bold')\n",
    "\n",
    "colors = sns.color_palette('husl', n_colors=len(df_gpt2))\n",
    "\n",
    "# 1. Latency comparison\n",
    "ax = axes[0, 0]\n",
    "ax.bar(df_gpt2['name'], df_gpt2['mean_latency_ms'], color=colors, alpha=0.8, edgecolor='black')\n",
    "ax.errorbar(df_gpt2['name'], df_gpt2['mean_latency_ms'], \n",
    "            yerr=df_gpt2['std_latency_ms'], fmt='none', color='black', capsize=5)\n",
    "ax.set_ylabel('Latency (ms/sample)', fontweight='bold')\n",
    "ax.set_title('Inference Latency', fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Speedup vs FP32\n",
    "ax = axes[0, 1]\n",
    "ax.bar(df_gpt2['name'], df_gpt2['speedup_vs_fp32'], color=colors, alpha=0.8, edgecolor='black')\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', linewidth=2, label='FP32 Baseline')\n",
    "ax.set_ylabel('Speedup (x)', fontweight='bold')\n",
    "ax.set_title('Speedup vs FP32', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Energy per sample\n",
    "ax = axes[0, 2]\n",
    "if df_gpt2['energy_per_sample_mj'].max() > 0:\n",
    "    ax.bar(df_gpt2['name'], df_gpt2['energy_per_sample_mj'], color=colors, alpha=0.8, edgecolor='black')\n",
    "    ax.set_ylabel('Energy (mJ/sample)', fontweight='bold')\n",
    "    ax.set_title('Energy per Sample', fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Throughput\n",
    "ax = axes[1, 0]\n",
    "ax.bar(df_gpt2['name'], df_gpt2['throughput'], color=colors, alpha=0.8, edgecolor='black')\n",
    "ax.set_ylabel('Throughput (samples/sec)', fontweight='bold')\n",
    "ax.set_title('Inference Throughput', fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 5. Perplexity\n",
    "ax = axes[1, 1]\n",
    "ax.bar(df_gpt2['name'], df_gpt2['perplexity'], color=colors, alpha=0.8, edgecolor='black')\n",
    "ax.set_ylabel('Perplexity', fontweight='bold')\n",
    "ax.set_title('Model Perplexity (Lower is Better)', fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 6. Model size\n",
    "ax = axes[1, 2]\n",
    "ax.bar(df_gpt2['name'], df_gpt2['model_size_mb'], color=colors, alpha=0.8, edgecolor='black')\n",
    "ax.set_ylabel('Size (MB)', fontweight='bold')\n",
    "ax.set_title('Model Size', fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dirs['gpt2'] / 'benchmark_overview.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"GPT-2 benchmark visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:53:49.277322Z",
     "iopub.status.busy": "2025-12-07T17:53:49.276879Z",
     "iopub.status.idle": "2025-12-07T17:53:51.621133Z",
     "shell.execute_reply": "2025-12-07T17:53:51.620417Z",
     "shell.execute_reply.started": "2025-12-07T17:53:49.277302Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# GPT-2 Per-Layer Energy Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('GPT-2 Small Per-Layer Energy Analysis (FP32)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Top 15 layers by energy\n",
    "ax = axes[0, 0]\n",
    "top_layers = gpt2_layer_stats.head(15)\n",
    "y_pos = range(len(top_layers))\n",
    "ax.barh(y_pos, top_layers[\"energy_mj\"], alpha=0.7)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels([name.split('.')[-1] if len(name.split('.')) > 1 else name for name in top_layers[\"layer_name\"]], fontsize=8)\n",
    "ax.set_xlabel(\"Energy (mJ)\", fontsize=10)\n",
    "ax.set_title(\"Top 15 Layers by Energy Consumption\", fontsize=12, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 2. Energy by layer type\n",
    "ax = axes[0, 1]\n",
    "type_energy_sorted = gpt2_type_energy.sort_values(\"energy_mj\", ascending=True)\n",
    "y_pos = range(len(type_energy_sorted))\n",
    "colors_type = plt.cm.Set3(range(len(type_energy_sorted)))\n",
    "ax.barh(y_pos, type_energy_sorted[\"energy_mj\"], alpha=0.7, color=colors_type)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(type_energy_sorted.index, fontsize=10)\n",
    "ax.set_xlabel(\"Energy (mJ)\", fontsize=10)\n",
    "ax.set_title(\"Energy Consumption by Layer Type\", fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 3. Pie chart of energy distribution\n",
    "ax = axes[1, 0]\n",
    "type_energy_pie = gpt2_type_energy[gpt2_type_energy[\"percent_energy\"] > 1.0]\n",
    "ax.pie(type_energy_pie[\"percent_energy\"], labels=type_energy_pie.index, autopct='%1.1f%%', startangle=90)\n",
    "ax.set_title(\"Energy Distribution by Layer Type\", fontsize=12, fontweight='bold')\n",
    "\n",
    "# 4. Cumulative energy contribution\n",
    "ax = axes[1, 1]\n",
    "cumulative = gpt2_layer_stats[\"energy_mj\"].cumsum() / gpt2_layer_stats[\"energy_mj\"].sum() * 100\n",
    "ax.plot(range(len(cumulative)), cumulative, linewidth=2)\n",
    "ax.axhline(y=80, color='r', linestyle='--', alpha=0.7, label='80% threshold')\n",
    "ax.axhline(y=90, color='orange', linestyle='--', alpha=0.7, label='90% threshold')\n",
    "ax.set_xlabel(\"Number of Layers\", fontsize=10)\n",
    "ax.set_ylabel(\"Cumulative Energy (%)\", fontsize=10)\n",
    "ax.set_title(\"Cumulative Energy Contribution\", fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dirs['gpt2'] / 'per_layer_energy_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"GPT-2 per-layer energy visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 10: GPT-2 Prediction Impact by Layer Type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:53:51.622193Z",
     "iopub.status.busy": "2025-12-07T17:53:51.621919Z",
     "iopub.status.idle": "2025-12-07T17:53:52.445281Z",
     "shell.execute_reply": "2025-12-07T17:53:52.444502Z",
     "shell.execute_reply.started": "2025-12-07T17:53:51.622174Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Figure 10: GPT-2 Prediction Impact by Layer Type\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "type_impact_sorted = gpt2_type_impact.sort_values(\"impact_score\", ascending=True)\n",
    "y_pos = range(len(type_impact_sorted))\n",
    "colors_type = plt.cm.Set3(range(len(type_impact_sorted)))\n",
    "\n",
    "bars = ax.barh(y_pos, type_impact_sorted[\"impact_score\"], alpha=0.7, color=colors_type)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(type_impact_sorted.index, fontsize=12)\n",
    "ax.set_xlabel(\"Prediction Impact Score\", fontsize=14, fontweight='bold')\n",
    "ax.set_title(\"GPT-2 prediction-impact scores across major layer categories\", fontsize=16, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "for i, (idx, row) in enumerate(type_impact_sorted.iterrows()):\n",
    "    ax.text(row[\"impact_score\"] + (type_impact_sorted[\"impact_score\"].max() * 0.02), i,\n",
    "            f'{row[\"impact_score\"]:.3f}',\n",
    "            va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dirs['gpt2'] / 'prediction_impact_by_type.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"GPT-2 Prediction Impact by Layer Type saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 11 & 12: GPT-2 Energy-Impact Correlation Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:53:52.446383Z",
     "iopub.status.busy": "2025-12-07T17:53:52.446130Z",
     "iopub.status.idle": "2025-12-07T17:53:55.307784Z",
     "shell.execute_reply": "2025-12-07T17:53:55.307156Z",
     "shell.execute_reply.started": "2025-12-07T17:53:52.446366Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Merge energy and impact data\n",
    "merged_gpt2 = gpt2_layer_stats[[\"layer_name\", \"energy_j\", \"energy_mj\", \"mean_time_ms\", \"layer_type\"]].merge(\n",
    "    gpt2_impact_df[[\"layer_name\", \"kl_divergence\", \"logit_l2\", \"perplexity_change\", \"impact_score\"]],\n",
    "    on=\"layer_name\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Normalize for correlation\n",
    "merged_gpt2[\"energy_normalized\"] = (merged_gpt2[\"energy_mj\"] - merged_gpt2[\"energy_mj\"].min()) / \\\n",
    "                                    (merged_gpt2[\"energy_mj\"].max() - merged_gpt2[\"energy_mj\"].min())\n",
    "merged_gpt2[\"impact_normalized\"] = (merged_gpt2[\"impact_score\"] - merged_gpt2[\"impact_score\"].min()) / \\\n",
    "                                     (merged_gpt2[\"impact_score\"].max() - merged_gpt2[\"impact_score\"].min())\n",
    "\n",
    "# Figure 11: Scatter plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "\n",
    "layer_types = merged_gpt2[\"layer_type\"].unique()\n",
    "colors_map = dict(zip(layer_types, plt.cm.Set3(range(len(layer_types)))))\n",
    "\n",
    "for layer_type in layer_types:\n",
    "    mask = merged_gpt2[\"layer_type\"] == layer_type\n",
    "    ax.scatter(\n",
    "        merged_gpt2.loc[mask, \"energy_mj\"],\n",
    "        merged_gpt2.loc[mask, \"impact_score\"],\n",
    "        label=layer_type,\n",
    "        alpha=0.6,\n",
    "        s=100,\n",
    "        color=colors_map[layer_type]\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Energy Consumption (mJ)\", fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel(\"Prediction Impact Score\", fontsize=14, fontweight='bold')\n",
    "ax.set_title(\"Energy consumption versus prediction impact for\\nall GPT-2 layers\",\n",
    "             fontsize=16, fontweight='bold')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "corr_coef = merged_gpt2[\"energy_mj\"].corr(merged_gpt2[\"impact_score\"])\n",
    "ax.text(0.05, 0.95, f'Pearson Correlation: {corr_coef:.3f}',\n",
    "        transform=ax.transAxes, fontsize=13, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7),\n",
    "        verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dirs['gpt2'] / 'energy_vs_impact_scatter.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Energy vs Impact Scatter (correlation={corr_coef:.3f})\")\n",
    "\n",
    "# Figure 12: Correlation Matrix\n",
    "corr_columns = [\"energy_j\", \"energy_mj\", \"mean_time_ms\", \"kl_divergence\", \"logit_l2\",\n",
    "                \"perplexity_change\", \"impact_score\", \"energy_normalized\", \"impact_normalized\"]\n",
    "corr_matrix = merged_gpt2[corr_columns].corr()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 12))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.8},\n",
    "    mask=mask,\n",
    "    ax=ax,\n",
    "    vmin=-1,\n",
    "    vmax=1\n",
    ")\n",
    "\n",
    "ax.set_title(\"Correlation Matrix: Energy vs Prediction Impact\\n(All {} GPT-2 Layers)\".format(len(merged_gpt2)),\n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dirs['gpt2'] / 'energy_impact_correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Correlation Matrix saved\")\n",
    "\n",
    "# Print correlation insights\n",
    "print(f\"\\n Key Correlations (GPT-2):\")\n",
    "print(f\"  Energy vs Impact Score:      {corr_matrix.loc['energy_mj', 'impact_score']:.4f}\")\n",
    "print(f\"  Energy vs KL Divergence:     {corr_matrix.loc['energy_mj', 'kl_divergence']:.4f}\")\n",
    "print(f\"  Energy vs Logit L2:          {corr_matrix.loc['energy_mj', 'logit_l2']:.4f}\")\n",
    "\n",
    "# Save merged data\n",
    "merged_gpt2.to_csv(output_dirs['gpt2'] / 'energy_impact_merged.csv', index=False)\n",
    "print(f\"\\n Merged data saved ({len(merged_gpt2)} layers)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Comparative Analysis: DistilBERT vs GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:53:55.308761Z",
     "iopub.status.busy": "2025-12-07T17:53:55.308499Z",
     "iopub.status.idle": "2025-12-07T17:53:57.157559Z",
     "shell.execute_reply": "2025-12-07T17:53:57.156825Z",
     "shell.execute_reply.started": "2025-12-07T17:53:55.308728Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create side-by-side comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('Comparative Analysis: DistilBERT vs GPT-2 Small', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Speedup comparison\n",
    "ax = axes[0, 0]\n",
    "x = np.arange(2)\n",
    "width = 0.35\n",
    "distilbert_fp16_speedup = df_distilbert[df_distilbert['name'] == 'FP16']['speedup_vs_fp32'].values[0]\n",
    "gpt2_fp16_speedup = df_gpt2[df_gpt2['name'] == 'FP16']['speedup_vs_fp32'].values[0]\n",
    "ax.bar(x[0], distilbert_fp16_speedup, width, label='DistilBERT', color='#1f77b4', alpha=0.8)\n",
    "ax.bar(x[1], gpt2_fp16_speedup, width, label='GPT-2', color='#ff7f0e', alpha=0.8)\n",
    "ax.set_ylabel('Speedup (x)', fontweight='bold')\n",
    "ax.set_title('FP16 Speedup vs FP32', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['DistilBERT', 'GPT-2'])\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', alpha=0.5)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Energy efficiency comparison\n",
    "ax = axes[0, 1]\n",
    "distilbert_fp16_energy_eff = df_distilbert[df_distilbert['name'] == 'FP16']['energy_reduction_vs_fp32'].values[0]\n",
    "gpt2_fp16_energy_eff = df_gpt2[df_gpt2['name'] == 'FP16']['energy_reduction_vs_fp32'].values[0]\n",
    "ax.bar(x[0], distilbert_fp16_energy_eff, width, label='DistilBERT', color='#1f77b4', alpha=0.8)\n",
    "ax.bar(x[1], gpt2_fp16_energy_eff, width, label='GPT-2', color='#ff7f0e', alpha=0.8)\n",
    "ax.set_ylabel('Energy Efficiency (x)', fontweight='bold')\n",
    "ax.set_title('FP16 Energy Efficiency vs FP32', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['DistilBERT', 'GPT-2'])\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', alpha=0.5)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Model size comparison\n",
    "ax = axes[1, 0]\n",
    "distilbert_fp32_size = df_distilbert[df_distilbert['name'] == 'FP32']['model_size_mb'].values[0]\n",
    "distilbert_fp16_size = df_distilbert[df_distilbert['name'] == 'FP16']['model_size_mb'].values[0]\n",
    "gpt2_fp32_size = df_gpt2[df_gpt2['name'] == 'FP32']['model_size_mb'].values[0]\n",
    "gpt2_fp16_size = df_gpt2[df_gpt2['name'] == 'FP16']['model_size_mb'].values[0]\n",
    "\n",
    "x_pos = np.arange(2)\n",
    "width = 0.35\n",
    "ax.bar(x_pos - width/2, [distilbert_fp32_size, gpt2_fp32_size], width, label='FP32', alpha=0.8)\n",
    "ax.bar(x_pos + width/2, [distilbert_fp16_size, gpt2_fp16_size], width, label='FP16', alpha=0.8)\n",
    "ax.set_ylabel('Model Size (MB)', fontweight='bold')\n",
    "ax.set_title('Model Size Comparison', fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(['DistilBERT', 'GPT-2'])\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Number of parameters\n",
    "ax = axes[1, 1]\n",
    "params = [67, 124]  # DistilBERT: 67M, GPT-2: 124M\n",
    "ax.bar(x, params, color=['#1f77b4', '#ff7f0e'], alpha=0.8)\n",
    "ax.set_ylabel('Parameters (Millions)', fontweight='bold')\n",
    "ax.set_title('Model Parameter Count', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['DistilBERT\\n(67M)', 'GPT-2\\n(124M)'])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dirs['base'] / 'comparative_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Comparative analysis visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Summary Statistics and Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T17:53:57.158567Z",
     "iopub.status.busy": "2025-12-07T17:53:57.158296Z",
     "iopub.status.idle": "2025-12-07T17:53:57.173075Z",
     "shell.execute_reply": "2025-12-07T17:53:57.172287Z",
     "shell.execute_reply.started": "2025-12-07T17:53:57.158544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n DISTILBERT (SST-2 Sentiment Classification)\")\n",
    "\n",
    "for _, row in df_distilbert.iterrows():\n",
    "    print(f\"\\n{row['name']}:\")\n",
    "    print(f\"  Latency:    {row['mean_latency_ms']:.2f} ± {row['std_latency_ms']:.2f} ms/batch\")\n",
    "    print(f\"  Throughput: {row['throughput']:.1f} samples/sec\")\n",
    "    print(f\"  Accuracy:   {row['accuracy']:.2f}%\")\n",
    "    print(f\"  Energy:     {row['energy_per_sample_mj']:.2f} mJ/sample\")\n",
    "    print(f\"  Model Size: {row['model_size_mb']:.2f} MB\")\n",
    "    if row['name'] == 'FP16':\n",
    "        print(f\"  Speedup:    {row['speedup_vs_fp32']:.2f}x faster\")\n",
    "        print(f\"  Energy Eff: {row['energy_reduction_vs_fp32']:.2f}x more efficient\")\n",
    "\n",
    "print(\"\\n GPT-2 SMALL (WikiText-2 Language Modeling)\")\n",
    "\n",
    "for _, row in df_gpt2.iterrows():\n",
    "    print(f\"\\n{row['name']}:\")\n",
    "    print(f\"  Latency:     {row['mean_latency_ms']:.2f} ± {row['std_latency_ms']:.2f} ms/sample\")\n",
    "    print(f\"  Throughput:  {row['throughput']:.1f} samples/sec\")\n",
    "    print(f\"  Tokens/sec:  {row['tokens_per_sec']:.1f}\")\n",
    "    print(f\"  Perplexity:  {row['perplexity']:.2f}\")\n",
    "    print(f\"  Energy:      {row['energy_per_sample_mj']:.2f} mJ/sample\")\n",
    "    print(f\"  Model Size:  {row['model_size_mb']:.2f} MB\")\n",
    "    if row['name'] == 'FP16':\n",
    "        print(f\"  Speedup:     {row['speedup_vs_fp32']:.2f}x faster\")\n",
    "        print(f\"  Energy Eff:  {row['energy_reduction_vs_fp32']:.2f}x more efficient\")\n",
    "\n",
    "print(\"\\n KEY FINDINGS\")\n",
    "\n",
    "print(\"\\n1. FP16 quantization provides significant benefits:\")\n",
    "print(f\"   - DistilBERT: {df_distilbert[df_distilbert['name']=='FP16']['speedup_vs_fp32'].values[0]:.2f}x speedup, {df_distilbert[df_distilbert['name']=='FP16']['energy_reduction_vs_fp32'].values[0]:.2f}x energy reduction\")\n",
    "print(f\"   - GPT-2:      {df_gpt2[df_gpt2['name']=='FP16']['speedup_vs_fp32'].values[0]:.2f}x speedup, {df_gpt2[df_gpt2['name']=='FP16']['energy_reduction_vs_fp32'].values[0]:.2f}x energy reduction\")\n",
    "print(\"\\n2. No quality degradation observed:\")\n",
    "print(f\"   - DistilBERT accuracy unchanged: {df_distilbert[df_distilbert['name']=='FP32']['accuracy'].values[0]:.2f}% (FP32) vs {df_distilbert[df_distilbert['name']=='FP16']['accuracy'].values[0]:.2f}% (FP16)\")\n",
    "print(f\"   - GPT-2 perplexity nearly identical: {df_gpt2[df_gpt2['name']=='FP32']['perplexity'].values[0]:.2f} (FP32) vs {df_gpt2[df_gpt2['name']=='FP16']['perplexity'].values[0]:.2f} (FP16)\")\n",
    "print(\"\\n3. Model size reduced by 50% with FP16\")\n",
    "print(\"\\n4. Per-layer profiling identifies quantization candidates\")\n",
    "\n",
    "print(\"\\n All results saved to:\", output_dirs['base'])\n",
    "print(\"  - DistilBERT results:\", output_dirs['distilbert'])\n",
    "print(\"  - GPT-2 results:     \", output_dirs['gpt2'])\n",
    "print(\"  - Datasets:          \", output_dirs['datasets'])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

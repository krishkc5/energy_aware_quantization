{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Day 1: Pre-tokenized Dataset for Energy Measurement\n## Optimized for Kaggle GPU P100\n\nThis notebook creates a pre-tokenized dataset with **ZERO I/O overhead** during energy measurements.\n\n**⚠️ IMPORTANT: Enable GPU in Kaggle**\n- Go to Settings (right panel) → Accelerator → GPU P100\n- Click \"Save\" and wait for session to restart","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Verify GPU Access","metadata":{}},{"cell_type":"code","source":"import torch\n\n# Check GPU availability\nprint(\"=\"*60)\nprint(\"GPU CHECK\")\nprint(\"=\"*60)\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"CUDA version: {torch.version.cuda}\")\n    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n    print(f\"Device count: {torch.cuda.device_count()}\")\n    print(f\"Current device: {torch.cuda.current_device()}\")\n    \n    # Check memory\n    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"Total GPU memory: {total_memory:.2f} GB\")\n    print(\"\\n✓ GPU is ready!\")\nelse:\n    print(\"\\n⚠️ WARNING: GPU not available!\")\n    print(\"Please enable GPU in Kaggle settings (Accelerator → GPU P100)\")\n\nprint(\"=\"*60)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T02:46:25.124782Z","iopub.execute_input":"2025-12-01T02:46:25.125452Z","iopub.status.idle":"2025-12-01T02:46:25.131727Z","shell.execute_reply.started":"2025-12-01T02:46:25.125426Z","shell.execute_reply":"2025-12-01T02:46:25.131010Z"}},"outputs":[{"name":"stdout","text":"============================================================\nGPU CHECK\n============================================================\nPyTorch version: 2.6.0+cu124\nCUDA available: True\nCUDA version: 12.4\nDevice name: Tesla P100-PCIE-16GB\nDevice count: 1\nCurrent device: 0\nTotal GPU memory: 17.06 GB\n\n✓ GPU is ready!\n============================================================\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Step 2: Install Dependencies\n\nKaggle has most packages pre-installed, but we'll ensure we have the latest versions.","metadata":{}},{"cell_type":"code","source":"# Install/upgrade required packages\n!pip install -q transformers datasets accelerate\n\nprint(\"✓ Dependencies installed/verified\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T02:46:25.132847Z","iopub.execute_input":"2025-12-01T02:46:25.133298Z","iopub.status.idle":"2025-12-01T02:47:52.452343Z","shell.execute_reply.started":"2025-12-01T02:46:25.133281Z","shell.execute_reply":"2025-12-01T02:47:52.451544Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m✓ Dependencies installed/verified\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Step 3: Define Dataset Preparation Functions\n\nThese functions will create our pre-tokenized dataset.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nDataset Preparation Module\nPre-tokenize dataset to eliminate I/O overhead during energy measurement\n\"\"\"\n\nimport torch\nfrom transformers import DistilBertTokenizer\nfrom datasets import load_dataset\nfrom pathlib import Path\nimport json\n\n\ndef prepare_tokenized_dataset(\n    num_samples: int = 50,\n    max_length: int = 128,\n    dataset_name: str = \"sst2\",\n    output_dir: str = \"/kaggle/working/tokenized_data\",\n    seed: int = 42\n):\n    \"\"\"\n    Pre-tokenize dataset and save to disk.\n    \n    Args:\n        num_samples: Number of examples to tokenize (30-50 recommended)\n        max_length: Maximum sequence length (128 is good for DistilBERT)\n        dataset_name: Which GLUE task to use (\"sst2\", \"mnli\")\n        output_dir: Directory to save tokenized data\n        seed: Random seed for reproducibility\n    \"\"\"\n    \n    print(\"=\"*60)\n    print(\"Pre-tokenizing Dataset for Energy Measurement\")\n    print(\"=\"*60)\n    \n    # Create output directory\n    output_path = Path(output_dir)\n    output_path.mkdir(exist_ok=True)\n    \n    # Load tokenizer\n    print(\"\\n[1/5] Loading DistilBERT tokenizer...\")\n    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n    \n    # Load dataset\n    print(f\"[2/5] Loading {dataset_name} validation set...\")\n    if dataset_name == \"sst2\":\n        dataset = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n        text_key = \"sentence\"\n        label_key = \"label\"\n        num_labels = 2\n    elif dataset_name == \"mnli\":\n        dataset = load_dataset(\"glue\", \"mnli\", split=\"validation_matched\")\n        text_key = \"premise\"\n        label_key = \"label\"\n        num_labels = 3\n    else:\n        raise ValueError(f\"Dataset {dataset_name} not supported\")\n    \n    # Sample examples\n    print(f\"[3/5] Selecting {num_samples} examples (seed={seed})...\")\n    dataset = dataset.shuffle(seed=seed).select(range(num_samples))\n    \n    # Tokenize all examples\n    print(f\"[4/5] Tokenizing with max_length={max_length}...\")\n    texts = [example[text_key] for example in dataset]\n    labels = [example[label_key] for example in dataset]\n    \n    # Tokenize in batch\n    encodings = tokenizer(\n        texts,\n        padding='max_length',\n        truncation=True,\n        max_length=max_length,\n        return_tensors='pt'\n    )\n    \n    labels_tensor = torch.tensor(labels, dtype=torch.long)\n    \n    # Save tensors\n    print(f\"[5/5] Saving to {output_dir}...\")\n    torch.save(encodings['input_ids'], output_path / 'input_ids.pt')\n    torch.save(encodings['attention_mask'], output_path / 'attention_mask.pt')\n    torch.save(labels_tensor, output_path / 'labels.pt')\n    \n    # Save metadata\n    metadata = {\n        'num_samples': num_samples,\n        'max_length': max_length,\n        'dataset_name': dataset_name,\n        'num_labels': num_labels,\n        'seed': seed,\n        'tokenizer': 'distilbert-base-uncased',\n    }\n    \n    with open(output_path / 'metadata.json', 'w') as f:\n        json.dump(metadata, f, indent=2)\n    \n    # Print summary\n    print(\"\\n\" + \"=\"*60)\n    print(\"Dataset Preparation Complete!\")\n    print(\"=\"*60)\n    print(f\"Number of samples:     {num_samples}\")\n    print(f\"Max sequence length:   {max_length}\")\n    print(f\"Dataset:               {dataset_name}\")\n    print(f\"Number of labels:      {num_labels}\")\n    print(f\"\\nSaved files:\")\n    print(f\"  - input_ids.pt       {encodings['input_ids'].shape}\")\n    print(f\"  - attention_mask.pt  {encodings['attention_mask'].shape}\")\n    print(f\"  - labels.pt          {labels_tensor.shape}\")\n    print(f\"  - metadata.json\")\n    print(\"=\"*60)\n    \n    # Show examples\n    print(\"\\nFirst 3 examples:\")\n    for i in range(min(3, num_samples)):\n        print(f\"\\n{i+1}. {texts[i][:70]}...\")\n        print(f\"   Label: {labels[i]}\")\n    \n    return metadata\n\n\nclass PreTokenizedDataset:\n    \"\"\"\n    Efficient dataset class for pre-tokenized data.\n    Zero I/O overhead during iteration.\n    \"\"\"\n    \n    def __init__(self, data_dir: str = \"/kaggle/working/tokenized_data\"):\n        \"\"\"Load pre-tokenized dataset from disk.\"\"\"\n        data_path = Path(data_dir)\n        \n        # Load all data into memory once\n        self.input_ids = torch.load(data_path / 'input_ids.pt')\n        self.attention_mask = torch.load(data_path / 'attention_mask.pt')\n        self.labels = torch.load(data_path / 'labels.pt')\n        \n        with open(data_path / 'metadata.json', 'r') as f:\n            self.metadata = json.load(f)\n        \n        self.num_samples = len(self.labels)\n    \n    def __len__(self):\n        return self.num_samples\n    \n    def __getitem__(self, idx):\n        \"\"\"Get a single example.\"\"\"\n        return {\n            'input_ids': self.input_ids[idx],\n            'attention_mask': self.attention_mask[idx],\n            'labels': self.labels[idx]\n        }\n    \n    def get_batch(self, batch_size: int = 8):\n        \"\"\"Iterate over batches with zero I/O overhead.\"\"\"\n        for i in range(0, self.num_samples, batch_size):\n            end_idx = min(i + batch_size, self.num_samples)\n            yield {\n                'input_ids': self.input_ids[i:end_idx],\n                'attention_mask': self.attention_mask[i:end_idx],\n                'labels': self.labels[i:end_idx]\n            }\n    \n    def to_device(self, device):\n        \"\"\"Move all tensors to device (GPU) at once.\"\"\"\n        self.input_ids = self.input_ids.to(device)\n        self.attention_mask = self.attention_mask.to(device)\n        self.labels = self.labels.to(device)\n        return self\n\n\nprint(\"✓ Dataset preparation functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T02:47:52.453349Z","iopub.execute_input":"2025-12-01T02:47:52.453563Z","iopub.status.idle":"2025-12-01T02:47:56.105863Z","shell.execute_reply.started":"2025-12-01T02:47:52.453527Z","shell.execute_reply":"2025-12-01T02:47:56.105049Z"}},"outputs":[{"name":"stdout","text":"✓ Dataset preparation functions defined\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Step 4: Create Pre-tokenized Dataset\n\nCreate 50 pre-tokenized examples from SST-2 dataset.","metadata":{}},{"cell_type":"code","source":"# Create the tokenized dataset\nmetadata = prepare_tokenized_dataset(\n    num_samples=50,\n    max_length=128,\n    dataset_name='sst2',\n    output_dir='/kaggle/working/tokenized_data',\n    seed=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T02:47:56.107586Z","iopub.execute_input":"2025-12-01T02:47:56.107936Z","iopub.status.idle":"2025-12-01T02:48:07.375748Z","shell.execute_reply.started":"2025-12-01T02:47:56.107917Z","shell.execute_reply":"2025-12-01T02:48:07.375040Z"}},"outputs":[{"name":"stdout","text":"============================================================\nPre-tokenizing Dataset for Energy Measurement\n============================================================\n\n[1/5] Loading DistilBERT tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"349ce2e126104033b79cebd9ed66c681"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f2eb0d05be54bd98bf8069bf4adc4ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f21dbe7c14704317b04de9c712254eaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e7cec4b75f64a7cb33cf11a6fa8c25d"}},"metadata":{}},{"name":"stdout","text":"[2/5] Loading sst2 validation set...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fa27875e23e4a67aa0b35fc0e264daf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sst2/train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2f519bcdf4249d3b3b3d104241df3cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sst2/validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b05951adb09d4460bb0f6dcc237404d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sst2/test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c3afae2cb494f3d9f84ce91086432d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d44c841a8b4436ca72cfe9726e917fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdc49d403f8e4701a3817f1a8796a2ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c29d512d6b648bbad764079a68948b1"}},"metadata":{}},{"name":"stdout","text":"[3/5] Selecting 50 examples (seed=42)...\n[4/5] Tokenizing with max_length=128...\n[5/5] Saving to /kaggle/working/tokenized_data...\n\n============================================================\nDataset Preparation Complete!\n============================================================\nNumber of samples:     50\nMax sequence length:   128\nDataset:               sst2\nNumber of labels:      2\n\nSaved files:\n  - input_ids.pt       torch.Size([50, 128])\n  - attention_mask.pt  torch.Size([50, 128])\n  - labels.pt          torch.Size([50])\n  - metadata.json\n============================================================\n\nFirst 3 examples:\n\n1. it gets onto the screen just about as much of the novella as one could...\n   Label: 1\n\n2. my big fat greek wedding uses stereotypes in a delightful blend of swe...\n   Label: 1\n\n3. for the most part , director anne-sophie birot 's first feature is a s...\n   Label: 1\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Step 5: Verify the Dataset","metadata":{}},{"cell_type":"code","source":"# Load and verify the dataset\ndataset = PreTokenizedDataset('/kaggle/working/tokenized_data')\n\nprint(\"=\"*60)\nprint(\"Dataset Verification\")\nprint(\"=\"*60)\nprint(f\"Number of samples: {len(dataset)}\")\nprint(f\"Metadata: {dataset.metadata}\")\n\n# Check first example\nexample = dataset[0]\nprint(f\"\\nFirst example:\")\nprint(f\"  input_ids shape:      {example['input_ids'].shape}\")\nprint(f\"  attention_mask shape: {example['attention_mask'].shape}\")\nprint(f\"  label:                {example['labels'].item()}\")\n\n# Test batch iteration\nprint(f\"\\nBatch iteration test (batch_size=8):\")\nbatch_count = 0\nfor batch in dataset.get_batch(8):\n    batch_count += 1\n    print(f\"  Batch {batch_count}: {batch['input_ids'].shape}, labels: {batch['labels'].tolist()}\")\n\nprint(f\"\\n✓ Dataset verified! Total batches: {batch_count}\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T02:48:07.376434Z","iopub.execute_input":"2025-12-01T02:48:07.376972Z","iopub.status.idle":"2025-12-01T02:48:07.390135Z","shell.execute_reply.started":"2025-12-01T02:48:07.376943Z","shell.execute_reply":"2025-12-01T02:48:07.389401Z"}},"outputs":[{"name":"stdout","text":"============================================================\nDataset Verification\n============================================================\nNumber of samples: 50\nMetadata: {'num_samples': 50, 'max_length': 128, 'dataset_name': 'sst2', 'num_labels': 2, 'seed': 42, 'tokenizer': 'distilbert-base-uncased'}\n\nFirst example:\n  input_ids shape:      torch.Size([128])\n  attention_mask shape: torch.Size([128])\n  label:                1\n\nBatch iteration test (batch_size=8):\n  Batch 1: torch.Size([8, 128]), labels: [1, 1, 1, 1, 1, 0, 1, 0]\n  Batch 2: torch.Size([8, 128]), labels: [1, 0, 0, 1, 1, 1, 1, 0]\n  Batch 3: torch.Size([8, 128]), labels: [1, 0, 1, 0, 0, 1, 0, 0]\n  Batch 4: torch.Size([8, 128]), labels: [0, 0, 0, 1, 0, 1, 1, 1]\n  Batch 5: torch.Size([8, 128]), labels: [1, 0, 1, 0, 1, 1, 0, 1]\n  Batch 6: torch.Size([8, 128]), labels: [1, 0, 0, 1, 1, 0, 1, 1]\n  Batch 7: torch.Size([2, 128]), labels: [0, 1]\n\n✓ Dataset verified! Total batches: 7\n============================================================\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Step 6: Load DistilBERT Model","metadata":{}},{"cell_type":"code","source":"from transformers import DistilBertForSequenceClassification\nimport torch\n\n# Setup device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load FINE-TUNED model for SST-2 (this is the fix!)\nprint(\"\\nLoading fine-tuned DistilBERT model for SST-2...\")\nmodel = DistilBertForSequenceClassification.from_pretrained(\n    'distilbert-base-uncased-finetuned-sst-2-english',  # ← Changed from 'distilbert-base-uncased'\n    num_labels=2  # Binary classification for SST-2\n)\n\nmodel = model.to(device)\nmodel.eval()\n\nprint(\"✓ Fine-tuned model loaded and moved to\", device)\n\n# Show model size\nparam_count = sum(p.numel() for p in model.parameters())\nprint(f\"\\nModel parameters: {param_count:,} ({param_count/1e6:.1f}M)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T02:57:03.800288Z","iopub.execute_input":"2025-12-01T02:57:03.800989Z","iopub.status.idle":"2025-12-01T02:57:05.692942Z","shell.execute_reply.started":"2025-12-01T02:57:03.800953Z","shell.execute_reply":"2025-12-01T02:57:05.692089Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\nLoading fine-tuned DistilBERT model for SST-2...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"854667051b7d4d589c9f4f325515dc68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5edc62890d1f4cc99199ce717f5dbaee"}},"metadata":{}},{"name":"stdout","text":"✓ Fine-tuned model loaded and moved to cuda\n\nModel parameters: 66,955,010 (67.0M)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Step 7: Move Dataset to GPU (ONE TIME)\n\n**KEY FEATURE:** Move all data to GPU once. During measurement, there will be ZERO CPU→GPU transfers!","metadata":{}},{"cell_type":"code","source":"# Reload dataset and move to GPU\ndataset = PreTokenizedDataset('/kaggle/working/tokenized_data')\n\nif torch.cuda.is_available():\n    print(\"Moving dataset to GPU...\")\n    dataset.to_device(device)\n    print(\"✓ Dataset on GPU\")\n    \n    # Verify\n    batch = next(dataset.get_batch(8))\n    print(f\"\\nVerification:\")\n    print(f\"  Batch input_ids device: {batch['input_ids'].device}\")\n    print(f\"  Batch labels device:    {batch['labels'].device}\")\n    \n    # Check GPU memory\n    allocated = torch.cuda.memory_allocated(0) / 1e6\n    print(f\"\\nGPU memory allocated: {allocated:.2f} MB\")\nelse:\n    print(\"CPU mode - dataset stays in CPU memory\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T02:57:09.062434Z","iopub.execute_input":"2025-12-01T02:57:09.063264Z","iopub.status.idle":"2025-12-01T02:57:09.074197Z","shell.execute_reply.started":"2025-12-01T02:57:09.063237Z","shell.execute_reply":"2025-12-01T02:57:09.073385Z"}},"outputs":[{"name":"stdout","text":"Moving dataset to GPU...\n✓ Dataset on GPU\n\nVerification:\n  Batch input_ids device: cuda:0\n  Batch labels device:    cuda:0\n\nGPU memory allocated: 280.07 MB\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Step 8: Run Baseline Inference (FP32)\n\nTest the complete pipeline with **ZERO I/O** during inference.","metadata":{}},{"cell_type":"code","source":"import time\n\nprint(\"=\"*60)\nprint(\"FP32 Baseline Inference (ZERO I/O!)\")\nprint(\"=\"*60)\n\n# Warmup\nprint(\"\\nWarming up (2 batches)...\")\nwith torch.no_grad():\n    for i, batch in enumerate(dataset.get_batch(8)):\n        if i >= 2:\n            break\n        _ = model(\n            input_ids=batch['input_ids'],\n            attention_mask=batch['attention_mask']\n        )\n\nif torch.cuda.is_available():\n    torch.cuda.synchronize()\n\nprint(\"✓ Warmup complete\")\n\n# Actual inference measurement\nprint(\"\\nRunning inference...\")\ncorrect = 0\ntotal = 0\nstart_time = time.perf_counter()\n\nwith torch.no_grad():\n    for batch in dataset.get_batch(batch_size=8):\n        # Forward pass (NO I/O!)\n        outputs = model(\n            input_ids=batch['input_ids'],\n            attention_mask=batch['attention_mask']\n        )\n        \n        # Compute accuracy\n        preds = outputs.logits.argmax(dim=-1)\n        correct += (preds == batch['labels']).sum().item()\n        total += len(batch['labels'])\n\nif torch.cuda.is_available():\n    torch.cuda.synchronize()\n\nend_time = time.perf_counter()\n\n# Results\nlatency = end_time - start_time\naccuracy = 100.0 * correct / total\nthroughput = total / latency\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Results\")\nprint(\"=\"*60)\nprint(f\"Accuracy:    {accuracy:.2f}% ({correct}/{total})\")\nprint(f\"Latency:     {latency:.3f} seconds\")\nprint(f\"Throughput:  {throughput:.2f} samples/second\")\nprint(f\"Per-sample:  {latency/total*1000:.2f} ms\")\nprint(\"=\"*60)\n\n# Sanity check\nif accuracy > 75:\n    print(\"\\n✓ PASS: Accuracy looks good!\")\nelse:\n    print(\"\\n⚠️ WARNING: Accuracy is low. Expected ~85-90% for pretrained DistilBERT on SST-2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T02:57:11.982580Z","iopub.execute_input":"2025-12-01T02:57:11.983367Z","iopub.status.idle":"2025-12-01T02:57:12.137412Z","shell.execute_reply.started":"2025-12-01T02:57:11.983334Z","shell.execute_reply":"2025-12-01T02:57:12.136526Z"}},"outputs":[{"name":"stdout","text":"============================================================\nFP32 Baseline Inference (ZERO I/O!)\n============================================================\n\nWarming up (2 batches)...\n✓ Warmup complete\n\nRunning inference...\n\n============================================================\nResults\n============================================================\nAccuracy:    86.00% (43/50)\nLatency:     0.108 seconds\nThroughput:  461.90 samples/second\nPer-sample:  2.16 ms\n============================================================\n\n✓ PASS: Accuracy looks good!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Step 9: Measure GPU Power (nvidia-smi)\n\nNow let's measure GPU power consumption during inference.","metadata":{}},{"cell_type":"code","source":"# Check if nvidia-smi is available\nimport subprocess\n\ntry:\n    result = subprocess.run(['nvidia-smi', '--query-gpu=name,power.draw', \n                           '--format=csv,noheader'], \n                          capture_output=True, text=True, timeout=2)\n    \n    if result.returncode == 0:\n        print(\"=\"*60)\n        print(\"GPU Power Monitoring Available\")\n        print(\"=\"*60)\n        print(result.stdout.strip())\n        print(\"\\n✓ nvidia-smi is available for power monitoring\")\n    else:\n        print(\"⚠️ nvidia-smi not responding properly\")\nexcept Exception as e:\n    print(f\"⚠️ nvidia-smi not available: {e}\")\n    print(\"This is expected in some Kaggle environments.\")\n    print(\"You may need to use your local machine or a cloud GPU with nvidia-smi access.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T02:57:14.637754Z","iopub.execute_input":"2025-12-01T02:57:14.638047Z","iopub.status.idle":"2025-12-01T02:57:14.673165Z","shell.execute_reply.started":"2025-12-01T02:57:14.638024Z","shell.execute_reply":"2025-12-01T02:57:14.672560Z"}},"outputs":[{"name":"stdout","text":"============================================================\nGPU Power Monitoring Available\n============================================================\nTesla P100-PCIE-16GB, 37.27 W\n\n✓ nvidia-smi is available for power monitoring\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## Step 10: Integrate with Energy Measurement Harness\n\nHere's how to integrate with Krishna's energy measurement pipeline:","metadata":{}},{"cell_type":"code","source":"# Save the energy_utils.py content (from your uploaded file)\n# This is Krishna's measurement harness\n\n# For demonstration, here's the integration pattern:\n\nprint(\"=\"*60)\nprint(\"Energy Measurement Integration Pattern\")\nprint(\"=\"*60)\n\nintegration_code = '''\n# Krishna's harness integration:\n\nfrom energy_utils import GPUPowerMonitor, benchmark_model_energy\nfrom prepare_dataset import PreTokenizedDataset\nimport torch\n\n# Setup\ndevice = torch.device('cuda')\nmodel = load_your_model().to(device).eval()\n\n# Load dataset BEFORE measurement (ONE TIME)\ndataset = PreTokenizedDataset('/kaggle/working/tokenized_data')\ndataset.to_device(device)  # All data on GPU\n\n# Warmup\nfor i, batch in enumerate(dataset.get_batch(8)):\n    if i >= 2:\n        break\n    _ = model(batch['input_ids'], batch['attention_mask'])\n\ntorch.cuda.synchronize()\n\n# Measure energy\nmonitor = GPUPowerMonitor(gpu_id=0, sample_interval_ms=200)\nmonitor.start()\n\n# Inference (NO I/O!)\nwith torch.no_grad():\n    for batch in dataset.get_batch(8):\n        outputs = model(batch['input_ids'], batch['attention_mask'])\n\ntorch.cuda.synchronize()\nenergy_stats = monitor.stop()\n\nprint(f\"Energy: {energy_stats['energy_joules']:.3f} J\")\nprint(f\"Power:  {energy_stats['mean_power_watts']:.2f} W\")\n'''\n\nprint(integration_code)\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T02:57:17.449346Z","iopub.execute_input":"2025-12-01T02:57:17.450141Z","iopub.status.idle":"2025-12-01T02:57:17.455959Z","shell.execute_reply.started":"2025-12-01T02:57:17.450105Z","shell.execute_reply":"2025-12-01T02:57:17.455130Z"}},"outputs":[{"name":"stdout","text":"============================================================\nEnergy Measurement Integration Pattern\n============================================================\n\n# Krishna's harness integration:\n\nfrom energy_utils import GPUPowerMonitor, benchmark_model_energy\nfrom prepare_dataset import PreTokenizedDataset\nimport torch\n\n# Setup\ndevice = torch.device('cuda')\nmodel = load_your_model().to(device).eval()\n\n# Load dataset BEFORE measurement (ONE TIME)\ndataset = PreTokenizedDataset('/kaggle/working/tokenized_data')\ndataset.to_device(device)  # All data on GPU\n\n# Warmup\nfor i, batch in enumerate(dataset.get_batch(8)):\n    if i >= 2:\n        break\n    _ = model(batch['input_ids'], batch['attention_mask'])\n\ntorch.cuda.synchronize()\n\n# Measure energy\nmonitor = GPUPowerMonitor(gpu_id=0, sample_interval_ms=200)\nmonitor.start()\n\n# Inference (NO I/O!)\nwith torch.no_grad():\n    for batch in dataset.get_batch(8):\n        outputs = model(batch['input_ids'], batch['attention_mask'])\n\ntorch.cuda.synchronize()\nenergy_stats = monitor.stop()\n\nprint(f\"Energy: {energy_stats['energy_joules']:.3f} J\")\nprint(f\"Power:  {energy_stats['mean_power_watts']:.2f} W\")\n\n============================================================\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## Step 11: Create Different Dataset Sizes (for Ablations)","metadata":{}},{"cell_type":"code","source":"# Create datasets for ablation studies\nprint(\"Creating multiple dataset configurations...\\n\")\n\nconfigs = [\n    {'num_samples': 30, 'name': 'small'},\n    {'num_samples': 50, 'name': 'standard'},\n    {'num_samples': 100, 'name': 'large'},\n]\n\nfor config in configs:\n    output_dir = f\"/kaggle/working/tokenized_data_{config['name']}\"\n    print(f\"Creating {config['name']} dataset ({config['num_samples']} samples)...\")\n    \n    prepare_tokenized_dataset(\n        num_samples=config['num_samples'],\n        max_length=128,\n        dataset_name='sst2',\n        output_dir=output_dir,\n        seed=42\n    )\n    print()\n\nprint(\"✓ All dataset configurations created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T02:57:21.922189Z","iopub.execute_input":"2025-12-01T02:57:21.922476Z","iopub.status.idle":"2025-12-01T02:57:26.042825Z","shell.execute_reply.started":"2025-12-01T02:57:21.922454Z","shell.execute_reply":"2025-12-01T02:57:26.042030Z"}},"outputs":[{"name":"stdout","text":"Creating multiple dataset configurations...\n\nCreating small dataset (30 samples)...\n============================================================\nPre-tokenizing Dataset for Energy Measurement\n============================================================\n\n[1/5] Loading DistilBERT tokenizer...\n[2/5] Loading sst2 validation set...\n[3/5] Selecting 30 examples (seed=42)...\n[4/5] Tokenizing with max_length=128...\n[5/5] Saving to /kaggle/working/tokenized_data_small...\n\n============================================================\nDataset Preparation Complete!\n============================================================\nNumber of samples:     30\nMax sequence length:   128\nDataset:               sst2\nNumber of labels:      2\n\nSaved files:\n  - input_ids.pt       torch.Size([30, 128])\n  - attention_mask.pt  torch.Size([30, 128])\n  - labels.pt          torch.Size([30])\n  - metadata.json\n============================================================\n\nFirst 3 examples:\n\n1. it gets onto the screen just about as much of the novella as one could...\n   Label: 1\n\n2. my big fat greek wedding uses stereotypes in a delightful blend of swe...\n   Label: 1\n\n3. for the most part , director anne-sophie birot 's first feature is a s...\n   Label: 1\n\nCreating standard dataset (50 samples)...\n============================================================\nPre-tokenizing Dataset for Energy Measurement\n============================================================\n\n[1/5] Loading DistilBERT tokenizer...\n[2/5] Loading sst2 validation set...\n[3/5] Selecting 50 examples (seed=42)...\n[4/5] Tokenizing with max_length=128...\n[5/5] Saving to /kaggle/working/tokenized_data_standard...\n\n============================================================\nDataset Preparation Complete!\n============================================================\nNumber of samples:     50\nMax sequence length:   128\nDataset:               sst2\nNumber of labels:      2\n\nSaved files:\n  - input_ids.pt       torch.Size([50, 128])\n  - attention_mask.pt  torch.Size([50, 128])\n  - labels.pt          torch.Size([50])\n  - metadata.json\n============================================================\n\nFirst 3 examples:\n\n1. it gets onto the screen just about as much of the novella as one could...\n   Label: 1\n\n2. my big fat greek wedding uses stereotypes in a delightful blend of swe...\n   Label: 1\n\n3. for the most part , director anne-sophie birot 's first feature is a s...\n   Label: 1\n\nCreating large dataset (100 samples)...\n============================================================\nPre-tokenizing Dataset for Energy Measurement\n============================================================\n\n[1/5] Loading DistilBERT tokenizer...\n[2/5] Loading sst2 validation set...\n[3/5] Selecting 100 examples (seed=42)...\n[4/5] Tokenizing with max_length=128...\n[5/5] Saving to /kaggle/working/tokenized_data_large...\n\n============================================================\nDataset Preparation Complete!\n============================================================\nNumber of samples:     100\nMax sequence length:   128\nDataset:               sst2\nNumber of labels:      2\n\nSaved files:\n  - input_ids.pt       torch.Size([100, 128])\n  - attention_mask.pt  torch.Size([100, 128])\n  - labels.pt          torch.Size([100])\n  - metadata.json\n============================================================\n\nFirst 3 examples:\n\n1. it gets onto the screen just about as much of the novella as one could...\n   Label: 1\n\n2. my big fat greek wedding uses stereotypes in a delightful blend of swe...\n   Label: 1\n\n3. for the most part , director anne-sophie birot 's first feature is a s...\n   Label: 1\n\n✓ All dataset configurations created!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Step 12: Summary and Next Steps","metadata":{}},{"cell_type":"code","source":"print(\"=\"*70)\nprint(\" \"*20 + \"DAY 1 CHECKPOINT COMPLETE ✓\")\nprint(\"=\"*70)\n\nprint(\"\\n Files Created:\")\n!ls -lh /kaggle/working/tokenized_data/\n\nprint(\"\\n Accomplished:\")\nprint(\"  ✓ Created pre-tokenized dataset (50 samples)\")\nprint(\"  ✓ Verified zero I/O during iteration\")\nprint(\"  ✓ Tested with DistilBERT FP32 model\")\nprint(f\"  ✓ Baseline accuracy: {accuracy:.2f}%\")\nprint(\"  ✓ Dataset on GPU (zero transfer cost during inference)\")\nprint(\"  ✓ Ready for energy measurement\")\n\nprint(\"\\n Key Metrics (FP32 Baseline):\")\nprint(f\"  • Accuracy:    {accuracy:.2f}%\")\nprint(f\"  • Latency:     {latency:.3f} s\")\nprint(f\"  • Throughput:  {throughput:.2f} samples/s\")\nprint(f\"  • Device:      {device}\")\n\nprint(\"\\n Ready for Day 2:\")\nprint(\"  1. Taara: working FP32 baseline with known accuracy\")\nprint(\"  2. Krishna: Dataset integrates with your energy harness\")\nprint(\"  3. Thomas: Ready to add FP16 and INT8 quantization\")\n\nprint(\"\\n Critical Achievement:\")\nprint(\"  ZERO I/O during inference measurement!\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"Next: Integrate with Krishna's energy measurement harness\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T03:04:43.187877Z","iopub.execute_input":"2025-12-01T03:04:43.188592Z","iopub.status.idle":"2025-12-01T03:04:43.353612Z","shell.execute_reply.started":"2025-12-01T03:04:43.188561Z","shell.execute_reply":"2025-12-01T03:04:43.352954Z"}},"outputs":[{"name":"stdout","text":"======================================================================\n                    DAY 1 CHECKPOINT COMPLETE ✓\n======================================================================\n\n Files Created:\ntotal 112K\n-rw-r--r-- 1 root root  52K Dec  1 02:48 attention_mask.pt\n-rw-r--r-- 1 root root  52K Dec  1 02:48 input_ids.pt\n-rw-r--r-- 1 root root 1.6K Dec  1 02:48 labels.pt\n-rw-r--r-- 1 root root  145 Dec  1 02:48 metadata.json\n\n Accomplished:\n  ✓ Created pre-tokenized dataset (50 samples)\n  ✓ Verified zero I/O during iteration\n  ✓ Tested with DistilBERT FP32 model\n  ✓ Baseline accuracy: 86.00%\n  ✓ Dataset on GPU (zero transfer cost during inference)\n  ✓ Ready for energy measurement\n\n Key Metrics (FP32 Baseline):\n  • Accuracy:    86.00%\n  • Latency:     0.108 s\n  • Throughput:  461.90 samples/s\n  • Device:      cuda\n\n Ready for Day 2:\n  1. Taara: working FP32 baseline with known accuracy\n  2. Krishna: Dataset integrates with your energy harness\n  3. Thomas: Ready to add FP16 and INT8 quantization\n\n Critical Achievement:\n  ZERO I/O during inference measurement!\n\n======================================================================\nNext: Integrate with Krishna's energy measurement harness\n======================================================================\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## Notes for Team\n\n### Taara:\n- Dataset is ready and tested\n- FP32 baseline accuracy confirmed (~{accuracy:.1f}%)\n- Can proceed to Day 2 tasks\n\n### For Krishna:\n- Zero I/O confirmed during iteration\n- All data can be pre-loaded to GPU\n- Ready to integrate with `energy_utils.py`\n- Check if nvidia-smi works in your environment\n\n### For Thomas:\n- Same dataset works for all precision levels\n-  num_labels=2 for SST-2\n- Ready for FP16, INT8, mixed precision experiments\n\n### Important Files:\n- `/kaggle/working/tokenized_data/` - Main dataset (50 samples)\n- `/kaggle/working/tokenized_data_small/` - Small dataset (30 samples)\n- `/kaggle/working/tokenized_data_large/` - Large dataset (100 samples)\n\n### To Download:\nYou can download the tokenized data using Kaggle's output feature or save to your Kaggle dataset.","metadata":{}},{"cell_type":"code","source":"!zip -r tokenized_datasets.zip /kaggle/working/tokenized_data*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T02:59:31.614957Z","iopub.execute_input":"2025-12-01T02:59:31.615686Z","iopub.status.idle":"2025-12-01T02:59:31.785047Z","shell.execute_reply.started":"2025-12-01T02:59:31.615659Z","shell.execute_reply":"2025-12-01T02:59:31.783787Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/tokenized_data/ (stored 0%)\n  adding: kaggle/working/tokenized_data/metadata.json (deflated 23%)\n  adding: kaggle/working/tokenized_data/input_ids.pt (deflated 92%)\n  adding: kaggle/working/tokenized_data/labels.pt (deflated 67%)\n  adding: kaggle/working/tokenized_data/attention_mask.pt (deflated 98%)\n  adding: kaggle/working/tokenized_data_large/ (stored 0%)\n  adding: kaggle/working/tokenized_data_large/metadata.json (deflated 23%)\n  adding: kaggle/working/tokenized_data_large/input_ids.pt (deflated 93%)\n  adding: kaggle/working/tokenized_data_large/labels.pt (deflated 73%)\n  adding: kaggle/working/tokenized_data_large/attention_mask.pt (deflated 99%)\n  adding: kaggle/working/tokenized_data_small/ (stored 0%)\n  adding: kaggle/working/tokenized_data_small/metadata.json (deflated 23%)\n  adding: kaggle/working/tokenized_data_small/input_ids.pt (deflated 92%)\n  adding: kaggle/working/tokenized_data_small/labels.pt (deflated 63%)\n  adding: kaggle/working/tokenized_data_small/attention_mask.pt (deflated 98%)\n  adding: kaggle/working/tokenized_data_standard/ (stored 0%)\n  adding: kaggle/working/tokenized_data_standard/metadata.json (deflated 23%)\n  adding: kaggle/working/tokenized_data_standard/input_ids.pt (deflated 92%)\n  adding: kaggle/working/tokenized_data_standard/labels.pt (deflated 67%)\n  adding: kaggle/working/tokenized_data_standard/attention_mask.pt (deflated 98%)\n","output_type":"stream"}],"execution_count":22}]}
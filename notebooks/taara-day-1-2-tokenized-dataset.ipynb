{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1: Pre-tokenized Dataset for Energy Measurement\n",
    "## Optimized for Kaggle GPU P100\n",
    "\n",
    "This notebook creates a pre-tokenized dataset with **ZERO I/O overhead** during energy measurements.\n",
    "\n",
    "**⚠️ IMPORTANT: Enable GPU in Kaggle**\n",
    "- Go to Settings (right panel) → Accelerator → GPU P100\n",
    "- Click \"Save\" and wait for session to restart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify GPU Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-01T02:46:25.125452Z",
     "iopub.status.busy": "2025-12-01T02:46:25.124782Z",
     "iopub.status.idle": "2025-12-01T02:46:25.131727Z",
     "shell.execute_reply": "2025-12-01T02:46:25.131010Z",
     "shell.execute_reply.started": "2025-12-01T02:46:25.125426Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GPU CHECK\n",
      "============================================================\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "Device name: Tesla P100-PCIE-16GB\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Total GPU memory: 17.06 GB\n",
      "\n",
      "✓ GPU is ready!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"=\"*60)\n",
    "print(\"GPU CHECK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    \n",
    "    # Check memory\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"Total GPU memory: {total_memory:.2f} GB\")\n",
    "    print(\"\\n✓ GPU is ready!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ WARNING: GPU not available!\")\n",
    "    print(\"Please enable GPU in Kaggle settings (Accelerator → GPU P100)\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies\n",
    "\n",
    "Kaggle has most packages pre-installed, but we'll ensure we have the latest versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T02:46:25.133298Z",
     "iopub.status.busy": "2025-12-01T02:46:25.132847Z",
     "iopub.status.idle": "2025-12-01T02:47:52.452343Z",
     "shell.execute_reply": "2025-12-01T02:47:52.451544Z",
     "shell.execute_reply.started": "2025-12-01T02:46:25.133281Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "pylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m✓ Dependencies installed/verified\n"
     ]
    }
   ],
   "source": [
    "# Install/upgrade required packages\n",
    "!pip install -q transformers datasets accelerate\n",
    "\n",
    "print(\"✓ Dependencies installed/verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Dataset Preparation Functions\n",
    "\n",
    "These functions will create our pre-tokenized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T02:47:52.453563Z",
     "iopub.status.busy": "2025-12-01T02:47:52.453349Z",
     "iopub.status.idle": "2025-12-01T02:47:56.105863Z",
     "shell.execute_reply": "2025-12-01T02:47:56.105049Z",
     "shell.execute_reply.started": "2025-12-01T02:47:52.453527Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset preparation functions defined\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dataset Preparation Module\n",
    "Pre-tokenize dataset to eliminate I/O overhead during energy measurement\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "\n",
    "def prepare_tokenized_dataset(\n",
    "    num_samples: int = None,\n",
    "    max_length: int = 128,\n",
    "    dataset_name: str = \"sst2\",\n",
    "    output_dir: str = \"/kaggle/working/tokenized_data\",\n",
    "    seed: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Pre-tokenize dataset and save to disk.\n",
    "    \n",
    "    Args:\n",
    "        num_samples: Number of examples to tokenize. If None, uses entire dataset.\n",
    "        max_length: Maximum sequence length (128 is good for DistilBERT)\n",
    "        dataset_name: Which GLUE task to use (\"sst2\", \"mnli\")\n",
    "        output_dir: Directory to save tokenized data\n",
    "        seed: Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"Pre-tokenizing Dataset for Energy Measurement\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create output directory\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(\"\\n[1/5] Loading DistilBERT tokenizer...\")\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    \n",
    "    # Load dataset\n",
    "    print(f\"[2/5] Loading {dataset_name} validation set...\")\n",
    "    if dataset_name == \"sst2\":\n",
    "        dataset = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
    "        text_key = \"sentence\"\n",
    "        label_key = \"label\"\n",
    "        num_labels = 2\n",
    "    elif dataset_name == \"mnli\":\n",
    "        dataset = load_dataset(\"glue\", \"mnli\", split=\"validation_matched\")\n",
    "        text_key = \"premise\"\n",
    "        label_key = \"label\"\n",
    "        num_labels = 3\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {dataset_name} not supported\")\n",
    "    \n",
    "    # Get full dataset size\n",
    "    full_dataset_size = len(dataset)\n",
    "    print(f\"  Full dataset size: {full_dataset_size} samples\")\n",
    "    \n",
    "    # Sample examples (if num_samples is specified and less than full size)\n",
    "    if num_samples is None:\n",
    "        print(f\"[3/5] Using entire dataset ({full_dataset_size} samples)...\")\n",
    "        dataset = dataset.shuffle(seed=seed)\n",
    "        actual_num_samples = full_dataset_size\n",
    "    elif num_samples >= full_dataset_size:\n",
    "        print(f\"[3/5] Requested {num_samples} samples, but dataset only has {full_dataset_size}. Using all {full_dataset_size} samples...\")\n",
    "        dataset = dataset.shuffle(seed=seed)\n",
    "        actual_num_samples = full_dataset_size\n",
    "    else:\n",
    "        print(f\"[3/5] Selecting {num_samples} examples from {full_dataset_size} (seed={seed})...\")\n",
    "        dataset = dataset.shuffle(seed=seed).select(range(num_samples))\n",
    "        actual_num_samples = num_samples\n",
    "    \n",
    "    # Tokenize all examples\n",
    "    print(f\"[4/5] Tokenizing with max_length={max_length}...\")\n",
    "    texts = [example[text_key] for example in dataset]\n",
    "    labels = [example[label_key] for example in dataset]\n",
    "    \n",
    "    # Tokenize in batch\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    # Save tensors\n",
    "    print(f\"[5/5] Saving to {output_dir}...\")\n",
    "    torch.save(encodings['input_ids'], output_path / 'input_ids.pt')\n",
    "    torch.save(encodings['attention_mask'], output_path / 'attention_mask.pt')\n",
    "    torch.save(labels_tensor, output_path / 'labels.pt')\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'num_samples': actual_num_samples,\n",
    "        'max_length': max_length,\n",
    "        'dataset_name': dataset_name,\n",
    "        'num_labels': num_labels,\n",
    "        'seed': seed,\n",
    "        'tokenizer': 'distilbert-base-uncased',\n",
    "    }\n",
    "    \n",
    "    with open(output_path / 'metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Dataset Preparation Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Number of samples:     {actual_num_samples}\")\n",
    "    print(f\"Max sequence length:   {max_length}\")\n",
    "    print(f\"Dataset:               {dataset_name}\")\n",
    "    print(f\"Number of labels:      {num_labels}\")\n",
    "    print(f\"\\nSaved files:\")\n",
    "    print(f\"  - input_ids.pt       {encodings['input_ids'].shape}\")\n",
    "    print(f\"  - attention_mask.pt  {encodings['attention_mask'].shape}\")\n",
    "    print(f\"  - labels.pt          {labels_tensor.shape}\")\n",
    "    print(f\"  - metadata.json\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Show examples\n",
    "    print(\"\\nFirst 3 examples:\")\n",
    "    for i in range(min(3, actual_num_samples)):\n",
    "        print(f\"\\n{i+1}. {texts[i][:70]}...\")\n",
    "        print(f\"   Label: {labels[i]}\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "class PreTokenizedDataset:\n",
    "    \"\"\"\n",
    "    Efficient dataset class for pre-tokenized data.\n",
    "    Zero I/O overhead during iteration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: str = \"/kaggle/working/tokenized_data\"):\n",
    "        \"\"\"Load pre-tokenized dataset from disk.\"\"\"\n",
    "        data_path = Path(data_dir)\n",
    "        \n",
    "        # Load all data into memory once\n",
    "        self.input_ids = torch.load(data_path / 'input_ids.pt')\n",
    "        self.attention_mask = torch.load(data_path / 'attention_mask.pt')\n",
    "        self.labels = torch.load(data_path / 'labels.pt')\n",
    "        \n",
    "        with open(data_path / 'metadata.json', 'r') as f:\n",
    "            self.metadata = json.load(f)\n",
    "        \n",
    "        self.num_samples = len(self.labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a single example.\"\"\"\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "    \n",
    "    def get_batch(self, batch_size: int = 8):\n",
    "        \"\"\"Iterate over batches with zero I/O overhead.\"\"\"\n",
    "        for i in range(0, self.num_samples, batch_size):\n",
    "            end_idx = min(i + batch_size, self.num_samples)\n",
    "            yield {\n",
    "                'input_ids': self.input_ids[i:end_idx],\n",
    "                'attention_mask': self.attention_mask[i:end_idx],\n",
    "                'labels': self.labels[i:end_idx]\n",
    "            }\n",
    "    \n",
    "    def to_device(self, device):\n",
    "        \"\"\"Move all tensors to device (GPU) at once.\"\"\"\n",
    "        self.input_ids = self.input_ids.to(device)\n",
    "        self.attention_mask = self.attention_mask.to(device)\n",
    "        self.labels = self.labels.to(device)\n",
    "        return self\n",
    "\n",
    "\n",
    "print(\"✓ Dataset preparation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Pre-tokenized Dataset\n",
    "\n",
    "Create 50 pre-tokenized examples from SST-2 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T02:47:56.107936Z",
     "iopub.status.busy": "2025-12-01T02:47:56.107586Z",
     "iopub.status.idle": "2025-12-01T02:48:07.375748Z",
     "shell.execute_reply": "2025-12-01T02:48:07.375040Z",
     "shell.execute_reply.started": "2025-12-01T02:47:56.107917Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Pre-tokenizing Dataset for Energy Measurement\n",
      "============================================================\n",
      "\n",
      "[1/5] Loading DistilBERT tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349ce2e126104033b79cebd9ed66c681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2eb0d05be54bd98bf8069bf4adc4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21dbe7c14704317b04de9c712254eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7cec4b75f64a7cb33cf11a6fa8c25d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/5] Loading sst2 validation set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa27875e23e4a67aa0b35fc0e264daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f519bcdf4249d3b3b3d104241df3cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sst2/train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05951adb09d4460bb0f6dcc237404d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sst2/validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3afae2cb494f3d9f84ce91086432d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sst2/test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d44c841a8b4436ca72cfe9726e917fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc49d403f8e4701a3817f1a8796a2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c29d512d6b648bbad764079a68948b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/5] Selecting 50 examples (seed=42)...\n",
      "[4/5] Tokenizing with max_length=128...\n",
      "[5/5] Saving to /kaggle/working/tokenized_data...\n",
      "\n",
      "============================================================\n",
      "Dataset Preparation Complete!\n",
      "============================================================\n",
      "Number of samples:     50\n",
      "Max sequence length:   128\n",
      "Dataset:               sst2\n",
      "Number of labels:      2\n",
      "\n",
      "Saved files:\n",
      "  - input_ids.pt       torch.Size([50, 128])\n",
      "  - attention_mask.pt  torch.Size([50, 128])\n",
      "  - labels.pt          torch.Size([50])\n",
      "  - metadata.json\n",
      "============================================================\n",
      "\n",
      "First 3 examples:\n",
      "\n",
      "1. it gets onto the screen just about as much of the novella as one could...\n",
      "   Label: 1\n",
      "\n",
      "2. my big fat greek wedding uses stereotypes in a delightful blend of swe...\n",
      "   Label: 1\n",
      "\n",
      "3. for the most part , director anne-sophie birot 's first feature is a s...\n",
      "   Label: 1\n"
     ]
    }
   ],
   "source": [
    "# Create the tokenized dataset\n",
    "metadata = prepare_tokenized_dataset(\n",
    "    num_samples=50,\n",
    "    max_length=128,\n",
    "    dataset_name='sst2',\n",
    "    output_dir='/kaggle/working/tokenized_data',\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Verify the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T02:48:07.376972Z",
     "iopub.status.busy": "2025-12-01T02:48:07.376434Z",
     "iopub.status.idle": "2025-12-01T02:48:07.390135Z",
     "shell.execute_reply": "2025-12-01T02:48:07.389401Z",
     "shell.execute_reply.started": "2025-12-01T02:48:07.376943Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Dataset Verification\n",
      "============================================================\n",
      "Number of samples: 50\n",
      "Metadata: {'num_samples': 50, 'max_length': 128, 'dataset_name': 'sst2', 'num_labels': 2, 'seed': 42, 'tokenizer': 'distilbert-base-uncased'}\n",
      "\n",
      "First example:\n",
      "  input_ids shape:      torch.Size([128])\n",
      "  attention_mask shape: torch.Size([128])\n",
      "  label:                1\n",
      "\n",
      "Batch iteration test (batch_size=8):\n",
      "  Batch 1: torch.Size([8, 128]), labels: [1, 1, 1, 1, 1, 0, 1, 0]\n",
      "  Batch 2: torch.Size([8, 128]), labels: [1, 0, 0, 1, 1, 1, 1, 0]\n",
      "  Batch 3: torch.Size([8, 128]), labels: [1, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Batch 4: torch.Size([8, 128]), labels: [0, 0, 0, 1, 0, 1, 1, 1]\n",
      "  Batch 5: torch.Size([8, 128]), labels: [1, 0, 1, 0, 1, 1, 0, 1]\n",
      "  Batch 6: torch.Size([8, 128]), labels: [1, 0, 0, 1, 1, 0, 1, 1]\n",
      "  Batch 7: torch.Size([2, 128]), labels: [0, 1]\n",
      "\n",
      "✓ Dataset verified! Total batches: 7\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Load and verify the dataset\n",
    "dataset = PreTokenizedDataset('/kaggle/working/tokenized_data')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Dataset Verification\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of samples: {len(dataset)}\")\n",
    "print(f\"Metadata: {dataset.metadata}\")\n",
    "\n",
    "# Check first example\n",
    "example = dataset[0]\n",
    "print(f\"\\nFirst example:\")\n",
    "print(f\"  input_ids shape:      {example['input_ids'].shape}\")\n",
    "print(f\"  attention_mask shape: {example['attention_mask'].shape}\")\n",
    "print(f\"  label:                {example['labels'].item()}\")\n",
    "\n",
    "# Test batch iteration\n",
    "print(f\"\\nBatch iteration test (batch_size=8):\")\n",
    "batch_count = 0\n",
    "for batch in dataset.get_batch(8):\n",
    "    batch_count += 1\n",
    "    print(f\"  Batch {batch_count}: {batch['input_ids'].shape}, labels: {batch['labels'].tolist()}\")\n",
    "\n",
    "print(f\"\\n✓ Dataset verified! Total batches: {batch_count}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Load DistilBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T02:57:03.800989Z",
     "iopub.status.busy": "2025-12-01T02:57:03.800288Z",
     "iopub.status.idle": "2025-12-01T02:57:05.692942Z",
     "shell.execute_reply": "2025-12-01T02:57:05.692089Z",
     "shell.execute_reply.started": "2025-12-01T02:57:03.800953Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Loading fine-tuned DistilBERT model for SST-2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854667051b7d4d589c9f4f325515dc68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5edc62890d1f4cc99199ce717f5dbaee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Fine-tuned model loaded and moved to cuda\n",
      "\n",
      "Model parameters: 66,955,010 (67.0M)\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load FINE-TUNED model for SST-2 (this is the fix!)\n",
    "print(\"\\nLoading fine-tuned DistilBERT model for SST-2...\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased-finetuned-sst-2-english',  # ← Changed from 'distilbert-base-uncased'\n",
    "    num_labels=2  # Binary classification for SST-2\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"✓ Fine-tuned model loaded and moved to\", device)\n",
    "\n",
    "# Show model size\n",
    "param_count = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nModel parameters: {param_count:,} ({param_count/1e6:.1f}M)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Move Dataset to GPU (ONE TIME)\n",
    "\n",
    "**KEY FEATURE:** Move all data to GPU once. During measurement, there will be ZERO CPU→GPU transfers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T02:57:09.063264Z",
     "iopub.status.busy": "2025-12-01T02:57:09.062434Z",
     "iopub.status.idle": "2025-12-01T02:57:09.074197Z",
     "shell.execute_reply": "2025-12-01T02:57:09.073385Z",
     "shell.execute_reply.started": "2025-12-01T02:57:09.063237Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving dataset to GPU...\n",
      "✓ Dataset on GPU\n",
      "\n",
      "Verification:\n",
      "  Batch input_ids device: cuda:0\n",
      "  Batch labels device:    cuda:0\n",
      "\n",
      "GPU memory allocated: 280.07 MB\n"
     ]
    }
   ],
   "source": [
    "# Reload dataset and move to GPU\n",
    "dataset = PreTokenizedDataset('/kaggle/working/tokenized_data')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Moving dataset to GPU...\")\n",
    "    dataset.to_device(device)\n",
    "    print(\"✓ Dataset on GPU\")\n",
    "    \n",
    "    # Verify\n",
    "    batch = next(dataset.get_batch(8))\n",
    "    print(f\"\\nVerification:\")\n",
    "    print(f\"  Batch input_ids device: {batch['input_ids'].device}\")\n",
    "    print(f\"  Batch labels device:    {batch['labels'].device}\")\n",
    "    \n",
    "    # Check GPU memory\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e6\n",
    "    print(f\"\\nGPU memory allocated: {allocated:.2f} MB\")\n",
    "else:\n",
    "    print(\"CPU mode - dataset stays in CPU memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Run Baseline Inference (FP32)\n",
    "\n",
    "Test the complete pipeline with **ZERO I/O** during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T02:57:11.983367Z",
     "iopub.status.busy": "2025-12-01T02:57:11.982580Z",
     "iopub.status.idle": "2025-12-01T02:57:12.137412Z",
     "shell.execute_reply": "2025-12-01T02:57:12.136526Z",
     "shell.execute_reply.started": "2025-12-01T02:57:11.983334Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FP32 Baseline Inference (ZERO I/O!)\n",
      "============================================================\n",
      "\n",
      "Warming up (2 batches)...\n",
      "✓ Warmup complete\n",
      "\n",
      "Running inference...\n",
      "\n",
      "============================================================\n",
      "Results\n",
      "============================================================\n",
      "Accuracy:    86.00% (43/50)\n",
      "Latency:     0.108 seconds\n",
      "Throughput:  461.90 samples/second\n",
      "Per-sample:  2.16 ms\n",
      "============================================================\n",
      "\n",
      "✓ PASS: Accuracy looks good!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FP32 Baseline Inference (ZERO I/O!)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Warmup\n",
    "print(\"\\nWarming up (2 batches)...\")\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(dataset.get_batch(8)):\n",
    "        if i >= 2:\n",
    "            break\n",
    "        _ = model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask']\n",
    "        )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "print(\"✓ Warmup complete\")\n",
    "\n",
    "# Actual inference measurement\n",
    "print(\"\\nRunning inference...\")\n",
    "correct = 0\n",
    "total = 0\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataset.get_batch(batch_size=8):\n",
    "        # Forward pass (NO I/O!)\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask']\n",
    "        )\n",
    "        \n",
    "        # Compute accuracy\n",
    "        preds = outputs.logits.argmax(dim=-1)\n",
    "        correct += (preds == batch['labels']).sum().item()\n",
    "        total += len(batch['labels'])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "# Results\n",
    "latency = end_time - start_time\n",
    "accuracy = 100.0 * correct / total\n",
    "throughput = total / latency\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Results\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:    {accuracy:.2f}% ({correct}/{total})\")\n",
    "print(f\"Latency:     {latency:.3f} seconds\")\n",
    "print(f\"Throughput:  {throughput:.2f} samples/second\")\n",
    "print(f\"Per-sample:  {latency/total*1000:.2f} ms\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sanity check\n",
    "if accuracy > 75:\n",
    "    print(\"\\n✓ PASS: Accuracy looks good!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ WARNING: Accuracy is low. Expected ~85-90% for pretrained DistilBERT on SST-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Measure GPU Power (nvidia-smi)\n",
    "\n",
    "Now let's measure GPU power consumption during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T02:57:14.638047Z",
     "iopub.status.busy": "2025-12-01T02:57:14.637754Z",
     "iopub.status.idle": "2025-12-01T02:57:14.673165Z",
     "shell.execute_reply": "2025-12-01T02:57:14.672560Z",
     "shell.execute_reply.started": "2025-12-01T02:57:14.638024Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GPU Power Monitoring Available\n",
      "============================================================\n",
      "Tesla P100-PCIE-16GB, 37.27 W\n",
      "\n",
      "✓ nvidia-smi is available for power monitoring\n"
     ]
    }
   ],
   "source": [
    "# Check if nvidia-smi is available\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=name,power.draw', \n",
    "                           '--format=csv,noheader'], \n",
    "                          capture_output=True, text=True, timeout=2)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"=\"*60)\n",
    "        print(\"GPU Power Monitoring Available\")\n",
    "        print(\"=\"*60)\n",
    "        print(result.stdout.strip())\n",
    "        print(\"\\n✓ nvidia-smi is available for power monitoring\")\n",
    "    else:\n",
    "        print(\"⚠️ nvidia-smi not responding properly\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ nvidia-smi not available: {e}\")\n",
    "    print(\"This is expected in some Kaggle environments.\")\n",
    "    print(\"You may need to use your local machine or a cloud GPU with nvidia-smi access.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Integrate with Energy Measurement Harness\n",
    "\n",
    "Here's how to integrate with Krishna's energy measurement pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T02:57:17.450141Z",
     "iopub.status.busy": "2025-12-01T02:57:17.449346Z",
     "iopub.status.idle": "2025-12-01T02:57:17.455959Z",
     "shell.execute_reply": "2025-12-01T02:57:17.455130Z",
     "shell.execute_reply.started": "2025-12-01T02:57:17.450105Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Energy Measurement Integration Pattern\n",
      "============================================================\n",
      "\n",
      "# Krishna's harness integration:\n",
      "\n",
      "from energy_utils import GPUPowerMonitor, benchmark_model_energy\n",
      "from prepare_dataset import PreTokenizedDataset\n",
      "import torch\n",
      "\n",
      "# Setup\n",
      "device = torch.device('cuda')\n",
      "model = load_your_model().to(device).eval()\n",
      "\n",
      "# Load dataset BEFORE measurement (ONE TIME)\n",
      "dataset = PreTokenizedDataset('/kaggle/working/tokenized_data')\n",
      "dataset.to_device(device)  # All data on GPU\n",
      "\n",
      "# Warmup\n",
      "for i, batch in enumerate(dataset.get_batch(8)):\n",
      "    if i >= 2:\n",
      "        break\n",
      "    _ = model(batch['input_ids'], batch['attention_mask'])\n",
      "\n",
      "torch.cuda.synchronize()\n",
      "\n",
      "# Measure energy\n",
      "monitor = GPUPowerMonitor(gpu_id=0, sample_interval_ms=200)\n",
      "monitor.start()\n",
      "\n",
      "# Inference (NO I/O!)\n",
      "with torch.no_grad():\n",
      "    for batch in dataset.get_batch(8):\n",
      "        outputs = model(batch['input_ids'], batch['attention_mask'])\n",
      "\n",
      "torch.cuda.synchronize()\n",
      "energy_stats = monitor.stop()\n",
      "\n",
      "print(f\"Energy: {energy_stats['energy_joules']:.3f} J\")\n",
      "print(f\"Power:  {energy_stats['mean_power_watts']:.2f} W\")\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Save the energy_utils.py content (from your uploaded file)\n",
    "# This is Krishna's measurement harness\n",
    "\n",
    "# For demonstration, here's the integration pattern:\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Energy Measurement Integration Pattern\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "integration_code = '''\n",
    "# Krishna's harness integration:\n",
    "\n",
    "from energy_utils import GPUPowerMonitor, benchmark_model_energy\n",
    "from prepare_dataset import PreTokenizedDataset\n",
    "import torch\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda')\n",
    "model = load_your_model().to(device).eval()\n",
    "\n",
    "# Load dataset BEFORE measurement (ONE TIME)\n",
    "dataset = PreTokenizedDataset('/kaggle/working/tokenized_data')\n",
    "dataset.to_device(device)  # All data on GPU\n",
    "\n",
    "# Warmup\n",
    "for i, batch in enumerate(dataset.get_batch(8)):\n",
    "    if i >= 2:\n",
    "        break\n",
    "    _ = model(batch['input_ids'], batch['attention_mask'])\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Measure energy\n",
    "monitor = GPUPowerMonitor(gpu_id=0, sample_interval_ms=200)\n",
    "monitor.start()\n",
    "\n",
    "# Inference (NO I/O!)\n",
    "with torch.no_grad():\n",
    "    for batch in dataset.get_batch(8):\n",
    "        outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "energy_stats = monitor.stop()\n",
    "\n",
    "print(f\"Energy: {energy_stats['energy_joules']:.3f} J\")\n",
    "print(f\"Power:  {energy_stats['mean_power_watts']:.2f} W\")\n",
    "'''\n",
    "\n",
    "print(integration_code)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Create Different Dataset Sizes (for Ablations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T02:57:21.922476Z",
     "iopub.status.busy": "2025-12-01T02:57:21.922189Z",
     "iopub.status.idle": "2025-12-01T02:57:26.042825Z",
     "shell.execute_reply": "2025-12-01T02:57:26.042030Z",
     "shell.execute_reply.started": "2025-12-01T02:57:21.922454Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating multiple dataset configurations...\n",
      "\n",
      "Creating small dataset (30 samples)...\n",
      "============================================================\n",
      "Pre-tokenizing Dataset for Energy Measurement\n",
      "============================================================\n",
      "\n",
      "[1/5] Loading DistilBERT tokenizer...\n",
      "[2/5] Loading sst2 validation set...\n",
      "[3/5] Selecting 30 examples (seed=42)...\n",
      "[4/5] Tokenizing with max_length=128...\n",
      "[5/5] Saving to /kaggle/working/tokenized_data_small...\n",
      "\n",
      "============================================================\n",
      "Dataset Preparation Complete!\n",
      "============================================================\n",
      "Number of samples:     30\n",
      "Max sequence length:   128\n",
      "Dataset:               sst2\n",
      "Number of labels:      2\n",
      "\n",
      "Saved files:\n",
      "  - input_ids.pt       torch.Size([30, 128])\n",
      "  - attention_mask.pt  torch.Size([30, 128])\n",
      "  - labels.pt          torch.Size([30])\n",
      "  - metadata.json\n",
      "============================================================\n",
      "\n",
      "First 3 examples:\n",
      "\n",
      "1. it gets onto the screen just about as much of the novella as one could...\n",
      "   Label: 1\n",
      "\n",
      "2. my big fat greek wedding uses stereotypes in a delightful blend of swe...\n",
      "   Label: 1\n",
      "\n",
      "3. for the most part , director anne-sophie birot 's first feature is a s...\n",
      "   Label: 1\n",
      "\n",
      "Creating standard dataset (50 samples)...\n",
      "============================================================\n",
      "Pre-tokenizing Dataset for Energy Measurement\n",
      "============================================================\n",
      "\n",
      "[1/5] Loading DistilBERT tokenizer...\n",
      "[2/5] Loading sst2 validation set...\n",
      "[3/5] Selecting 50 examples (seed=42)...\n",
      "[4/5] Tokenizing with max_length=128...\n",
      "[5/5] Saving to /kaggle/working/tokenized_data_standard...\n",
      "\n",
      "============================================================\n",
      "Dataset Preparation Complete!\n",
      "============================================================\n",
      "Number of samples:     50\n",
      "Max sequence length:   128\n",
      "Dataset:               sst2\n",
      "Number of labels:      2\n",
      "\n",
      "Saved files:\n",
      "  - input_ids.pt       torch.Size([50, 128])\n",
      "  - attention_mask.pt  torch.Size([50, 128])\n",
      "  - labels.pt          torch.Size([50])\n",
      "  - metadata.json\n",
      "============================================================\n",
      "\n",
      "First 3 examples:\n",
      "\n",
      "1. it gets onto the screen just about as much of the novella as one could...\n",
      "   Label: 1\n",
      "\n",
      "2. my big fat greek wedding uses stereotypes in a delightful blend of swe...\n",
      "   Label: 1\n",
      "\n",
      "3. for the most part , director anne-sophie birot 's first feature is a s...\n",
      "   Label: 1\n",
      "\n",
      "Creating large dataset (100 samples)...\n",
      "============================================================\n",
      "Pre-tokenizing Dataset for Energy Measurement\n",
      "============================================================\n",
      "\n",
      "[1/5] Loading DistilBERT tokenizer...\n",
      "[2/5] Loading sst2 validation set...\n",
      "[3/5] Selecting 100 examples (seed=42)...\n",
      "[4/5] Tokenizing with max_length=128...\n",
      "[5/5] Saving to /kaggle/working/tokenized_data_large...\n",
      "\n",
      "============================================================\n",
      "Dataset Preparation Complete!\n",
      "============================================================\n",
      "Number of samples:     100\n",
      "Max sequence length:   128\n",
      "Dataset:               sst2\n",
      "Number of labels:      2\n",
      "\n",
      "Saved files:\n",
      "  - input_ids.pt       torch.Size([100, 128])\n",
      "  - attention_mask.pt  torch.Size([100, 128])\n",
      "  - labels.pt          torch.Size([100])\n",
      "  - metadata.json\n",
      "============================================================\n",
      "\n",
      "First 3 examples:\n",
      "\n",
      "1. it gets onto the screen just about as much of the novella as one could...\n",
      "   Label: 1\n",
      "\n",
      "2. my big fat greek wedding uses stereotypes in a delightful blend of swe...\n",
      "   Label: 1\n",
      "\n",
      "3. for the most part , director anne-sophie birot 's first feature is a s...\n",
      "   Label: 1\n",
      "\n",
      "✓ All dataset configurations created!\n"
     ]
    }
   ],
   "source": [
    "# Create datasets for ablation studies\n",
    "print(\"Creating multiple dataset configurations...\\n\")\n",
    "\n",
    "configs = [\n",
    "    {'num_samples': 30, 'name': 'small'},\n",
    "    {'num_samples': 50, 'name': 'standard'},\n",
    "    {'num_samples': 100, 'name': 'large'},\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    output_dir = f\"/kaggle/working/tokenized_data_{config['name']}\"\n",
    "    print(f\"Creating {config['name']} dataset ({config['num_samples']} samples)...\")\n",
    "    \n",
    "    prepare_tokenized_dataset(\n",
    "        num_samples=config['num_samples'],\n",
    "        max_length=128,\n",
    "        dataset_name='sst2',\n",
    "        output_dir=output_dir,\n",
    "        seed=42\n",
    "    )\n",
    "    print()\n",
    "\n",
    "print(\"✓ All dataset configurations created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T03:04:43.188592Z",
     "iopub.status.busy": "2025-12-01T03:04:43.187877Z",
     "iopub.status.idle": "2025-12-01T03:04:43.353612Z",
     "shell.execute_reply": "2025-12-01T03:04:43.352954Z",
     "shell.execute_reply.started": "2025-12-01T03:04:43.188561Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "                    DAY 1 CHECKPOINT COMPLETE ✓\n",
      "======================================================================\n",
      "\n",
      " Files Created:\n",
      "total 112K\n",
      "-rw-r--r-- 1 root root  52K Dec  1 02:48 attention_mask.pt\n",
      "-rw-r--r-- 1 root root  52K Dec  1 02:48 input_ids.pt\n",
      "-rw-r--r-- 1 root root 1.6K Dec  1 02:48 labels.pt\n",
      "-rw-r--r-- 1 root root  145 Dec  1 02:48 metadata.json\n",
      "\n",
      " Accomplished:\n",
      "  ✓ Created pre-tokenized dataset (50 samples)\n",
      "  ✓ Verified zero I/O during iteration\n",
      "  ✓ Tested with DistilBERT FP32 model\n",
      "  ✓ Baseline accuracy: 86.00%\n",
      "  ✓ Dataset on GPU (zero transfer cost during inference)\n",
      "  ✓ Ready for energy measurement\n",
      "\n",
      " Key Metrics (FP32 Baseline):\n",
      "  • Accuracy:    86.00%\n",
      "  • Latency:     0.108 s\n",
      "  • Throughput:  461.90 samples/s\n",
      "  • Device:      cuda\n",
      "\n",
      " Ready for Day 2:\n",
      "  1. Taara: working FP32 baseline with known accuracy\n",
      "  2. Krishna: Dataset integrates with your energy harness\n",
      "  3. Thomas: Ready to add FP16 and INT8 quantization\n",
      "\n",
      " Critical Achievement:\n",
      "  ZERO I/O during inference measurement!\n",
      "\n",
      "======================================================================\n",
      "Next: Integrate with Krishna's energy measurement harness\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\" \"*20 + \"DAY 1 CHECKPOINT COMPLETE ✓\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n Files Created:\")\n",
    "!ls -lh /kaggle/working/tokenized_data/\n",
    "\n",
    "print(\"\\n Accomplished:\")\n",
    "print(\"  ✓ Created pre-tokenized dataset (50 samples)\")\n",
    "print(\"  ✓ Verified zero I/O during iteration\")\n",
    "print(\"  ✓ Tested with DistilBERT FP32 model\")\n",
    "print(f\"  ✓ Baseline accuracy: {accuracy:.2f}%\")\n",
    "print(\"  ✓ Dataset on GPU (zero transfer cost during inference)\")\n",
    "print(\"  ✓ Ready for energy measurement\")\n",
    "\n",
    "print(\"\\n Key Metrics (FP32 Baseline):\")\n",
    "print(f\"  • Accuracy:    {accuracy:.2f}%\")\n",
    "print(f\"  • Latency:     {latency:.3f} s\")\n",
    "print(f\"  • Throughput:  {throughput:.2f} samples/s\")\n",
    "print(f\"  • Device:      {device}\")\n",
    "\n",
    "print(\"\\n Ready for Day 2:\")\n",
    "print(\"  1. Taara: working FP32 baseline with known accuracy\")\n",
    "print(\"  2. Krishna: Dataset integrates with your energy harness\")\n",
    "print(\"  3. Thomas: Ready to add FP16 and INT8 quantization\")\n",
    "\n",
    "print(\"\\n Critical Achievement:\")\n",
    "print(\"  ZERO I/O during inference measurement!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Next: Integrate with Krishna's energy measurement harness\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for Team\n",
    "\n",
    "### Taara:\n",
    "- Dataset is ready and tested\n",
    "- FP32 baseline accuracy confirmed (~{accuracy:.1f}%)\n",
    "- Can proceed to Day 2 tasks\n",
    "\n",
    "### For Krishna:\n",
    "- Zero I/O confirmed during iteration\n",
    "- All data can be pre-loaded to GPU\n",
    "- Ready to integrate with `energy_utils.py`\n",
    "- Check if nvidia-smi works in your environment\n",
    "\n",
    "### For Thomas:\n",
    "- Same dataset works for all precision levels\n",
    "-  num_labels=2 for SST-2\n",
    "- Ready for FP16, INT8, mixed precision experiments\n",
    "\n",
    "### Important Files:\n",
    "- `/kaggle/working/tokenized_data/` - Main dataset (50 samples)\n",
    "- `/kaggle/working/tokenized_data_small/` - Small dataset (30 samples)\n",
    "- `/kaggle/working/tokenized_data_large/` - Large dataset (100 samples)\n",
    "\n",
    "### To Download:\n",
    "You can download the tokenized data using Kaggle's output feature or save to your Kaggle dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T02:59:31.615686Z",
     "iopub.status.busy": "2025-12-01T02:59:31.614957Z",
     "iopub.status.idle": "2025-12-01T02:59:31.785047Z",
     "shell.execute_reply": "2025-12-01T02:59:31.783787Z",
     "shell.execute_reply.started": "2025-12-01T02:59:31.615659Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/tokenized_data/ (stored 0%)\n",
      "  adding: kaggle/working/tokenized_data/metadata.json (deflated 23%)\n",
      "  adding: kaggle/working/tokenized_data/input_ids.pt (deflated 92%)\n",
      "  adding: kaggle/working/tokenized_data/labels.pt (deflated 67%)\n",
      "  adding: kaggle/working/tokenized_data/attention_mask.pt (deflated 98%)\n",
      "  adding: kaggle/working/tokenized_data_large/ (stored 0%)\n",
      "  adding: kaggle/working/tokenized_data_large/metadata.json (deflated 23%)\n",
      "  adding: kaggle/working/tokenized_data_large/input_ids.pt (deflated 93%)\n",
      "  adding: kaggle/working/tokenized_data_large/labels.pt (deflated 73%)\n",
      "  adding: kaggle/working/tokenized_data_large/attention_mask.pt (deflated 99%)\n",
      "  adding: kaggle/working/tokenized_data_small/ (stored 0%)\n",
      "  adding: kaggle/working/tokenized_data_small/metadata.json (deflated 23%)\n",
      "  adding: kaggle/working/tokenized_data_small/input_ids.pt (deflated 92%)\n",
      "  adding: kaggle/working/tokenized_data_small/labels.pt (deflated 63%)\n",
      "  adding: kaggle/working/tokenized_data_small/attention_mask.pt (deflated 98%)\n",
      "  adding: kaggle/working/tokenized_data_standard/ (stored 0%)\n",
      "  adding: kaggle/working/tokenized_data_standard/metadata.json (deflated 23%)\n",
      "  adding: kaggle/working/tokenized_data_standard/input_ids.pt (deflated 92%)\n",
      "  adding: kaggle/working/tokenized_data_standard/labels.pt (deflated 67%)\n",
      "  adding: kaggle/working/tokenized_data_standard/attention_mask.pt (deflated 98%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r tokenized_datasets.zip /kaggle/working/tokenized_data*"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

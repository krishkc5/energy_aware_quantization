{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Quantization Benchmark: GPT-2 Small on WikiText-2\n",
    "\n",
    "**Native-Only Execution**: This notebook only runs quantization formats that are natively supported.\n",
    "\n",
    "**Formats Tested:**\n",
    "- FP32 (Baseline)\n",
    "- FP16 (Half Precision)\n",
    "\n",
    "**Model**: GPT-2 Small (124M parameters)\n",
    "\n",
    "**Task**: Language Modeling (WikiText-2)\n",
    "\n",
    "**Metrics:**\n",
    "- Latency (ms/sample)\n",
    "- Throughput (samples/second)\n",
    "- Perplexity (lower is better)\n",
    "- Energy (mJ/sample)\n",
    "- Power (W)\n",
    "- Model Size (MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:19:46.109362Z",
     "iopub.status.busy": "2025-12-04T04:19:46.108945Z",
     "iopub.status.idle": "2025-12-04T04:19:46.116319Z",
     "shell.execute_reply": "2025-12-04T04:19:46.115647Z",
     "shell.execute_reply.started": "2025-12-04T04:19:46.109327Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import time\n",
    "import threading\n",
    "import subprocess\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Native Format Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:19:46.117748Z",
     "iopub.status.busy": "2025-12-04T04:19:46.117516Z",
     "iopub.status.idle": "2025-12-04T04:19:46.131187Z",
     "shell.execute_reply": "2025-12-04T04:19:46.130430Z",
     "shell.execute_reply.started": "2025-12-04T04:19:46.117731Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check which formats are natively supported\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# FP32 and FP16 are always supported on CUDA\n",
    "FP32_SUPPORTED = True\n",
    "FP16_SUPPORTED = torch.cuda.is_available()\n",
    "\n",
    "# Mixed precision (autocast) is supported on CUDA\n",
    "MIXED_SUPPORTED = torch.cuda.is_available()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"NATIVE FORMAT SUPPORT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"FP32:            {'âœ“ Supported' if FP32_SUPPORTED else 'âœ— Not supported'}\")\n",
    "print(f\"FP16:            {'âœ“ Supported' if FP16_SUPPORTED else 'âœ— Not supported'}\")\n",
    "print(f\"Mixed Precision: {'âœ“ Supported' if MIXED_SUPPORTED else 'âœ— Not supported'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Build list of formats to test\n",
    "FORMATS_TO_TEST = []\n",
    "if FP32_SUPPORTED:\n",
    "    FORMATS_TO_TEST.append('fp32')\n",
    "if FP16_SUPPORTED:\n",
    "    FORMATS_TO_TEST.append('fp16')\n",
    "if MIXED_SUPPORTED:\n",
    "    FORMATS_TO_TEST.append('mixed')\n",
    "\n",
    "print(f\"\\nFormats to benchmark: {FORMATS_TO_TEST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-Tokenized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:19:46.132239Z",
     "iopub.status.busy": "2025-12-04T04:19:46.132025Z",
     "iopub.status.idle": "2025-12-04T04:19:46.148667Z",
     "shell.execute_reply": "2025-12-04T04:19:46.148104Z",
     "shell.execute_reply.started": "2025-12-04T04:19:46.132223Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Search for tokenized dataset\n",
    "possible_paths = [\n",
    "    Path(cwd) / \"..\" / \"datasets\" / \"gpt2_tokenized_data\",\n",
    "    Path(cwd) / \"datasets\" / \"gpt2_tokenized_data\",\n",
    "    Path(cwd) / \"energy_aware_quantization\" / \"datasets\" / \"gpt2_tokenized_data\",\n",
    "    Path(\"/kaggle/working/gpt2_tokenized_data\"),\n",
    "]\n",
    "\n",
    "dataset_path = None\n",
    "for path in possible_paths:\n",
    "    if path.exists() and (path / \"input_ids.pt\").exists():\n",
    "        dataset_path = path\n",
    "        break\n",
    "\n",
    "if dataset_path is None:\n",
    "    current = Path(cwd)\n",
    "    for _ in range(5):\n",
    "        test_path = current / \"datasets\" / \"gpt2_tokenized_data\"\n",
    "        if test_path.exists() and (test_path / \"input_ids.pt\").exists():\n",
    "            dataset_path = test_path\n",
    "            break\n",
    "        current = current.parent\n",
    "\n",
    "if dataset_path is None:\n",
    "    raise FileNotFoundError(\"Could not find GPT-2 tokenized dataset. Please run gpt2_tokenized_dataset.ipynb first.\")\n",
    "\n",
    "print(f\"Loading dataset from: {dataset_path}\")\n",
    "\n",
    "# Load tensors\n",
    "input_ids = torch.load(dataset_path / \"input_ids.pt\", map_location='cpu')\n",
    "attention_mask = torch.load(dataset_path / \"attention_mask.pt\", map_location='cpu')\n",
    "labels = torch.load(dataset_path / \"labels.pt\", map_location='cpu')\n",
    "\n",
    "print(f\"âœ“ Loaded {input_ids.shape[0]} samples\")\n",
    "print(f\"  Input shape: {input_ids.shape}\")\n",
    "print(f\"  Device: {input_ids.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Monitoring Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:19:46.149698Z",
     "iopub.status.busy": "2025-12-04T04:19:46.149462Z",
     "iopub.status.idle": "2025-12-04T04:19:47.209702Z",
     "shell.execute_reply": "2025-12-04T04:19:47.208810Z",
     "shell.execute_reply.started": "2025-12-04T04:19:46.149674Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PowerLogger:\n",
    "    \"\"\"Background thread for GPU power monitoring using nvidia-smi\"\"\"\n",
    "    \n",
    "    def __init__(self, poll_interval_ms=50):\n",
    "        self.poll_interval_ms = poll_interval_ms\n",
    "        self.power_samples = []\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "        \n",
    "    def _monitor_power(self):\n",
    "        \"\"\"Background monitoring loop\"\"\"\n",
    "        while self.running:\n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    ['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits', '--id=0'],\n",
    "                    capture_output=True,\n",
    "                    text=True,\n",
    "                    timeout=1.0\n",
    "                )\n",
    "                if result.returncode == 0:\n",
    "                    # Parse only the first line (GPU 0) and handle multiple values\n",
    "                    output = result.stdout.strip().split('\\n')[0].strip()\n",
    "                    power_w = float(output)\n",
    "                    self.power_samples.append(power_w)\n",
    "            except Exception as e:\n",
    "                # Silently skip failed reads during monitoring\n",
    "                pass\n",
    "            \n",
    "            time.sleep(self.poll_interval_ms / 1000.0)\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"Start power monitoring in background thread\"\"\"\n",
    "        self.power_samples = []\n",
    "        self.running = True\n",
    "        self.thread = threading.Thread(target=self._monitor_power, daemon=True)\n",
    "        self.thread.start()\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop power monitoring and return statistics\"\"\"\n",
    "        self.running = False\n",
    "        if self.thread:\n",
    "            self.thread.join(timeout=2.0)\n",
    "        \n",
    "        if len(self.power_samples) == 0:\n",
    "            return {'mean_power_w': 0, 'std_power_w': 0, 'num_samples': 0}\n",
    "        \n",
    "        return {\n",
    "            'mean_power_w': np.mean(self.power_samples),\n",
    "            'std_power_w': np.std(self.power_samples),\n",
    "            'num_samples': len(self.power_samples)\n",
    "        }\n",
    "\n",
    "# Test power logger\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Testing power logger...\")\n",
    "    logger = PowerLogger(poll_interval_ms=50)\n",
    "    logger.start()\n",
    "    time.sleep(1.0)\n",
    "    stats = logger.stop()\n",
    "    print(f\"âœ“ Power logger working: {stats['mean_power_w']:.2f}W (n={stats['num_samples']} samples)\")\n",
    "else:\n",
    "    print(\"âš ï¸  GPU not available, power monitoring disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:19:47.212624Z",
     "iopub.status.busy": "2025-12-04T04:19:47.211930Z",
     "iopub.status.idle": "2025-12-04T04:19:47.217715Z",
     "shell.execute_reply": "2025-12-04T04:19:47.217000Z",
     "shell.execute_reply.started": "2025-12-04T04:19:47.212600Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2\"\n",
    "\n",
    "def load_model(precision: str, device: str = \"cuda\"):\n",
    "    \"\"\"\n",
    "    Load GPT-2 model with specified precision.\n",
    "    \n",
    "    Args:\n",
    "        precision: 'fp32', 'fp16', or 'mixed'\n",
    "        device: 'cuda' or 'cpu'\n",
    "    \n",
    "    Returns:\n",
    "        model, model_size_mb\n",
    "    \"\"\"\n",
    "    print(f\"Loading {precision.upper()} model...\")\n",
    "    \n",
    "    model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Apply precision\n",
    "    if precision == 'fp16':\n",
    "        model = model.half()\n",
    "    elif precision == 'mixed':\n",
    "        # Mixed precision uses FP32 model + autocast during inference\n",
    "        pass\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Calculate model size\n",
    "    model_size_mb = sum(p.element_size() * p.nelement() for p in model.parameters()) / (1024 ** 2)\n",
    "    \n",
    "    print(f\"  âœ“ Size: {model_size_mb:.2f} MB\")\n",
    "    print(f\"  âœ“ Dtype: {next(model.parameters()).dtype}\")\n",
    "    \n",
    "    return model, model_size_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Function (Lab 3 Methodology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:19:47.218914Z",
     "iopub.status.busy": "2025-12-04T04:19:47.218636Z",
     "iopub.status.idle": "2025-12-04T04:19:47.234581Z",
     "shell.execute_reply": "2025-12-04T04:19:47.233916Z",
     "shell.execute_reply.started": "2025-12-04T04:19:47.218891Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def benchmark_model(\n",
    "    model,\n",
    "    input_ids,\n",
    "    attention_mask,\n",
    "    labels,\n",
    "    name: str,\n",
    "    device: str,\n",
    "    num_iters: int = 100,\n",
    "    warmup_iters: int = 10,\n",
    "    use_autocast: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Benchmark GPT-2 model following Lab 3 methodology.\n",
    "    \n",
    "    4-phase process:\n",
    "    1. Warmup\n",
    "    2. Start power logger\n",
    "    3. Run benchmark (measure latency + perplexity)\n",
    "    4. Compute metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Benchmarking: {name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Move data to device\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    num_samples = input_ids.shape[0]\n",
    "    \n",
    "    # [PHASE 1/4] Warmup\n",
    "    print(f\"[1/4] Warmup ({warmup_iters} iterations)...\")\n",
    "    with torch.no_grad():\n",
    "        for i in range(warmup_iters):\n",
    "            idx = i % num_samples\n",
    "            sample_input = input_ids[idx].unsqueeze(0)\n",
    "            sample_mask = attention_mask[idx].unsqueeze(0)\n",
    "            \n",
    "            if use_autocast:\n",
    "                with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    _ = model(input_ids=sample_input, attention_mask=sample_mask)\n",
    "            else:\n",
    "                _ = model(input_ids=sample_input, attention_mask=sample_mask)\n",
    "    \n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # [PHASE 2/4] Start power logger\n",
    "    print(f\"[2/4] Starting power logger...\")\n",
    "    power_logger = None\n",
    "    if device == \"cuda\":\n",
    "        power_logger = PowerLogger(poll_interval_ms=50)\n",
    "        power_logger.start()\n",
    "        time.sleep(0.5)  # Let logger stabilize\n",
    "    \n",
    "    # [PHASE 3/4] Run benchmark\n",
    "    print(f\"[3/4] Running benchmark ({num_iters} iterations)...\")\n",
    "    latencies = []\n",
    "    total_loss = 0\n",
    "    num_tokens = 0\n",
    "    \n",
    "    benchmark_start = time.perf_counter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_iters):\n",
    "            # Cycle through dataset\n",
    "            idx = i % num_samples\n",
    "            sample_input = input_ids[idx].unsqueeze(0)\n",
    "            sample_mask = attention_mask[idx].unsqueeze(0)\n",
    "            sample_labels = labels[idx].unsqueeze(0)\n",
    "            \n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            iter_start = time.perf_counter()\n",
    "            \n",
    "            if use_autocast:\n",
    "                with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    outputs = model(\n",
    "                        input_ids=sample_input,\n",
    "                        attention_mask=sample_mask,\n",
    "                        labels=sample_labels\n",
    "                    )\n",
    "            else:\n",
    "                outputs = model(\n",
    "                    input_ids=sample_input,\n",
    "                    attention_mask=sample_mask,\n",
    "                    labels=sample_labels\n",
    "                )\n",
    "            \n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            iter_end = time.perf_counter()\n",
    "            latencies.append(iter_end - iter_start)\n",
    "            \n",
    "            # Accumulate loss\n",
    "            total_loss += outputs.loss.item()\n",
    "            num_tokens += sample_mask.sum().item()\n",
    "            \n",
    "            if (i + 1) % 25 == 0:\n",
    "                print(f\"    Progress: {i+1}/{num_iters}\", end='\\r')\n",
    "    \n",
    "    benchmark_end = time.perf_counter()\n",
    "    print()  # New line after progress\n",
    "    \n",
    "    # Stop power logger\n",
    "    power_stats = None\n",
    "    if power_logger:\n",
    "        time.sleep(0.5)  # Let final samples collect\n",
    "        power_stats = power_logger.stop()\n",
    "    \n",
    "    # [PHASE 4/4] Compute metrics\n",
    "    print(f\"[4/4] Computing metrics...\")\n",
    "    \n",
    "    total_time_s = benchmark_end - benchmark_start\n",
    "    mean_latency_ms = np.mean(latencies) * 1000\n",
    "    std_latency_ms = np.std(latencies) * 1000\n",
    "    avg_loss = total_loss / num_iters\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    throughput = num_iters / total_time_s  # samples per second\n",
    "    tokens_per_sec = num_tokens / total_time_s\n",
    "    \n",
    "    # Energy calculations\n",
    "    mean_power_w = 0\n",
    "    energy_per_sample_mj = 0\n",
    "    if power_stats and power_stats['num_samples'] > 0:\n",
    "        mean_power_w = power_stats['mean_power_w']\n",
    "        total_energy_j = mean_power_w * total_time_s\n",
    "        energy_per_sample_mj = (total_energy_j / num_iters) * 1000  # Convert to mJ\n",
    "    \n",
    "    results = {\n",
    "        'name': name,\n",
    "        'mean_latency_ms': mean_latency_ms,\n",
    "        'std_latency_ms': std_latency_ms,\n",
    "        'perplexity': perplexity,\n",
    "        'avg_loss': avg_loss,\n",
    "        'throughput': throughput,\n",
    "        'tokens_per_sec': tokens_per_sec,\n",
    "        'mean_power_w': mean_power_w,\n",
    "        'energy_per_sample_mj': energy_per_sample_mj,\n",
    "        'total_time_s': total_time_s,\n",
    "        'total_iters': num_iters\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Results:\")\n",
    "    print(f\"  Latency:     {mean_latency_ms:.3f} Â± {std_latency_ms:.3f} ms/sample\")\n",
    "    print(f\"  Throughput:  {throughput:.1f} samples/sec\")\n",
    "    print(f\"  Tokens/sec:  {tokens_per_sec:.1f}\")\n",
    "    print(f\"  Perplexity:  {perplexity:.2f}\")\n",
    "    if power_stats:\n",
    "        print(f\"  Power:       {mean_power_w:.2f} W\")\n",
    "        print(f\"  Energy:      {energy_per_sample_mj:.2f} mJ/sample\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run All Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:19:47.235679Z",
     "iopub.status.busy": "2025-12-04T04:19:47.235423Z",
     "iopub.status.idle": "2025-12-04T04:20:02.234637Z",
     "shell.execute_reply": "2025-12-04T04:20:02.233925Z",
     "shell.execute_reply.started": "2025-12-04T04:19:47.235662Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_results = []\n",
    "model_sizes = {}\n",
    "\n",
    "for precision in FORMATS_TO_TEST:\n",
    "    try:\n",
    "        # Load model\n",
    "        model, model_size_mb = load_model(precision, device=device)\n",
    "        model_sizes[precision] = model_size_mb\n",
    "        \n",
    "        # Benchmark\n",
    "        use_autocast = (precision == 'mixed')\n",
    "        results = benchmark_model(\n",
    "            model=model,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            name=precision.upper(),\n",
    "            device=device,\n",
    "            num_iters=100,\n",
    "            warmup_iters=10,\n",
    "            use_autocast=use_autocast\n",
    "        )\n",
    "        \n",
    "        results['model_size_mb'] = model_size_mb\n",
    "        all_results.append(results)\n",
    "        \n",
    "        # Clean up\n",
    "        del model\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Small delay between benchmarks\n",
    "        time.sleep(2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error benchmarking {precision}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ALL BENCHMARKS COMPLETE\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:20:02.235720Z",
     "iopub.status.busy": "2025-12-04T04:20:02.235438Z",
     "iopub.status.idle": "2025-12-04T04:20:02.254462Z",
     "shell.execute_reply": "2025-12-04T04:20:02.253643Z",
     "shell.execute_reply.started": "2025-12-04T04:20:02.235700Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create results dataframe\n",
    "df_results = pd.DataFrame(all_results)\n",
    "\n",
    "# Calculate speedup vs FP32\n",
    "if 'fp32' in [r['name'].lower() for r in all_results]:\n",
    "    fp32_latency = df_results[df_results['name'].str.upper() == 'FP32']['mean_latency_ms'].values[0]\n",
    "    df_results['speedup_vs_fp32'] = fp32_latency / df_results['mean_latency_ms']\n",
    "    \n",
    "    fp32_energy = df_results[df_results['name'].str.upper() == 'FP32']['energy_per_sample_mj'].values[0]\n",
    "    if fp32_energy > 0:\n",
    "        df_results['energy_reduction_vs_fp32'] = fp32_energy / df_results['energy_per_sample_mj']\n",
    "    else:\n",
    "        df_results['energy_reduction_vs_fp32'] = 1.0\n",
    "    \n",
    "    fp32_size = df_results[df_results['name'].str.upper() == 'FP32']['model_size_mb'].values[0]\n",
    "    df_results['size_reduction_vs_fp32'] = fp32_size / df_results['model_size_mb']\n",
    "else:\n",
    "    df_results['speedup_vs_fp32'] = 1.0\n",
    "    df_results['energy_reduction_vs_fp32'] = 1.0\n",
    "    df_results['size_reduction_vs_fp32'] = 1.0\n",
    "\n",
    "# Reorder columns\n",
    "column_order = [\n",
    "    'name',\n",
    "    'mean_latency_ms',\n",
    "    'std_latency_ms',\n",
    "    'speedup_vs_fp32',\n",
    "    'throughput',\n",
    "    'tokens_per_sec',\n",
    "    'perplexity',\n",
    "    'mean_power_w',\n",
    "    'energy_per_sample_mj',\n",
    "    'energy_reduction_vs_fp32',\n",
    "    'model_size_mb',\n",
    "    'size_reduction_vs_fp32'\n",
    "]\n",
    "df_results = df_results[column_order]\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"GPT-2 QUANTIZATION BENCHMARK RESULTS\")\n",
    "print(\"=\"*100)\n",
    "print(df_results.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:20:02.255642Z",
     "iopub.status.busy": "2025-12-04T04:20:02.255385Z",
     "iopub.status.idle": "2025-12-04T04:20:02.267751Z",
     "shell.execute_reply": "2025-12-04T04:20:02.267017Z",
     "shell.execute_reply.started": "2025-12-04T04:20:02.255624Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STATISTICAL ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n1. Latency Statistics (ms/sample):\")\n",
    "for _, row in df_results.iterrows():\n",
    "    print(f\"  {row['name']:12s}: {row['mean_latency_ms']:7.3f} Â± {row['std_latency_ms']:6.3f}\")\n",
    "\n",
    "print(\"\\n2. Speedup vs FP32:\")\n",
    "for _, row in df_results.iterrows():\n",
    "    speedup = row['speedup_vs_fp32']\n",
    "    print(f\"  {row['name']:12s}: {speedup:.2f}x faster\")\n",
    "\n",
    "print(\"\\n3. Energy Efficiency vs FP32:\")\n",
    "for _, row in df_results.iterrows():\n",
    "    if row['energy_per_sample_mj'] > 0:\n",
    "        reduction = row['energy_reduction_vs_fp32']\n",
    "        print(f\"  {row['name']:12s}: {reduction:.2f}x more efficient ({row['energy_per_sample_mj']:.2f} mJ/sample)\")\n",
    "\n",
    "print(\"\\n4. Model Size Reduction:\")\n",
    "for _, row in df_results.iterrows():\n",
    "    reduction = row['size_reduction_vs_fp32']\n",
    "    print(f\"  {row['name']:12s}: {reduction:.2f}x smaller ({row['model_size_mb']:.2f} MB)\")\n",
    "\n",
    "print(\"\\n5. Perplexity Comparison (lower is better):\")\n",
    "for _, row in df_results.iterrows():\n",
    "    print(f\"  {row['name']:12s}: {row['perplexity']:.2f}\")\n",
    "\n",
    "# Best performer in each category\n",
    "print(\"\\n6. Best Performers:\")\n",
    "if len(df_results) > 1:\n",
    "    fastest = df_results.loc[df_results['mean_latency_ms'].idxmin()]\n",
    "    print(f\"  Fastest:          {fastest['name']} ({fastest['mean_latency_ms']:.3f} ms)\")\n",
    "    \n",
    "    most_efficient = df_results.loc[df_results['energy_per_sample_mj'].idxmin()]\n",
    "    if most_efficient['energy_per_sample_mj'] > 0:\n",
    "        print(f\"  Most Efficient:   {most_efficient['name']} ({most_efficient['energy_per_sample_mj']:.2f} mJ/sample)\")\n",
    "    \n",
    "    smallest = df_results.loc[df_results['model_size_mb'].idxmin()]\n",
    "    print(f\"  Smallest:         {smallest['name']} ({smallest['model_size_mb']:.2f} MB)\")\n",
    "    \n",
    "    best_perplexity = df_results.loc[df_results['perplexity'].idxmin()]\n",
    "    print(f\"  Best Perplexity:  {best_perplexity['name']} ({best_perplexity['perplexity']:.2f})\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:20:02.269444Z",
     "iopub.status.busy": "2025-12-04T04:20:02.268577Z",
     "iopub.status.idle": "2025-12-04T04:20:03.268538Z",
     "shell.execute_reply": "2025-12-04T04:20:03.267684Z",
     "shell.execute_reply.started": "2025-12-04T04:20:02.269419Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('GPT-2 Quantization Benchmark Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Color palette\n",
    "colors = sns.color_palette('husl', n_colors=len(df_results))\n",
    "\n",
    "# 1. Latency comparison\n",
    "ax = axes[0, 0]\n",
    "bars = ax.bar(df_results['name'], df_results['mean_latency_ms'], color=colors, alpha=0.8)\n",
    "ax.errorbar(df_results['name'], df_results['mean_latency_ms'], \n",
    "            yerr=df_results['std_latency_ms'], fmt='none', color='black', capsize=5)\n",
    "ax.set_title('Latency (Lower is Better)', fontweight='bold')\n",
    "ax.set_ylabel('Latency (ms/sample)')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Speedup vs FP32\n",
    "ax = axes[0, 1]\n",
    "bars = ax.bar(df_results['name'], df_results['speedup_vs_fp32'], color=colors, alpha=0.8)\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', linewidth=2, label='FP32 Baseline')\n",
    "ax.set_title('Speedup vs FP32 (Higher is Better)', fontweight='bold')\n",
    "ax.set_ylabel('Speedup (x)')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Energy per sample\n",
    "ax = axes[0, 2]\n",
    "if df_results['energy_per_sample_mj'].max() > 0:\n",
    "    bars = ax.bar(df_results['name'], df_results['energy_per_sample_mj'], color=colors, alpha=0.8)\n",
    "    ax.set_title('Energy per Sample (Lower is Better)', fontweight='bold')\n",
    "    ax.set_ylabel('Energy (mJ/sample)')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'Energy data not available', \n",
    "            ha='center', va='center', transform=ax.transAxes)\n",
    "    ax.set_title('Energy per Sample', fontweight='bold')\n",
    "\n",
    "# 4. Throughput\n",
    "ax = axes[1, 0]\n",
    "bars = ax.bar(df_results['name'], df_results['throughput'], color=colors, alpha=0.8)\n",
    "ax.set_title('Throughput (Higher is Better)', fontweight='bold')\n",
    "ax.set_ylabel('Throughput (samples/sec)')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 5. Perplexity\n",
    "ax = axes[1, 1]\n",
    "bars = ax.bar(df_results['name'], df_results['perplexity'], color=colors, alpha=0.8)\n",
    "ax.set_title('Perplexity (Lower is Better)', fontweight='bold')\n",
    "ax.set_ylabel('Perplexity')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 6. Model size\n",
    "ax = axes[1, 2]\n",
    "bars = ax.bar(df_results['name'], df_results['model_size_mb'], color=colors, alpha=0.8)\n",
    "ax.set_title('Model Size (Lower is Better)', fontweight='bold')\n",
    "ax.set_ylabel('Size (MB)')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pareto Frontier: Speed vs Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:20:03.269788Z",
     "iopub.status.busy": "2025-12-04T04:20:03.269456Z",
     "iopub.status.idle": "2025-12-04T04:20:03.622822Z",
     "shell.execute_reply": "2025-12-04T04:20:03.622163Z",
     "shell.execute_reply.started": "2025-12-04T04:20:03.269771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Pareto frontier plot: Latency vs Perplexity\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot each format\n",
    "for i, row in df_results.iterrows():\n",
    "    ax.scatter(row['mean_latency_ms'], row['perplexity'], \n",
    "              s=200, c=[colors[i]], alpha=0.7, edgecolors='black', linewidth=2)\n",
    "    ax.annotate(row['name'], \n",
    "               (row['mean_latency_ms'], row['perplexity']),\n",
    "               xytext=(10, 5), textcoords='offset points',\n",
    "               fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Latency (ms/sample) - Lower is Better', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Perplexity - Lower is Better', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Speed vs Quality Trade-off (GPT-2)', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add ideal region (bottom-left corner)\n",
    "ax.axvline(x=df_results['mean_latency_ms'].min(), color='green', \n",
    "          linestyle='--', alpha=0.3, label='Best Latency')\n",
    "ax.axhline(y=df_results['perplexity'].min(), color='green', \n",
    "          linestyle='--', alpha=0.3, label='Best Perplexity')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Pareto frontier visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:20:03.624002Z",
     "iopub.status.busy": "2025-12-04T04:20:03.623683Z",
     "iopub.status.idle": "2025-12-04T04:20:03.864583Z",
     "shell.execute_reply": "2025-12-04T04:20:03.863916Z",
     "shell.execute_reply.started": "2025-12-04T04:20:03.623983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if df_results['mean_power_w'].max() > 0:\n",
    "    # Calculate efficiency ratio: throughput per watt\n",
    "    df_results['efficiency_ratio'] = df_results['throughput'] / df_results['mean_power_w']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    colors = sns.color_palette('summer', n_colors=len(df_results))[::-1]\n",
    "    \n",
    "    x_pos = np.arange(len(df_results))\n",
    "    bars = ax.bar(x_pos, df_results['efficiency_ratio'], color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    ax.set_xlabel('Quantization Format', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Samples per Second per Watt', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Energy Efficiency Ratio: Throughput per Watt\\n(Higher is Better)', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(df_results['name'], fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (idx, row) in enumerate(df_results.iterrows()):\n",
    "        ax.text(i, row['efficiency_ratio'] + max(df_results['efficiency_ratio'])*0.02, \n",
    "                f\"{row['efficiency_ratio']:.2f}\\nsamples/s/W\", \n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸  Power data not available - skipping Efficiency Ratio plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 15: Efficiency Ratio (Throughput per Watt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:20:03.865456Z",
     "iopub.status.busy": "2025-12-04T04:20:03.865242Z",
     "iopub.status.idle": "2025-12-04T04:20:03.870635Z",
     "shell.execute_reply": "2025-12-04T04:20:03.869718Z",
     "shell.execute_reply.started": "2025-12-04T04:20:03.865439Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Normalize metrics to 0-100 scale (higher is better)\n",
    "def normalize_metric(values, inverse=False):\n",
    "    \"\"\"Normalize to 0-100 scale. If inverse=True, smaller values are better.\"\"\"\n",
    "    min_val, max_val = values.min(), values.max()\n",
    "    if max_val == min_val:\n",
    "        return np.ones_like(values) * 50\n",
    "    \n",
    "    normalized = (values - min_val) / (max_val - min_val) * 100\n",
    "    if inverse:\n",
    "        normalized = 100 - normalized\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 14: Overall Performance Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:20:03.874216Z",
     "iopub.status.busy": "2025-12-04T04:20:03.873676Z",
     "iopub.status.idle": "2025-12-04T04:20:04.214482Z",
     "shell.execute_reply": "2025-12-04T04:20:04.213822Z",
     "shell.execute_reply.started": "2025-12-04T04:20:03.874195Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create normalized heatmap data\n",
    "heatmap_data = pd.DataFrame({\n",
    "    'Latency\\n(lower=better)': normalize_metric(df_results['mean_latency_ms'].values, inverse=True),\n",
    "    'Throughput\\n(higher=better)': normalize_metric(df_results['throughput'].values, inverse=False),\n",
    "    'Energy\\n(lower=better)': normalize_metric(df_results['energy_per_sample_mj'].values, inverse=True) if df_results['energy_per_sample_mj'].max() > 0 else np.ones(len(df_results)) * 50,\n",
    "    'Perplexity\\n(lower=better)': normalize_metric(df_results['perplexity'].values, inverse=True),\n",
    "    'Model Size\\n(lower=better)': normalize_metric(df_results['model_size_mb'].values, inverse=True),\n",
    "})\n",
    "heatmap_data.index = df_results['name'].values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.heatmap(heatmap_data.T, annot=True, fmt='.1f', cmap='RdYlGn', \n",
    "            cbar_kws={'label': 'Normalized Score (0-100, higher is better)'}, \n",
    "            linewidths=2, linecolor='white', ax=ax, \n",
    "            vmin=0, vmax=100, center=50)\n",
    "\n",
    "ax.set_title('Normalized Performance Heatmap\\n(All metrics normalized: 100 = Best, 0 = Worst)', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Quantization Format', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Performance Metric', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:20:04.215405Z",
     "iopub.status.busy": "2025-12-04T04:20:04.215220Z",
     "iopub.status.idle": "2025-12-04T04:20:04.460623Z",
     "shell.execute_reply": "2025-12-04T04:20:04.459600Z",
     "shell.execute_reply.started": "2025-12-04T04:20:04.215392Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calculate overall score (average of all normalized metrics)\n",
    "overall_scores = heatmap_data.mean(axis=1).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors_ranked = sns.color_palette('Spectral', n_colors=len(overall_scores))[::-1]\n",
    "\n",
    "bars = ax.barh(range(len(overall_scores)), overall_scores.values, color=colors_ranked, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax.set_yticks(range(len(overall_scores)))\n",
    "ax.set_yticklabels(overall_scores.index, fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Overall Performance Score (0-100)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Overall Performance Ranking\\n(Average of All Normalized Metrics)', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "ax.set_xlim(0, 100)\n",
    "\n",
    "# Add value labels\n",
    "for i, (name, score) in enumerate(overall_scores.items()):\n",
    "    ax.text(score + 2, i, f'{score:.1f}', \n",
    "            va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Add rank medals\n",
    "medals = ['ðŸ¥‡', 'ðŸ¥ˆ', 'ðŸ¥‰']\n",
    "for i in range(min(3, len(overall_scores))):\n",
    "    ax.text(-5, i, medals[i], fontsize=20, va='center', ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OVERALL PERFORMANCE RANKING\")\n",
    "print(\"=\"*70)\n",
    "for i, (name, score) in enumerate(overall_scores.items(), 1):\n",
    "    medal = medals[i-1] if i <= 3 else f\"{i}.\"\n",
    "    print(f\"{medal:4s} {name:15s}: {score:.1f}/100\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 13: Normalized Performance Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:20:04.462477Z",
     "iopub.status.busy": "2025-12-04T04:20:04.462207Z",
     "iopub.status.idle": "2025-12-04T04:20:04.912738Z",
     "shell.execute_reply": "2025-12-04T04:20:04.912042Z",
     "shell.execute_reply.started": "2025-12-04T04:20:04.462459Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "colors = sns.color_palette('Set1', n_colors=len(df_results))\n",
    "\n",
    "# Scatter plot\n",
    "for i, row in df_results.iterrows():\n",
    "    ax.scatter(row['mean_latency_ms'], row['perplexity'], \n",
    "              s=400, c=[colors[i]], alpha=0.7, edgecolors='black', linewidth=2.5, zorder=3)\n",
    "    \n",
    "    # Add labels\n",
    "    ax.annotate(row['name'], \n",
    "               (row['mean_latency_ms'], row['perplexity']),\n",
    "               xytext=(15, 10), textcoords='offset points',\n",
    "               fontsize=12, fontweight='bold',\n",
    "               bbox=dict(boxstyle='round,pad=0.5', facecolor=colors[i], alpha=0.3),\n",
    "               arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0', lw=1.5))\n",
    "\n",
    "ax.set_xlabel('Latency (ms/sample) - Lower is Better â†’', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Perplexity - Lower is Better â†’', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Speed vs Quality Trade-off\\n(Ideal: Bottom-Left Corner)', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add reference lines\n",
    "best_latency = df_results['mean_latency_ms'].min()\n",
    "best_perplexity = df_results['perplexity'].min()\n",
    "\n",
    "ax.axvline(x=best_latency, color='blue', linestyle=':', linewidth=2, alpha=0.5, label='Best Latency')\n",
    "ax.axhline(y=best_perplexity, color='blue', linestyle=':', linewidth=2, alpha=0.5, label='Best Perplexity')\n",
    "\n",
    "# Shade ideal quadrant\n",
    "ax.fill_between([ax.get_xlim()[0], best_latency], \n",
    "                 ax.get_ylim()[0], best_perplexity, \n",
    "                 color='blue', alpha=0.1, label='Ideal Region')\n",
    "\n",
    "ax.legend(fontsize=11, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 11: Latency vs Perplexity Trade-off (Speed vs Quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:20:04.914151Z",
     "iopub.status.busy": "2025-12-04T04:20:04.913882Z",
     "iopub.status.idle": "2025-12-04T04:20:05.346807Z",
     "shell.execute_reply": "2025-12-04T04:20:05.346300Z",
     "shell.execute_reply.started": "2025-12-04T04:20:04.914121Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if df_results['energy_per_sample_mj'].max() > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    colors = sns.color_palette('tab10', n_colors=len(df_results))\n",
    "    \n",
    "    # Scatter plot\n",
    "    for i, row in df_results.iterrows():\n",
    "        ax.scatter(row['mean_latency_ms'], row['energy_per_sample_mj'], \n",
    "                  s=400, c=[colors[i]], alpha=0.7, edgecolors='black', linewidth=2.5, zorder=3)\n",
    "        \n",
    "        # Add labels\n",
    "        ax.annotate(row['name'], \n",
    "                   (row['mean_latency_ms'], row['energy_per_sample_mj']),\n",
    "                   xytext=(15, 15), textcoords='offset points',\n",
    "                   fontsize=12, fontweight='bold',\n",
    "                   bbox=dict(boxstyle='round,pad=0.5', facecolor=colors[i], alpha=0.3),\n",
    "                   arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0', lw=1.5))\n",
    "    \n",
    "    ax.set_xlabel('Latency (ms/sample) - Lower is Better â†’', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Energy (mJ/sample) - Lower is Better â†’', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Energy-Latency Trade-off\\n(Ideal: Bottom-Left Corner)', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add ideal region shading\n",
    "    best_latency = df_results['mean_latency_ms'].min()\n",
    "    best_energy = df_results['energy_per_sample_mj'].min()\n",
    "    \n",
    "    ax.axvline(x=best_latency, color='green', linestyle=':', linewidth=2, alpha=0.5, label='Best Latency')\n",
    "    ax.axhline(y=best_energy, color='green', linestyle=':', linewidth=2, alpha=0.5, label='Best Energy')\n",
    "    \n",
    "    # Shade ideal quadrant\n",
    "    ax.fill_between([ax.get_xlim()[0], best_latency], \n",
    "                     ax.get_ylim()[0], best_energy, \n",
    "                     color='green', alpha=0.1, label='Ideal Region')\n",
    "    \n",
    "    ax.legend(fontsize=11, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸  Energy data not available - skipping Energy vs Latency plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 10: Energy vs Latency Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:20:05.347616Z",
     "iopub.status.busy": "2025-12-04T04:20:05.347430Z",
     "iopub.status.idle": "2025-12-04T04:20:05.546597Z",
     "shell.execute_reply": "2025-12-04T04:20:05.545930Z",
     "shell.execute_reply.started": "2025-12-04T04:20:05.347601Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = sns.color_palette('Blues_r', n_colors=len(df_results))\n",
    "\n",
    "x_pos = np.arange(len(df_results))\n",
    "bars = ax.bar(x_pos, df_results['model_size_mb'], color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel('Quantization Format', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Model Size (MB)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Model Memory Footprint\\n(Lower is Better)', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(df_results['name'], fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (idx, row) in enumerate(df_results.iterrows()):\n",
    "    reduction = row['size_reduction_vs_fp32']\n",
    "    label = f\"{row['model_size_mb']:.1f} MB\"\n",
    "    if reduction > 1.0:\n",
    "        label += f\"\\n({reduction:.1f}x smaller)\"\n",
    "    ax.text(i, row['model_size_mb'] + max(df_results['model_size_mb'])*0.02, \n",
    "            label, \n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 9: Model Size Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:20:05.547607Z",
     "iopub.status.busy": "2025-12-04T04:20:05.547355Z",
     "iopub.status.idle": "2025-12-04T04:20:05.775534Z",
     "shell.execute_reply": "2025-12-04T04:20:05.774828Z",
     "shell.execute_reply.started": "2025-12-04T04:20:05.547589Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = sns.color_palette('RdYlGn_r', n_colors=len(df_results))\n",
    "\n",
    "x_pos = np.arange(len(df_results))\n",
    "bars = ax.bar(x_pos, df_results['perplexity'], color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel('Quantization Format', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Perplexity', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Model Quality: Perplexity on WikiText-2\\n(Lower is Better)', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(df_results['name'], fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (idx, row) in enumerate(df_results.iterrows()):\n",
    "    ax.text(i, row['perplexity'] + max(df_results['perplexity'])*0.01, \n",
    "            f\"{row['perplexity']:.2f}\", \n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Add horizontal line for best perplexity\n",
    "best_ppl = df_results['perplexity'].min()\n",
    "ax.axhline(y=best_ppl, color='green', linestyle=':', linewidth=2, alpha=0.5, label=f'Best: {best_ppl:.2f}')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 8: Model Perplexity (Quality Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:20:05.776565Z",
     "iopub.status.busy": "2025-12-04T04:20:05.776336Z",
     "iopub.status.idle": "2025-12-04T04:20:05.985084Z",
     "shell.execute_reply": "2025-12-04T04:20:05.984411Z",
     "shell.execute_reply.started": "2025-12-04T04:20:05.776539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = sns.color_palette('coolwarm', n_colors=len(df_results))[::-1]\n",
    "\n",
    "x_pos = np.arange(len(df_results))\n",
    "bars = ax.bar(x_pos, df_results['tokens_per_sec'], color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel('Quantization Format', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Tokens per Second', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Token Processing Rate\\n(Higher is Better)', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(df_results['name'], fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (idx, row) in enumerate(df_results.iterrows()):\n",
    "    ax.text(i, row['tokens_per_sec'] + max(df_results['tokens_per_sec'])*0.02, \n",
    "            f\"{row['tokens_per_sec']:.0f}\\ntokens/s\", \n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 7: Tokens per Second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:20:05.986187Z",
     "iopub.status.busy": "2025-12-04T04:20:05.985875Z",
     "iopub.status.idle": "2025-12-04T04:20:06.191254Z",
     "shell.execute_reply": "2025-12-04T04:20:06.190497Z",
     "shell.execute_reply.started": "2025-12-04T04:20:05.986158Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = sns.color_palette('viridis', n_colors=len(df_results))[::-1]\n",
    "\n",
    "x_pos = np.arange(len(df_results))\n",
    "bars = ax.bar(x_pos, df_results['throughput'], color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel('Quantization Format', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Throughput (samples/second)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Inference Throughput\\n(Higher is Better)', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(df_results['name'], fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (idx, row) in enumerate(df_results.iterrows()):\n",
    "    ax.text(i, row['throughput'] + max(df_results['throughput'])*0.02, \n",
    "            f\"{row['throughput']:.1f}\\nsamples/s\", \n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 6: Throughput (Samples/Second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:20:06.192361Z",
     "iopub.status.busy": "2025-12-04T04:20:06.192045Z",
     "iopub.status.idle": "2025-12-04T04:20:06.400944Z",
     "shell.execute_reply": "2025-12-04T04:20:06.399762Z",
     "shell.execute_reply.started": "2025-12-04T04:20:06.192324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if df_results['mean_power_w'].max() > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    colors = sns.color_palette('plasma', n_colors=len(df_results))\n",
    "    \n",
    "    x_pos = np.arange(len(df_results))\n",
    "    bars = ax.bar(x_pos, df_results['mean_power_w'], color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    ax.set_xlabel('Quantization Format', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Power (Watts)', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Mean GPU Power Draw During Inference\\n(Lower is Better)', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(df_results['name'], fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (idx, row) in enumerate(df_results.iterrows()):\n",
    "        ax.text(i, row['mean_power_w'] + max(df_results['mean_power_w'])*0.02, \n",
    "                f\"{row['mean_power_w']:.1f} W\", \n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸  Power data not available (GPU power monitoring not functional)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 5: Mean Power Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:20:06.402266Z",
     "iopub.status.busy": "2025-12-04T04:20:06.402004Z",
     "iopub.status.idle": "2025-12-04T04:20:06.643931Z",
     "shell.execute_reply": "2025-12-04T04:20:06.643065Z",
     "shell.execute_reply.started": "2025-12-04T04:20:06.402238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if df_results['energy_per_sample_mj'].max() > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    colors_energy = ['#d62728' if x < 1.0 else '#2ca02c' for x in df_results['energy_reduction_vs_fp32']]\n",
    "    \n",
    "    x_pos = np.arange(len(df_results))\n",
    "    bars = ax.bar(x_pos, df_results['energy_reduction_vs_fp32'], color=colors_energy, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    # Add baseline reference\n",
    "    ax.axhline(y=1.0, color='black', linestyle='--', linewidth=2.5, label='FP32 Baseline (1.0x)', zorder=0)\n",
    "    \n",
    "    ax.set_xlabel('Quantization Format', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Energy Efficiency Factor (x)', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Energy Efficiency Relative to FP32\\n(Higher is Better)', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(df_results['name'], fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    ax.legend(fontsize=11, loc='upper left')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (idx, row) in enumerate(df_results.iterrows()):\n",
    "        efficiency = row['energy_reduction_vs_fp32']\n",
    "        label = f\"{efficiency:.2f}x\"\n",
    "        if efficiency >= 1.0:\n",
    "            label += f\"\\n({(efficiency-1)*100:.1f}% less energy)\"\n",
    "        else:\n",
    "            label += f\"\\n({(1-efficiency)*100:.1f}% more energy)\"\n",
    "        \n",
    "        y_offset = 0.05 if efficiency >= 1.0 else -0.05\n",
    "        va = 'bottom' if efficiency >= 1.0 else 'top'\n",
    "        ax.text(i, efficiency + y_offset, label, \n",
    "                ha='center', va=va, fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸  Energy data not available (GPU power monitoring not functional)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 4: Energy Efficiency vs FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:20:06.645296Z",
     "iopub.status.busy": "2025-12-04T04:20:06.644920Z",
     "iopub.status.idle": "2025-12-04T04:20:06.874748Z",
     "shell.execute_reply": "2025-12-04T04:20:06.874076Z",
     "shell.execute_reply.started": "2025-12-04T04:20:06.645269Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if df_results['energy_per_sample_mj'].max() > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    colors = sns.color_palette('YlOrRd', n_colors=len(df_results))[::-1]\n",
    "    \n",
    "    x_pos = np.arange(len(df_results))\n",
    "    bars = ax.bar(x_pos, df_results['energy_per_sample_mj'], color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    ax.set_xlabel('Quantization Format', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Energy (mJ/sample)', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Energy Consumption per Inference Sample\\n(Lower is Better)', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(df_results['name'], fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (idx, row) in enumerate(df_results.iterrows()):\n",
    "        ax.text(i, row['energy_per_sample_mj'] + max(df_results['energy_per_sample_mj'])*0.02, \n",
    "                f\"{row['energy_per_sample_mj']:.1f} mJ\", \n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸  Energy data not available (GPU power monitoring not functional)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3: Energy Consumption per Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:20:06.875682Z",
     "iopub.status.busy": "2025-12-04T04:20:06.875478Z",
     "iopub.status.idle": "2025-12-04T04:20:07.109003Z",
     "shell.execute_reply": "2025-12-04T04:20:07.108365Z",
     "shell.execute_reply.started": "2025-12-04T04:20:06.875668Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors_speedup = ['#d62728' if x < 1.0 else '#2ca02c' for x in df_results['speedup_vs_fp32']]\n",
    "\n",
    "x_pos = np.arange(len(df_results))\n",
    "bars = ax.bar(x_pos, df_results['speedup_vs_fp32'], color=colors_speedup, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add baseline reference\n",
    "ax.axhline(y=1.0, color='black', linestyle='--', linewidth=2.5, label='FP32 Baseline (1.0x)', zorder=0)\n",
    "\n",
    "ax.set_xlabel('Quantization Format', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Speedup Factor (x)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Speedup Relative to FP32\\n(Higher is Better)', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(df_results['name'], fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax.legend(fontsize=11, loc='upper left')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (idx, row) in enumerate(df_results.iterrows()):\n",
    "    speedup = row['speedup_vs_fp32']\n",
    "    label = f\"{speedup:.2f}x\"\n",
    "    if speedup >= 1.0:\n",
    "        label += f\"\\n({(speedup-1)*100:.1f}% faster)\"\n",
    "    else:\n",
    "        label += f\"\\n({(1-speedup)*100:.1f}% slower)\"\n",
    "    \n",
    "    y_offset = 0.05 if speedup >= 1.0 else -0.05\n",
    "    va = 'bottom' if speedup >= 1.0 else 'top'\n",
    "    ax.text(i, speedup + y_offset, label, \n",
    "            ha='center', va=va, fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2: Speedup vs FP32 Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:20:07.109845Z",
     "iopub.status.busy": "2025-12-04T04:20:07.109669Z",
     "iopub.status.idle": "2025-12-04T04:20:07.339567Z",
     "shell.execute_reply": "2025-12-04T04:20:07.338848Z",
     "shell.execute_reply.started": "2025-12-04T04:20:07.109831Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = sns.color_palette('Set2', n_colors=len(df_results))\n",
    "\n",
    "x_pos = np.arange(len(df_results))\n",
    "bars = ax.bar(x_pos, df_results['mean_latency_ms'], color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "ax.errorbar(x_pos, df_results['mean_latency_ms'], \n",
    "            yerr=df_results['std_latency_ms'], \n",
    "            fmt='none', color='black', capsize=8, linewidth=2, capthick=2)\n",
    "\n",
    "ax.set_xlabel('Quantization Format', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Latency (ms/sample)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Inference Latency Comparison\\n(Lower is Better)', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(df_results['name'], fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (idx, row) in enumerate(df_results.iterrows()):\n",
    "    ax.text(i, row['mean_latency_ms'] + row['std_latency_ms'] + 0.5, \n",
    "            f\"{row['mean_latency_ms']:.2f}ms\", \n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1: Latency Comparison with Error Bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended Visualizations\n",
    "\n",
    "The following sections provide comprehensive visualizations of all benchmark metrics, one figure per metric for detailed analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results for Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:20:07.340665Z",
     "iopub.status.busy": "2025-12-04T04:20:07.340436Z",
     "iopub.status.idle": "2025-12-04T04:20:07.377386Z",
     "shell.execute_reply": "2025-12-04T04:20:07.376756Z",
     "shell.execute_reply.started": "2025-12-04T04:20:07.340648Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "output_dir = Path(\"../results\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "output_file = output_dir / \"gpt2_quantization_benchmark_results.csv\"\n",
    "df_results.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ“ Results saved to: {output_file}\")\n",
    "\n",
    "# Also save a summary markdown file\n",
    "summary_file = output_dir / \"gpt2_benchmark_summary.md\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(\"# GPT-2 Quantization Benchmark Summary\\n\\n\")\n",
    "    f.write(f\"**Date:** {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    f.write(f\"**Model:** {MODEL_NAME} (GPT-2 Small - 124M parameters)\\n\\n\")\n",
    "    f.write(f\"**Dataset:** WikiText-2\\n\\n\")\n",
    "    f.write(f\"**Device:** {device}\\n\\n\")\n",
    "    if torch.cuda.is_available():\n",
    "        f.write(f\"**GPU:** {torch.cuda.get_device_name(0)}\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Results\\n\\n\")\n",
    "    f.write(df_results.to_markdown(index=False))\n",
    "    f.write(\"\\n\\n## Key Findings\\n\\n\")\n",
    "    \n",
    "    if len(df_results) > 1:\n",
    "        fastest = df_results.loc[df_results['mean_latency_ms'].idxmin()]\n",
    "        f.write(f\"- **Fastest:** {fastest['name']} ({fastest['speedup_vs_fp32']:.2f}x faster than FP32)\\n\")\n",
    "        \n",
    "        if df_results['energy_per_sample_mj'].max() > 0:\n",
    "            most_efficient = df_results.loc[df_results['energy_per_sample_mj'].idxmin()]\n",
    "            f.write(f\"- **Most Efficient:** {most_efficient['name']} ({most_efficient['energy_reduction_vs_fp32']:.2f}x more efficient)\\n\")\n",
    "        \n",
    "        smallest = df_results.loc[df_results['model_size_mb'].idxmin()]\n",
    "        f.write(f\"- **Smallest:** {smallest['name']} ({smallest['size_reduction_vs_fp32']:.2f}x size reduction)\\n\")\n",
    "        \n",
    "        best_ppl = df_results.loc[df_results['perplexity'].idxmin()]\n",
    "        f.write(f\"- **Best Perplexity:** {best_ppl['name']} ({best_ppl['perplexity']:.2f})\\n\")\n",
    "\n",
    "print(f\"âœ“ Summary saved to: {summary_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BENCHMARK COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

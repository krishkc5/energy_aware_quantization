{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Quantization Benchmark: DistilBERT on SST-2\n",
    "\n",
    "**Native-Only Execution**: This notebook only runs quantization formats that are natively supported by the hardware.\n",
    "\n",
    "**Formats Tested:**\n",
    "- FP32 (Baseline)\n",
    "- FP16 (Half Precision)\n",
    "- BF16 (BFloat16) - Only if natively supported\n",
    "- Mixed Precision (torch.autocast)\n",
    "\n",
    "**Metrics:**\n",
    "- Latency (ms/batch)\n",
    "- Throughput (samples/second)\n",
    "- Accuracy (%)\n",
    "- Energy (mJ/sample)\n",
    "- Power (W)\n",
    "- Model Size (MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import time\n",
    "import threading\n",
    "import subprocess\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Native Format Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which formats are natively supported\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# FP32 and FP16 are always supported on CUDA\n",
    "FP32_SUPPORTED = True\n",
    "FP16_SUPPORTED = torch.cuda.is_available()\n",
    "\n",
    "# BF16 requires Ampere or newer (compute capability >= 8.0)\n",
    "BF16_SUPPORTED = False\n",
    "if torch.cuda.is_available():\n",
    "    BF16_SUPPORTED = torch.cuda.is_bf16_supported()\n",
    "\n",
    "# Mixed precision (autocast) is supported on CUDA\n",
    "MIXED_SUPPORTED = torch.cuda.is_available()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"NATIVE FORMAT SUPPORT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"FP32:            {'âœ“ Supported' if FP32_SUPPORTED else 'âœ— Not supported'}\")\n",
    "print(f\"FP16:            {'âœ“ Supported' if FP16_SUPPORTED else 'âœ— Not supported'}\")\n",
    "print(f\"BF16:            {'âœ“ Supported' if BF16_SUPPORTED else 'âœ— Not supported (requires Ampere+)'}\")\n",
    "print(f\"Mixed Precision: {'âœ“ Supported' if MIXED_SUPPORTED else 'âœ— Not supported'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Build list of formats to test\n",
    "FORMATS_TO_TEST = []\n",
    "if FP32_SUPPORTED:\n",
    "    FORMATS_TO_TEST.append('fp32')\n",
    "if FP16_SUPPORTED:\n",
    "    FORMATS_TO_TEST.append('fp16')\n",
    "if BF16_SUPPORTED:\n",
    "    FORMATS_TO_TEST.append('bf16')\n",
    "if MIXED_SUPPORTED:\n",
    "    FORMATS_TO_TEST.append('mixed')\n",
    "\n",
    "print(f\"\\nFormats to benchmark: {FORMATS_TO_TEST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Search for tokenized dataset\n",
    "possible_paths = [\n",
    "    Path(cwd) / \"..\" / \"datasets\" / \"tokenized_data\",\n",
    "    Path(cwd) / \"datasets\" / \"tokenized_data\",\n",
    "    Path(cwd) / \"energy_aware_quantization\" / \"datasets\" / \"tokenized_data\",\n",
    "]\n",
    "\n",
    "dataset_path = None\n",
    "for path in possible_paths:\n",
    "    if path.exists() and (path / \"input_ids.pt\").exists():\n",
    "        dataset_path = path\n",
    "        break\n",
    "\n",
    "if dataset_path is None:\n",
    "    current = Path(cwd)\n",
    "    for _ in range(5):\n",
    "        test_path = current / \"datasets\" / \"tokenized_data\"\n",
    "        if test_path.exists() and (test_path / \"input_ids.pt\").exists():\n",
    "            dataset_path = test_path\n",
    "            break\n",
    "        current = current.parent\n",
    "\n",
    "if dataset_path is None:\n",
    "    raise FileNotFoundError(\"Could not find tokenized dataset. Please run taara-day-1-2-tokenized-dataset.ipynb first.\")\n",
    "\n",
    "print(f\"Loading dataset from: {dataset_path}\")\n",
    "\n",
    "# Load tensors\n",
    "input_ids = torch.load(dataset_path / \"input_ids.pt\", map_location='cpu')\n",
    "attention_mask = torch.load(dataset_path / \"attention_mask.pt\", map_location='cpu')\n",
    "labels = torch.load(dataset_path / \"labels.pt\", map_location='cpu')\n",
    "\n",
    "print(f\"âœ“ Loaded {input_ids.shape[0]} samples\")\n",
    "print(f\"  Input shape: {input_ids.shape}\")\n",
    "print(f\"  Device: {input_ids.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Monitoring Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class PowerLogger:\n    \"\"\"Background thread for GPU power monitoring using nvidia-smi\"\"\"\n    \n    def __init__(self, poll_interval_ms=50, gpu_index=0):\n        self.poll_interval_ms = poll_interval_ms\n        self.gpu_index = gpu_index\n        self.power_samples = []\n        self.running = False\n        self.thread = None\n        \n    def _monitor_power(self):\n        \"\"\"Background monitoring loop\"\"\"\n        while self.running:\n            try:\n                result = subprocess.run(\n                    ['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits'],\n                    capture_output=True,\n                    text=True,\n                    timeout=1.0\n                )\n                if result.returncode == 0:\n                    # Handle multiple GPU power values (one per line)\n                    power_lines = result.stdout.strip().split('\\n')\n                    \n                    # Use the specified GPU index (default: 0)\n                    if len(power_lines) > self.gpu_index:\n                        power_str = power_lines[self.gpu_index].strip()\n                        power_w = float(power_str)\n                        self.power_samples.append(power_w)\n                    else:\n                        # If only one GPU, use first value\n                        power_w = float(power_lines[0].strip())\n                        self.power_samples.append(power_w)\n            except (ValueError, IndexError) as e:\n                # Skip malformed readings\n                pass\n            except Exception as e:\n                print(f\"Warning: Failed to read power: {e}\")\n            \n            time.sleep(self.poll_interval_ms / 1000.0)\n    \n    def start(self):\n        \"\"\"Start power monitoring in background thread\"\"\"\n        self.power_samples = []\n        self.running = True\n        self.thread = threading.Thread(target=self._monitor_power, daemon=True)\n        self.thread.start()\n    \n    def stop(self):\n        \"\"\"Stop power monitoring and return statistics\"\"\"\n        self.running = False\n        if self.thread:\n            self.thread.join(timeout=2.0)\n        \n        if len(self.power_samples) == 0:\n            return {'mean_power_w': 0, 'std_power_w': 0, 'num_samples': 0}\n        \n        return {\n            'mean_power_w': np.mean(self.power_samples),\n            'std_power_w': np.std(self.power_samples),\n            'num_samples': len(self.power_samples)\n        }\n\n# Test power logger\nif torch.cuda.is_available():\n    print(\"Testing power logger...\")\n    \n    # First, check how many GPUs nvidia-smi reports\n    try:\n        result = subprocess.run(\n            ['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits'],\n            capture_output=True,\n            text=True,\n            timeout=2.0\n        )\n        power_lines = result.stdout.strip().split('\\n')\n        num_power_readings = len(power_lines)\n        print(f\"  Detected {num_power_readings} power reading(s) from nvidia-smi\")\n        \n        if num_power_readings > 1:\n            print(f\"  Note: Multiple power values detected. Using GPU 0 for monitoring.\")\n            print(f\"  Current readings: {', '.join([f'{p.strip()}W' for p in power_lines])}\")\n    except Exception as e:\n        print(f\"  Warning: Could not detect GPU configuration: {e}\")\n    \n    # Test the logger\n    logger = PowerLogger(poll_interval_ms=50, gpu_index=0)\n    logger.start()\n    time.sleep(1.0)\n    stats = logger.stop()\n    print(f\"âœ“ Power logger working: {stats['mean_power_w']:.2f}W (n={stats['num_samples']} samples)\")\nelse:\n    print(\"âš ï¸  GPU not available, power monitoring disabled\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "def load_model(precision: str, device: str = \"cuda\"):\n",
    "    \"\"\"\n",
    "    Load model with specified precision.\n",
    "    \n",
    "    Args:\n",
    "        precision: 'fp32', 'fp16', 'bf16', or 'mixed'\n",
    "        device: 'cuda' or 'cpu'\n",
    "    \n",
    "    Returns:\n",
    "        model, model_size_mb\n",
    "    \"\"\"\n",
    "    print(f\"Loading {precision.upper()} model...\")\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=2\n",
    "    )\n",
    "    \n",
    "    # Apply precision\n",
    "    if precision == 'fp16':\n",
    "        model = model.half()\n",
    "    elif precision == 'bf16':\n",
    "        model = model.to(dtype=torch.bfloat16)\n",
    "    elif precision == 'mixed':\n",
    "        # Mixed precision uses FP32 model + autocast during inference\n",
    "        pass\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Calculate model size\n",
    "    model_size_mb = sum(p.element_size() * p.nelement() for p in model.parameters()) / (1024 ** 2)\n",
    "    \n",
    "    print(f\"  âœ“ Size: {model_size_mb:.2f} MB\")\n",
    "    print(f\"  âœ“ Dtype: {next(model.parameters()).dtype}\")\n",
    "    \n",
    "    return model, model_size_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Function (Lab 3 Methodology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(\n",
    "    model,\n",
    "    input_ids,\n",
    "    attention_mask,\n",
    "    labels,\n",
    "    name: str,\n",
    "    device: str,\n",
    "    num_batches: int = 100,\n",
    "    warmup_batches: int = 10,\n",
    "    use_autocast: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Benchmark model following Lab 3 methodology.\n",
    "    \n",
    "    4-phase process:\n",
    "    1. Warmup\n",
    "    2. Start power logger\n",
    "    3. Run benchmark (measure latency + accuracy)\n",
    "    4. Compute metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Benchmarking: {name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Move data to device\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    batch_size = input_ids.shape[0]\n",
    "    \n",
    "    # [PHASE 1/4] Warmup\n",
    "    print(f\"[1/4] Warmup ({warmup_batches} batches)...\")\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup_batches):\n",
    "            if use_autocast:\n",
    "                with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            else:\n",
    "                _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # [PHASE 2/4] Start power logger\n",
    "    print(f\"[2/4] Starting power logger...\")\n",
    "    power_logger = None\n",
    "    if device == \"cuda\":\n",
    "        power_logger = PowerLogger(poll_interval_ms=50)\n",
    "        power_logger.start()\n",
    "        time.sleep(0.5)  # Let logger stabilize\n",
    "    \n",
    "    # [PHASE 3/4] Run benchmark\n",
    "    print(f\"[3/4] Running benchmark ({num_batches} batches)...\")\n",
    "    latencies = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    benchmark_start = time.perf_counter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_batches):\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            batch_start = time.perf_counter()\n",
    "            \n",
    "            if use_autocast:\n",
    "                with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            else:\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            batch_end = time.perf_counter()\n",
    "            latencies.append(batch_end - batch_start)\n",
    "            \n",
    "            # Calculate accuracy for this batch\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            if (i + 1) % 25 == 0:\n",
    "                print(f\"    Progress: {i+1}/{num_batches}\", end='\\r')\n",
    "    \n",
    "    benchmark_end = time.perf_counter()\n",
    "    print()  # New line after progress\n",
    "    \n",
    "    # Stop power logger\n",
    "    power_stats = None\n",
    "    if power_logger:\n",
    "        time.sleep(0.5)  # Let final samples collect\n",
    "        power_stats = power_logger.stop()\n",
    "    \n",
    "    # [PHASE 4/4] Compute metrics\n",
    "    print(f\"[4/4] Computing metrics...\")\n",
    "    \n",
    "    total_time_s = benchmark_end - benchmark_start\n",
    "    mean_latency_ms = np.mean(latencies) * 1000\n",
    "    std_latency_ms = np.std(latencies) * 1000\n",
    "    accuracy = 100.0 * correct / total\n",
    "    throughput = total / total_time_s  # samples per second\n",
    "    \n",
    "    # Energy calculations\n",
    "    mean_power_w = 0\n",
    "    energy_per_sample_mj = 0\n",
    "    if power_stats and power_stats['num_samples'] > 0:\n",
    "        mean_power_w = power_stats['mean_power_w']\n",
    "        total_energy_j = mean_power_w * total_time_s\n",
    "        energy_per_sample_mj = (total_energy_j / total) * 1000  # Convert to mJ\n",
    "    \n",
    "    results = {\n",
    "        'name': name,\n",
    "        'mean_latency_ms': mean_latency_ms,\n",
    "        'std_latency_ms': std_latency_ms,\n",
    "        'accuracy': accuracy,\n",
    "        'throughput': throughput,\n",
    "        'mean_power_w': mean_power_w,\n",
    "        'energy_per_sample_mj': energy_per_sample_mj,\n",
    "        'total_time_s': total_time_s,\n",
    "        'total_samples': total\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Results:\")\n",
    "    print(f\"  Latency:    {mean_latency_ms:.3f} Â± {std_latency_ms:.3f} ms/batch\")\n",
    "    print(f\"  Throughput: {throughput:.1f} samples/sec\")\n",
    "    print(f\"  Accuracy:   {accuracy:.2f}%\")\n",
    "    if power_stats:\n",
    "        print(f\"  Power:      {mean_power_w:.2f} W\")\n",
    "        print(f\"  Energy:     {energy_per_sample_mj:.2f} mJ/sample\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run All Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "model_sizes = {}\n",
    "\n",
    "for precision in FORMATS_TO_TEST:\n",
    "    try:\n",
    "        # Load model\n",
    "        model, model_size_mb = load_model(precision, device=device)\n",
    "        model_sizes[precision] = model_size_mb\n",
    "        \n",
    "        # Benchmark\n",
    "        use_autocast = (precision == 'mixed')\n",
    "        results = benchmark_model(\n",
    "            model=model,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            name=precision.upper(),\n",
    "            device=device,\n",
    "            num_batches=100,\n",
    "            warmup_batches=10,\n",
    "            use_autocast=use_autocast\n",
    "        )\n",
    "        \n",
    "        results['model_size_mb'] = model_size_mb\n",
    "        all_results.append(results)\n",
    "        \n",
    "        # Clean up\n",
    "        del model\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Small delay between benchmarks\n",
    "        time.sleep(2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error benchmarking {precision}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ALL BENCHMARKS COMPLETE\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe\n",
    "df_results = pd.DataFrame(all_results)\n",
    "\n",
    "# Calculate speedup vs FP32\n",
    "if 'fp32' in [r['name'].lower() for r in all_results]:\n",
    "    fp32_latency = df_results[df_results['name'].str.upper() == 'FP32']['mean_latency_ms'].values[0]\n",
    "    df_results['speedup_vs_fp32'] = fp32_latency / df_results['mean_latency_ms']\n",
    "    \n",
    "    fp32_energy = df_results[df_results['name'].str.upper() == 'FP32']['energy_per_sample_mj'].values[0]\n",
    "    if fp32_energy > 0:\n",
    "        df_results['energy_reduction_vs_fp32'] = fp32_energy / df_results['energy_per_sample_mj']\n",
    "    else:\n",
    "        df_results['energy_reduction_vs_fp32'] = 1.0\n",
    "    \n",
    "    fp32_size = df_results[df_results['name'].str.upper() == 'FP32']['model_size_mb'].values[0]\n",
    "    df_results['size_reduction_vs_fp32'] = fp32_size / df_results['model_size_mb']\n",
    "else:\n",
    "    df_results['speedup_vs_fp32'] = 1.0\n",
    "    df_results['energy_reduction_vs_fp32'] = 1.0\n",
    "    df_results['size_reduction_vs_fp32'] = 1.0\n",
    "\n",
    "# Reorder columns\n",
    "column_order = [\n",
    "    'name',\n",
    "    'mean_latency_ms',\n",
    "    'std_latency_ms',\n",
    "    'speedup_vs_fp32',\n",
    "    'throughput',\n",
    "    'accuracy',\n",
    "    'mean_power_w',\n",
    "    'energy_per_sample_mj',\n",
    "    'energy_reduction_vs_fp32',\n",
    "    'model_size_mb',\n",
    "    'size_reduction_vs_fp32'\n",
    "]\n",
    "df_results = df_results[column_order]\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"QUANTIZATION BENCHMARK RESULTS\")\n",
    "print(\"=\"*100)\n",
    "print(df_results.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STATISTICAL ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n1. Latency Statistics (ms/batch):\")\n",
    "for _, row in df_results.iterrows():\n",
    "    print(f\"  {row['name']:12s}: {row['mean_latency_ms']:7.3f} Â± {row['std_latency_ms']:6.3f}\")\n",
    "\n",
    "print(\"\\n2. Speedup vs FP32:\")\n",
    "for _, row in df_results.iterrows():\n",
    "    speedup = row['speedup_vs_fp32']\n",
    "    print(f\"  {row['name']:12s}: {speedup:.2f}x faster\")\n",
    "\n",
    "print(\"\\n3. Energy Efficiency vs FP32:\")\n",
    "for _, row in df_results.iterrows():\n",
    "    if row['energy_per_sample_mj'] > 0:\n",
    "        reduction = row['energy_reduction_vs_fp32']\n",
    "        print(f\"  {row['name']:12s}: {reduction:.2f}x more efficient ({row['energy_per_sample_mj']:.2f} mJ/sample)\")\n",
    "\n",
    "print(\"\\n4. Model Size Reduction:\")\n",
    "for _, row in df_results.iterrows():\n",
    "    reduction = row['size_reduction_vs_fp32']\n",
    "    print(f\"  {row['name']:12s}: {reduction:.2f}x smaller ({row['model_size_mb']:.2f} MB)\")\n",
    "\n",
    "print(\"\\n5. Accuracy Comparison:\")\n",
    "for _, row in df_results.iterrows():\n",
    "    print(f\"  {row['name']:12s}: {row['accuracy']:.2f}%\")\n",
    "\n",
    "# Best performer in each category\n",
    "print(\"\\n6. Best Performers:\")\n",
    "if len(df_results) > 1:\n",
    "    fastest = df_results.loc[df_results['mean_latency_ms'].idxmin()]\n",
    "    print(f\"  Fastest:          {fastest['name']} ({fastest['mean_latency_ms']:.3f} ms)\")\n",
    "    \n",
    "    most_efficient = df_results.loc[df_results['energy_per_sample_mj'].idxmin()]\n",
    "    if most_efficient['energy_per_sample_mj'] > 0:\n",
    "        print(f\"  Most Efficient:   {most_efficient['name']} ({most_efficient['energy_per_sample_mj']:.2f} mJ/sample)\")\n",
    "    \n",
    "    smallest = df_results.loc[df_results['model_size_mb'].idxmin()]\n",
    "    print(f\"  Smallest:         {smallest['name']} ({smallest['model_size_mb']:.2f} MB)\")\n",
    "    \n",
    "    most_accurate = df_results.loc[df_results['accuracy'].idxmax()]\n",
    "    print(f\"  Most Accurate:    {most_accurate['name']} ({most_accurate['accuracy']:.2f}%)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Quantization Benchmark Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Color palette\n",
    "colors = sns.color_palette('husl', n_colors=len(df_results))\n",
    "\n",
    "# 1. Latency comparison\n",
    "ax = axes[0, 0]\n",
    "bars = ax.bar(df_results['name'], df_results['mean_latency_ms'], color=colors, alpha=0.8)\n",
    "ax.errorbar(df_results['name'], df_results['mean_latency_ms'], \n",
    "            yerr=df_results['std_latency_ms'], fmt='none', color='black', capsize=5)\n",
    "ax.set_title('Latency (Lower is Better)', fontweight='bold')\n",
    "ax.set_ylabel('Latency (ms/batch)')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Speedup vs FP32\n",
    "ax = axes[0, 1]\n",
    "bars = ax.bar(df_results['name'], df_results['speedup_vs_fp32'], color=colors, alpha=0.8)\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', linewidth=2, label='FP32 Baseline')\n",
    "ax.set_title('Speedup vs FP32 (Higher is Better)', fontweight='bold')\n",
    "ax.set_ylabel('Speedup (x)')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Energy per sample\n",
    "ax = axes[0, 2]\n",
    "if df_results['energy_per_sample_mj'].max() > 0:\n",
    "    bars = ax.bar(df_results['name'], df_results['energy_per_sample_mj'], color=colors, alpha=0.8)\n",
    "    ax.set_title('Energy per Sample (Lower is Better)', fontweight='bold')\n",
    "    ax.set_ylabel('Energy (mJ/sample)')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'Energy data not available', \n",
    "            ha='center', va='center', transform=ax.transAxes)\n",
    "    ax.set_title('Energy per Sample', fontweight='bold')\n",
    "\n",
    "# 4. Throughput\n",
    "ax = axes[1, 0]\n",
    "bars = ax.bar(df_results['name'], df_results['throughput'], color=colors, alpha=0.8)\n",
    "ax.set_title('Throughput (Higher is Better)', fontweight='bold')\n",
    "ax.set_ylabel('Throughput (samples/sec)')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 5. Accuracy\n",
    "ax = axes[1, 1]\n",
    "bars = ax.bar(df_results['name'], df_results['accuracy'], color=colors, alpha=0.8)\n",
    "ax.set_title('Accuracy (Higher is Better)', fontweight='bold')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_ylim([min(df_results['accuracy']) - 1, 100])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 6. Model size\n",
    "ax = axes[1, 2]\n",
    "bars = ax.bar(df_results['name'], df_results['model_size_mb'], color=colors, alpha=0.8)\n",
    "ax.set_title('Model Size (Lower is Better)', fontweight='bold')\n",
    "ax.set_ylabel('Size (MB)')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pareto Frontier: Speed vs Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareto frontier plot: Latency vs Accuracy\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot each format\n",
    "for i, row in df_results.iterrows():\n",
    "    ax.scatter(row['mean_latency_ms'], row['accuracy'], \n",
    "              s=200, c=[colors[i]], alpha=0.7, edgecolors='black', linewidth=2)\n",
    "    ax.annotate(row['name'], \n",
    "               (row['mean_latency_ms'], row['accuracy']),\n",
    "               xytext=(10, 5), textcoords='offset points',\n",
    "               fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Latency (ms/batch) - Lower is Better', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy (%) - Higher is Better', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Speed vs Accuracy Trade-off', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add ideal region (top-left corner)\n",
    "ax.axvline(x=df_results['mean_latency_ms'].min(), color='green', \n",
    "          linestyle='--', alpha=0.3, label='Best Latency')\n",
    "ax.axhline(y=df_results['accuracy'].max(), color='green', \n",
    "          linestyle='--', alpha=0.3, label='Best Accuracy')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Pareto frontier visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results for Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "output_dir = Path(\"../results\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "output_file = output_dir / \"quantization_benchmark_results.csv\"\n",
    "df_results.to_csv(output_file, index=False)\n",
    "print(f\"\\nâœ“ Results saved to: {output_file}\")\n",
    "\n",
    "# Also save a summary markdown file\n",
    "summary_file = output_dir / \"benchmark_summary.md\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(\"# Quantization Benchmark Summary\\n\\n\")\n",
    "    f.write(f\"**Date:** {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    f.write(f\"**Model:** {MODEL_NAME}\\n\\n\")\n",
    "    f.write(f\"**Device:** {device}\\n\\n\")\n",
    "    if torch.cuda.is_available():\n",
    "        f.write(f\"**GPU:** {torch.cuda.get_device_name(0)}\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Results\\n\\n\")\n",
    "    f.write(df_results.to_markdown(index=False))\n",
    "    f.write(\"\\n\\n## Key Findings\\n\\n\")\n",
    "    \n",
    "    if len(df_results) > 1:\n",
    "        fastest = df_results.loc[df_results['mean_latency_ms'].idxmin()]\n",
    "        f.write(f\"- **Fastest:** {fastest['name']} ({fastest['speedup_vs_fp32']:.2f}x faster than FP32)\\n\")\n",
    "        \n",
    "        if df_results['energy_per_sample_mj'].max() > 0:\n",
    "            most_efficient = df_results.loc[df_results['energy_per_sample_mj'].idxmin()]\n",
    "            f.write(f\"- **Most Efficient:** {most_efficient['name']} ({most_efficient['energy_reduction_vs_fp32']:.2f}x more efficient)\\n\")\n",
    "        \n",
    "        smallest = df_results.loc[df_results['model_size_mb'].idxmin()]\n",
    "        f.write(f\"- **Smallest:** {smallest['name']} ({smallest['size_reduction_vs_fp32']:.2f}x size reduction)\\n\")\n",
    "        \n",
    "        most_accurate = df_results.loc[df_results['accuracy'].idxmax()]\n",
    "        f.write(f\"- **Most Accurate:** {most_accurate['name']} ({most_accurate['accuracy']:.2f}%)\\n\")\n",
    "\n",
    "print(f\"âœ“ Summary saved to: {summary_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BENCHMARK COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
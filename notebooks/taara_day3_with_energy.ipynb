{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Complete Accuracy + Energy Evaluation Pipeline - Kaggle Edition\n",
        "## Day 3: Combined Accuracy and Energy Measurement System\n",
        "\n",
        "**Purpose:** Evaluate model accuracy AND energy consumption for FP32, FP16, INT8\n",
        "\n",
        "**Works NOW for:** FP32 baseline  \n",
        "**Ready for:** FP16/INT8 when Thomas provides models  \n",
        "\n",
        "---\n",
        "\n",
        "**This notebook measures:**\n",
        "1. ‚úÖ Model accuracy\n",
        "2. ‚úÖ GPU energy consumption\n",
        "3. ‚úÖ Inference latency\n",
        "4. ‚úÖ GPU power draw\n",
        "5. ‚úÖ Throughput\n",
        "6. ‚úÖ Per-class statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets scikit-learn seaborn\n",
        "\n",
        "print(\"Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import json\n",
        "import time\n",
        "import subprocess\n",
        "import threading\n",
        "from datetime import datetime\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"All imports successful\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: PowerLogger Class (Energy Measurement)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PowerLogger:\n",
        "    \"\"\"GPU power monitoring using nvidia-smi.\"\"\"\n",
        "    \n",
        "    def __init__(self, gpu_id: int = 0, poll_interval_ms: int = 100):\n",
        "        self.gpu_id = gpu_id\n",
        "        self.poll_interval_ms = poll_interval_ms\n",
        "        self.proc = None\n",
        "        self.samples = []\n",
        "        self.thread = None\n",
        "        self.stop_flag = False\n",
        "        \n",
        "    def start(self):\n",
        "        \"\"\"Start power monitoring.\"\"\"\n",
        "        print(f\"[PowerLogger] Starting (poll: {self.poll_interval_ms}ms)...\")\n",
        "        \n",
        "        cmd = [\n",
        "            'nvidia-smi',\n",
        "            '--query-gpu=power.draw',\n",
        "            '--format=csv,noheader,nounits',\n",
        "            f'--id={self.gpu_id}',\n",
        "            '-lms', str(self.poll_interval_ms)\n",
        "        ]\n",
        "        \n",
        "        try:\n",
        "            self.proc = subprocess.Popen(\n",
        "                cmd,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                universal_newlines=True,\n",
        "                bufsize=1\n",
        "            )\n",
        "            \n",
        "            self.stop_flag = False\n",
        "            self.thread = threading.Thread(target=self._collect_samples)\n",
        "            self.thread.daemon = True\n",
        "            self.thread.start()\n",
        "            \n",
        "            print(\"[PowerLogger] ‚úì Started\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"[PowerLogger] ‚úó Failed: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def _collect_samples(self):\n",
        "        \"\"\"Collect power samples in background.\"\"\"\n",
        "        while not self.stop_flag and self.proc and self.proc.poll() is None:\n",
        "            line = self.proc.stdout.readline()\n",
        "            if line:\n",
        "                try:\n",
        "                    power = float(line.strip())\n",
        "                    self.samples.append(power)\n",
        "                except ValueError:\n",
        "                    pass\n",
        "    \n",
        "    def stop(self) -> List[float]:\n",
        "        \"\"\"Stop and return samples.\"\"\"\n",
        "        print(f\"[PowerLogger] Stopping...\")\n",
        "        \n",
        "        self.stop_flag = True\n",
        "        \n",
        "        if self.proc:\n",
        "            self.proc.terminate()\n",
        "            try:\n",
        "                self.proc.wait(timeout=2)\n",
        "            except subprocess.TimeoutExpired:\n",
        "                self.proc.kill()\n",
        "        \n",
        "        if self.thread:\n",
        "            self.thread.join(timeout=2)\n",
        "        \n",
        "        print(f\"[PowerLogger] ‚úì Stopped - {len(self.samples)} samples\")\n",
        "        \n",
        "        if len(self.samples) == 0:\n",
        "            print(\"[PowerLogger] ‚ö† WARNING: No samples collected!\")\n",
        "        \n",
        "        return self.samples.copy()\n",
        "\n",
        "\n",
        "print(\"‚úì PowerLogger class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Test PowerLogger (CRITICAL - Run First!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CRITICAL TEST - Run this first!\n",
        "print(\"Testing PowerLogger...\")\n",
        "print(\"This will run for 5 seconds\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "logger = PowerLogger(gpu_id=0, poll_interval_ms=100)\n",
        "logger.start()\n",
        "time.sleep(5)\n",
        "samples = logger.stop()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nTest Results:\")\n",
        "print(f\"  Samples collected: {len(samples)}\")\n",
        "\n",
        "if len(samples) > 0:\n",
        "    print(f\"  Sample values: {samples[:5]}\")\n",
        "    print(f\"  Mean power: {np.mean(samples):.2f} W\")\n",
        "    print(\"\\n‚úì PowerLogger WORKS! You can proceed.\")\n",
        "else:\n",
        "    print(\"\\n‚úó PowerLogger returned 0 samples!\")\n",
        "    print(\"\\nDebugging steps:\")\n",
        "    print(\"1. Test nvidia-smi manually\")\n",
        "    print(\"2. Try different interval formats\")\n",
        "    print(\"\\n‚ö† DO NOT PROCEED until this works!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Combined Results Class (Accuracy + Energy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class CombinedResults:\n",
        "    \"\"\"Container for combined accuracy and energy results.\"\"\"\n",
        "    precision_type: str\n",
        "    \n",
        "    # Accuracy metrics\n",
        "    accuracy: float\n",
        "    num_correct: int\n",
        "    num_total: int\n",
        "    per_class_accuracy: Dict[int, float]\n",
        "    confusion_matrix: np.ndarray\n",
        "    \n",
        "    # Energy metrics\n",
        "    avg_power_w: float\n",
        "    std_power_w: float\n",
        "    energy_total_j: float\n",
        "    energy_per_inference_j: float\n",
        "    energy_per_inference_mj: float\n",
        "    \n",
        "    # Latency metrics\n",
        "    inference_time_s: float\n",
        "    latency_per_sample_ms: float\n",
        "    throughput_samples_s: float\n",
        "    \n",
        "    # Configuration\n",
        "    batch_size: int\n",
        "    num_batches: int\n",
        "    num_power_samples: int\n",
        "    \n",
        "    def to_dict(self):\n",
        "        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n",
        "        result = asdict(self)\n",
        "        result['confusion_matrix'] = result['confusion_matrix'].tolist()\n",
        "        return result\n",
        "    \n",
        "    def summary(self) -> str:\n",
        "        \"\"\"Get a text summary of results.\"\"\"\n",
        "        lines = [\n",
        "            f\"{'='*70}\",\n",
        "            f\"Combined Evaluation: {self.precision_type}\",\n",
        "            f\"{'='*70}\",\n",
        "            f\"\",\n",
        "            f\"üìä ACCURACY METRICS:\",\n",
        "            f\"  Overall Accuracy:    {self.accuracy*100:.2f}% ({self.num_correct}/{self.num_total})\",\n",
        "            f\"  Per-Class Accuracy:\",\n",
        "        ]\n",
        "        for label, acc in self.per_class_accuracy.items():\n",
        "            lines.append(f\"    Class {label}: {acc*100:.2f}%\")\n",
        "        \n",
        "        lines.extend([\n",
        "            f\"\",\n",
        "            f\"‚ö° ENERGY METRICS:\",\n",
        "            f\"  Average Power:       {self.avg_power_w:.2f} W (¬±{self.std_power_w:.2f})\",\n",
        "            f\"  Total Energy:        {self.energy_total_j:.3f} J\",\n",
        "            f\"  Energy/Inference:    {self.energy_per_inference_mj:.3f} mJ\",\n",
        "            f\"  Power Samples:       {self.num_power_samples}\",\n",
        "            f\"\",\n",
        "            f\"‚è±Ô∏è  LATENCY METRICS:\",\n",
        "            f\"  Total Time:          {self.inference_time_s:.3f} s\",\n",
        "            f\"  Latency/Sample:      {self.latency_per_sample_ms:.3f} ms\",\n",
        "            f\"  Throughput:          {self.throughput_samples_s:.2f} samples/s\",\n",
        "            f\"  Batches:             {self.num_batches} √ó {self.batch_size}\",\n",
        "            f\"{'='*70}\"\n",
        "        ])\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "print(\"‚úì CombinedResults class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Combined Evaluator (Accuracy + Energy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CombinedEvaluator:\n",
        "    \"\"\"Combined accuracy and energy evaluation system.\"\"\"\n",
        "    \n",
        "    def __init__(self, device='cuda'):\n",
        "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"CombinedEvaluator initialized on {self.device}\")\n",
        "    \n",
        "    def evaluate(\n",
        "        self,\n",
        "        model: torch.nn.Module,\n",
        "        dataset,\n",
        "        batch_size: int = 8,\n",
        "        precision_type: str = 'FP32',\n",
        "        warmup_batches: int = 2,\n",
        "        measure_energy: bool = True\n",
        "    ) -> CombinedResults:\n",
        "        \"\"\"Evaluate model accuracy AND energy on dataset.\"\"\"\n",
        "        model.eval()\n",
        "        \n",
        "        # Warmup\n",
        "        print(f\"\\nWarming up with {warmup_batches} batches...\")\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(dataset.get_batch(batch_size)):\n",
        "                if i >= warmup_batches:\n",
        "                    break\n",
        "                _ = model(\n",
        "                    input_ids=batch['input_ids'],\n",
        "                    attention_mask=batch['attention_mask']\n",
        "                )\n",
        "        \n",
        "        if self.device.type == 'cuda':\n",
        "            torch.cuda.synchronize()\n",
        "        \n",
        "        print(\"‚úì Warmup complete\")\n",
        "        \n",
        "        # Start power monitoring\n",
        "        power_samples = []\n",
        "        if measure_energy:\n",
        "            logger = PowerLogger(gpu_id=0, poll_interval_ms=100)\n",
        "            logger.start()\n",
        "            time.sleep(0.5)  # Let logger stabilize\n",
        "        \n",
        "        # Actual evaluation\n",
        "        print(f\"\\nRunning evaluation on {len(dataset)} samples...\")\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "        num_batches = 0\n",
        "        \n",
        "        start_time = time.perf_counter()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in dataset.get_batch(batch_size):\n",
        "                outputs = model(\n",
        "                    input_ids=batch['input_ids'],\n",
        "                    attention_mask=batch['attention_mask']\n",
        "                )\n",
        "                \n",
        "                logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
        "                predictions = logits.argmax(dim=-1).cpu().numpy()\n",
        "                labels = batch['labels'].cpu().numpy()\n",
        "                \n",
        "                all_predictions.extend(predictions)\n",
        "                all_labels.extend(labels)\n",
        "                num_batches += 1\n",
        "        \n",
        "        if self.device.type == 'cuda':\n",
        "            torch.cuda.synchronize()\n",
        "        \n",
        "        end_time = time.perf_counter()\n",
        "        inference_time = end_time - start_time\n",
        "        \n",
        "        # Stop power monitoring\n",
        "        if measure_energy:\n",
        "            power_samples = logger.stop()\n",
        "        \n",
        "        # Compute accuracy metrics\n",
        "        all_predictions = np.array(all_predictions)\n",
        "        all_labels = np.array(all_labels)\n",
        "        \n",
        "        num_correct = (all_predictions == all_labels).sum()\n",
        "        num_total = len(all_labels)\n",
        "        accuracy = num_correct / num_total\n",
        "        \n",
        "        per_class_acc = self._compute_per_class_accuracy(all_predictions, all_labels)\n",
        "        conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
        "        \n",
        "        # Compute energy metrics\n",
        "        if len(power_samples) > 0:\n",
        "            avg_power = float(np.mean(power_samples))\n",
        "            std_power = float(np.std(power_samples))\n",
        "            energy_total = avg_power * inference_time\n",
        "            energy_per_inference = energy_total / num_total\n",
        "        else:\n",
        "            avg_power = 0.0\n",
        "            std_power = 0.0\n",
        "            energy_total = 0.0\n",
        "            energy_per_inference = 0.0\n",
        "        \n",
        "        # Compute latency metrics\n",
        "        latency_per_sample = (inference_time / num_total) * 1000  # ms\n",
        "        throughput = num_total / inference_time if inference_time > 0 else 0\n",
        "        \n",
        "        return CombinedResults(\n",
        "            precision_type=precision_type,\n",
        "            accuracy=accuracy,\n",
        "            num_correct=int(num_correct),\n",
        "            num_total=num_total,\n",
        "            per_class_accuracy=per_class_acc,\n",
        "            confusion_matrix=conf_matrix,\n",
        "            avg_power_w=avg_power,\n",
        "            std_power_w=std_power,\n",
        "            energy_total_j=energy_total,\n",
        "            energy_per_inference_j=energy_per_inference,\n",
        "            energy_per_inference_mj=energy_per_inference * 1000,\n",
        "            inference_time_s=inference_time,\n",
        "            latency_per_sample_ms=latency_per_sample,\n",
        "            throughput_samples_s=throughput,\n",
        "            batch_size=batch_size,\n",
        "            num_batches=num_batches,\n",
        "            num_power_samples=len(power_samples)\n",
        "        )\n",
        "    \n",
        "    def _compute_per_class_accuracy(self, predictions: np.ndarray, labels: np.ndarray) -> Dict[int, float]:\n",
        "        \"\"\"Compute accuracy per class.\"\"\"\n",
        "        unique_labels = np.unique(labels)\n",
        "        per_class = {}\n",
        "        \n",
        "        for label in unique_labels:\n",
        "            mask = labels == label\n",
        "            class_correct = (predictions[mask] == labels[mask]).sum()\n",
        "            class_total = mask.sum()\n",
        "            per_class[int(label)] = float(class_correct / class_total) if class_total > 0 else 0.0\n",
        "        \n",
        "        return per_class\n",
        "    \n",
        "    def plot_confusion_matrix(\n",
        "        self,\n",
        "        results: CombinedResults,\n",
        "        save_path: Optional[str] = None,\n",
        "        class_names: Optional[List[str]] = None\n",
        "    ):\n",
        "        \"\"\"Plot confusion matrix.\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "        \n",
        "        cm = results.confusion_matrix\n",
        "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        \n",
        "        sns.heatmap(\n",
        "            cm_normalized,\n",
        "            annot=True,\n",
        "            fmt='.2f',\n",
        "            cmap='Blues',\n",
        "            xticklabels=class_names or range(len(cm)),\n",
        "            yticklabels=class_names or range(len(cm)),\n",
        "            ax=ax\n",
        "        )\n",
        "        \n",
        "        ax.set_title(f'Confusion Matrix: {results.precision_type}', fontsize=14, fontweight='bold')\n",
        "        ax.set_ylabel('True Label', fontsize=12)\n",
        "        ax.set_xlabel('Predicted Label', fontsize=12)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "            print(f\"‚úì Confusion matrix saved to {save_path}\")\n",
        "        \n",
        "        plt.show()\n",
        "    \n",
        "    def save_results(self, results: CombinedResults, output_path: str):\n",
        "        \"\"\"Save results to JSON file.\"\"\"\n",
        "        output_path = Path(output_path)\n",
        "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(results.to_dict(), f, indent=2)\n",
        "        \n",
        "        print(f\"‚úì Results saved to {output_path}\")\n",
        "\n",
        "\n",
        "print(\"‚úì CombinedEvaluator class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Load Pre-tokenized Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, create dataset if it doesn't exist (inline from Day 1)\n",
        "data_path = Path('/kaggle/working/tokenized_data')\n",
        "\n",
        "if not data_path.exists():\n",
        "    print(\"Creating pre-tokenized dataset...\")\n",
        "    data_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "    dataset_raw = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
        "    dataset_raw = dataset_raw.shuffle(seed=42).select(range(50))\n",
        "    \n",
        "    texts = [example['sentence'] for example in dataset_raw]\n",
        "    labels = [example['label'] for example in dataset_raw]\n",
        "    \n",
        "    encodings = tokenizer(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    \n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "    \n",
        "    torch.save(encodings['input_ids'], data_path / 'input_ids.pt')\n",
        "    torch.save(encodings['attention_mask'], data_path / 'attention_mask.pt')\n",
        "    torch.save(labels_tensor, data_path / 'labels.pt')\n",
        "    \n",
        "    metadata = {\n",
        "        'num_samples': 50,\n",
        "        'max_length': 128,\n",
        "        'dataset_name': 'sst2',\n",
        "        'num_labels': 2,\n",
        "        'seed': 42,\n",
        "        'tokenizer': 'distilbert-base-uncased',\n",
        "    }\n",
        "    \n",
        "    with open(data_path / 'metadata.json', 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    \n",
        "    print(\"‚úì Dataset created\")\n",
        "else:\n",
        "    print(\"‚úì Dataset already exists\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PreTokenizedDataset:\n",
        "    \"\"\"Load pre-tokenized dataset with zero I/O overhead.\"\"\"\n",
        "    \n",
        "    def __init__(self, data_dir: str = '/kaggle/working/tokenized_data'):\n",
        "        data_path = Path(data_dir)\n",
        "        \n",
        "        print(f\"Loading dataset from {data_dir}...\")\n",
        "        self.input_ids = torch.load(data_path / 'input_ids.pt')\n",
        "        self.attention_mask = torch.load(data_path / 'attention_mask.pt')\n",
        "        self.labels = torch.load(data_path / 'labels.pt')\n",
        "        \n",
        "        with open(data_path / 'metadata.json', 'r') as f:\n",
        "            self.metadata = json.load(f)\n",
        "        \n",
        "        self.num_samples = len(self.labels)\n",
        "        print(f\"‚úì Loaded {self.num_samples} samples\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_mask[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "    \n",
        "    def get_batch(self, batch_size: int = 8):\n",
        "        \"\"\"Iterate over batches with zero I/O overhead.\"\"\"\n",
        "        for i in range(0, self.num_samples, batch_size):\n",
        "            end_idx = min(i + batch_size, self.num_samples)\n",
        "            yield {\n",
        "                'input_ids': self.input_ids[i:end_idx],\n",
        "                'attention_mask': self.attention_mask[i:end_idx],\n",
        "                'labels': self.labels[i:end_idx]\n",
        "            }\n",
        "    \n",
        "    def to_device(self, device):\n",
        "        \"\"\"Move all tensors to device (GPU) at once.\"\"\"\n",
        "        self.input_ids = self.input_ids.to(device)\n",
        "        self.attention_mask = self.attention_mask.to(device)\n",
        "        self.labels = self.labels.to(device)\n",
        "        print(f\"‚úì Dataset moved to {device}\")\n",
        "        return self\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "dataset = PreTokenizedDataset('/kaggle/working/tokenized_data')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    dataset.to_device(device)\n",
        "    print(\"‚úì Zero I/O setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Evaluate FP32 Baseline (Accuracy + Energy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"LOADING FP32 MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "model_fp32 = DistilBertForSequenceClassification.from_pretrained(\n",
        "    'distilbert-base-uncased-finetuned-sst-2-english',\n",
        "    num_labels=2\n",
        ").to(device)\n",
        "\n",
        "print(f\"‚úì Model loaded on {device}\")\n",
        "param_count = sum(p.numel() for p in model_fp32.parameters())\n",
        "print(f\"Parameters: {param_count:,} ({param_count/1e6:.1f}M)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EVALUATING FP32 (Accuracy + Energy)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "evaluator = CombinedEvaluator(device=device)\n",
        "\n",
        "results_fp32 = evaluator.evaluate(\n",
        "    model=model_fp32,\n",
        "    dataset=dataset,\n",
        "    batch_size=8,\n",
        "    precision_type='FP32',\n",
        "    warmup_batches=2,\n",
        "    measure_energy=True\n",
        ")\n",
        "\n",
        "print(\"\\n\" + results_fp32.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results\n",
        "evaluator.save_results(results_fp32, '/kaggle/working/results/fp32_combined.json')\n",
        "\n",
        "# Plot confusion matrix\n",
        "evaluator.plot_confusion_matrix(\n",
        "    results=results_fp32,\n",
        "    save_path='/kaggle/working/results/confusion_matrix_fp32.png',\n",
        "    class_names=['Negative', 'Positive']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: Save to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive CSV with all metrics\n",
        "results_data = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'precision': results_fp32.precision_type,\n",
        "    'accuracy_%': round(results_fp32.accuracy * 100, 2),\n",
        "    'num_correct': results_fp32.num_correct,\n",
        "    'num_total': results_fp32.num_total,\n",
        "    'avg_power_w': round(results_fp32.avg_power_w, 2),\n",
        "    'std_power_w': round(results_fp32.std_power_w, 2),\n",
        "    'energy_total_j': round(results_fp32.energy_total_j, 4),\n",
        "    'energy_per_inference_mj': round(results_fp32.energy_per_inference_mj, 4),\n",
        "    'latency_total_s': round(results_fp32.inference_time_s, 4),\n",
        "    'latency_per_sample_ms': round(results_fp32.latency_per_sample_ms, 3),\n",
        "    'throughput_samples_s': round(results_fp32.throughput_samples_s, 2),\n",
        "    'batch_size': results_fp32.batch_size,\n",
        "    'num_batches': results_fp32.num_batches,\n",
        "    'num_power_samples': results_fp32.num_power_samples\n",
        "}\n",
        "\n",
        "df_results = pd.DataFrame([results_data])\n",
        "\n",
        "output_path = Path('/kaggle/working/results')\n",
        "output_path.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "df_results.to_csv(output_path / 'combined_results.csv', index=False)\n",
        "print(f\"‚úì Results saved to {output_path / 'combined_results.csv'}\")\n",
        "\n",
        "print(\"\\nResults Table:\")\n",
        "display(df_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 9: Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DAY 3 COMPLETE: COMBINED ACCURACY + ENERGY EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n‚úì What You Measured:\")\n",
        "print(f\"  üìä Accuracy:           {results_fp32.accuracy*100:.2f}%\")\n",
        "print(f\"  ‚ö° Average Power:       {results_fp32.avg_power_w:.2f} W\")\n",
        "print(f\"  ‚ö° Energy/Inference:    {results_fp32.energy_per_inference_mj:.3f} mJ\")\n",
        "print(f\"  ‚è±Ô∏è  Latency/Sample:      {results_fp32.latency_per_sample_ms:.3f} ms\")\n",
        "print(f\"  üöÄ Throughput:          {results_fp32.throughput_samples_s:.2f} samples/s\")\n",
        "\n",
        "print(\"\\n‚úì Files Generated:\")\n",
        "for file in sorted(output_path.glob('*')):\n",
        "    size = file.stat().st_size / 1024\n",
        "    print(f\"  - {file.name:35s} {size:8.2f} KB\")\n",
        "\n",
        "print(\"\\n‚úì Ready For:\")\n",
        "print(\"  - FP16 evaluation (when Thomas provides model)\")\n",
        "print(\"  - INT8 evaluation (when Thomas provides model)\")\n",
        "print(\"  - Per-layer energy profiling\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"All measurements complete! üéâ\")\n",
        "print(\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per-Layer Energy Profiling for GPT-2\n",
    "## ESE 5390 Final Project: Layer-wise Energy Analysis\n",
    "\n",
    "This notebook analyzes the energy consumption of each layer in GPT-2 Small (FP32 baseline).\n",
    "\n",
    "**Goal**: Identify which layers consume the most energy to guide selective quantization strategies.\n",
    "\n",
    "**Approach**:\n",
    "1. Use PyTorch hooks to measure execution time per layer\n",
    "2. Monitor GPU power during inference\n",
    "3. Compute energy consumption per layer\n",
    "4. Visualize energy hotspots in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "# Force GPU usage\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cpu\":\n",
    "    raise RuntimeError(\"GPU not available! This notebook requires CUDA.\")\n",
    "print(f\"\\n‚úì Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load GPT-2 Tokenized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(f\"Current working directory: {cwd}\")\n",
    "\n",
    "# Search for tokenized dataset\n",
    "possible_paths = [\n",
    "    Path(cwd) / \"..\" / \"datasets\" / \"gpt2_tokenized_data\",\n",
    "    Path(cwd) / \"datasets\" / \"gpt2_tokenized_data\",\n",
    "    Path(cwd) / \"energy_aware_quantization\" / \"datasets\" / \"gpt2_tokenized_data\",\n",
    "    Path(\"/kaggle/working/gpt2_tokenized_data\"),\n",
    "]\n",
    "\n",
    "dataset_path = None\n",
    "for path in possible_paths:\n",
    "    if path.exists() and (path / \"input_ids.pt\").exists():\n",
    "        dataset_path = path\n",
    "        break\n",
    "\n",
    "if dataset_path is None:\n",
    "    current = Path(cwd)\n",
    "    for _ in range(5):\n",
    "        test_path = current / \"datasets\" / \"gpt2_tokenized_data\"\n",
    "        if test_path.exists() and (test_path / \"input_ids.pt\").exists():\n",
    "            dataset_path = test_path\n",
    "            break\n",
    "        current = current.parent\n",
    "\n",
    "if dataset_path is None:\n",
    "    raise FileNotFoundError(\"Could not find GPT-2 tokenized dataset. Please run gpt2_tokenized_dataset.ipynb first.\")\n",
    "\n",
    "print(f\"\\n‚úì Found dataset at: {dataset_path}\")\n",
    "\n",
    "# Load dataset - ALL ON GPU\n",
    "input_ids = torch.load(dataset_path / \"input_ids.pt\", map_location=device)\n",
    "attention_mask = torch.load(dataset_path / \"attention_mask.pt\", map_location=device)\n",
    "labels = torch.load(dataset_path / \"labels.pt\", map_location=device)\n",
    "\n",
    "print(f\"\\n‚úì Loaded dataset:\")\n",
    "print(f\"  - Samples: {input_ids.shape[0]}\")\n",
    "print(f\"  - Sequence length: {input_ids.shape[1]}\")\n",
    "print(f\"  - Device: {input_ids.device}\")\n",
    "print(f\"  - Memory: {input_ids.element_size() * input_ids.nelement() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load GPT-2 Model (FP32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32  # Explicitly FP32\n",
    ")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\n‚úì Model loaded\")\n",
    "print(f\"  - Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  - Device: {next(model.parameters()).device}\")\n",
    "print(f\"  - Dtype: {next(model.parameters()).dtype}\")\n",
    "\n",
    "# Print model architecture\n",
    "print(f\"\\nüìã Model Architecture:\")\n",
    "for name, module in model.named_children():\n",
    "    print(f\"  - {name}: {module.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Layer Type Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract layer type from name\n",
    "def get_layer_type(name: str) -> str:\n",
    "    \"\"\"Extract layer type from full layer name.\"\"\"\n",
    "    if \"attn.c_attn\" in name:\n",
    "        return \"Attention QKV Projection\"\n",
    "    elif \"attn.c_proj\" in name:\n",
    "        return \"Attention Output\"\n",
    "    elif \"mlp.c_fc\" in name:\n",
    "        return \"MLP Layer 1 (Expansion)\"\n",
    "    elif \"mlp.c_proj\" in name:\n",
    "        return \"MLP Layer 2 (Projection)\"\n",
    "    elif \"ln_\" in name:\n",
    "        return \"LayerNorm\"\n",
    "    elif \"wte\" in name or \"wpe\" in name:\n",
    "        return \"Embeddings\"\n",
    "    elif \"lm_head\" in name:\n",
    "        return \"LM Head\"\n",
    "    elif \"dropout\" in name:\n",
    "        return \"Dropout\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "print(\"‚úì Helper function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Layer Profiler with Timing Hooks\n",
    "\n",
    "We'll use PyTorch forward hooks to measure execution time for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerProfiler:\n",
    "    \"\"\"\n",
    "    Profile execution time of each layer using forward hooks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, device: str = \"cuda\"):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.hooks = []\n",
    "        self.layer_times = defaultdict(list)\n",
    "        self.layer_names = []\n",
    "        \n",
    "        # Register hooks for all named modules\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        \"\"\"Register forward pre/post hooks on all layers.\"\"\"\n",
    "        \n",
    "        def make_pre_hook(name):\n",
    "            def pre_hook(module, input):\n",
    "                if self.device == \"cuda\":\n",
    "                    torch.cuda.synchronize()\n",
    "                self.layer_times[name + \"_start\"].append(time.perf_counter())\n",
    "            return pre_hook\n",
    "        \n",
    "        def make_post_hook(name):\n",
    "            def post_hook(module, input, output):\n",
    "                if self.device == \"cuda\":\n",
    "                    torch.cuda.synchronize()\n",
    "                self.layer_times[name + \"_end\"].append(time.perf_counter())\n",
    "            return post_hook\n",
    "        \n",
    "        # Register hooks on important layers (leaf modules only)\n",
    "        for name, module in self.model.named_modules():\n",
    "            # Skip container modules and focus on actual computation layers\n",
    "            if len(list(module.children())) == 0:  # Leaf modules only\n",
    "                if isinstance(module, (nn.Linear, nn.LayerNorm, nn.Dropout, nn.GELU, nn.Embedding, nn.Conv1d)):\n",
    "                    self.layer_names.append(name)\n",
    "                    hook_pre = module.register_forward_pre_hook(make_pre_hook(name))\n",
    "                    hook_post = module.register_forward_hook(make_post_hook(name))\n",
    "                    self.hooks.append(hook_pre)\n",
    "                    self.hooks.append(hook_post)\n",
    "        \n",
    "        print(f\"‚úì Registered hooks on {len(self.layer_names)} layers\")\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset timing statistics.\"\"\"\n",
    "        self.layer_times.clear()\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Remove all hooks.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks.clear()\n",
    "    \n",
    "    def get_layer_times(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute average execution time per layer.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping layer name to average time in seconds\n",
    "        \"\"\"\n",
    "        layer_avg_times = {}\n",
    "        \n",
    "        for name in self.layer_names:\n",
    "            start_times = self.layer_times.get(name + \"_start\", [])\n",
    "            end_times = self.layer_times.get(name + \"_end\", [])\n",
    "            \n",
    "            if len(start_times) == len(end_times) and len(start_times) > 0:\n",
    "                durations = [end - start for start, end in zip(start_times, end_times)]\n",
    "                layer_avg_times[name] = np.mean(durations)\n",
    "        \n",
    "        return layer_avg_times\n",
    "    \n",
    "    def get_layer_stats(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get detailed statistics for each layer.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with layer statistics\n",
    "        \"\"\"\n",
    "        stats = []\n",
    "        \n",
    "        for name in self.layer_names:\n",
    "            start_times = self.layer_times.get(name + \"_start\", [])\n",
    "            end_times = self.layer_times.get(name + \"_end\", [])\n",
    "            \n",
    "            if len(start_times) == len(end_times) and len(start_times) > 0:\n",
    "                durations = [end - start for start, end in zip(start_times, end_times)]\n",
    "                \n",
    "                stats.append({\n",
    "                    \"layer_name\": name,\n",
    "                    \"mean_time_ms\": np.mean(durations) * 1000,\n",
    "                    \"std_time_ms\": np.std(durations) * 1000,\n",
    "                    \"min_time_ms\": np.min(durations) * 1000,\n",
    "                    \"max_time_ms\": np.max(durations) * 1000,\n",
    "                    \"total_time_s\": np.sum(durations),\n",
    "                    \"num_calls\": len(durations)\n",
    "                })\n",
    "        \n",
    "        df = pd.DataFrame(stats)\n",
    "        \n",
    "        # Add percentage of total time\n",
    "        if len(df) > 0:\n",
    "            total_time = df[\"total_time_s\"].sum()\n",
    "            df[\"percent_total\"] = (df[\"total_time_s\"] / total_time) * 100\n",
    "        \n",
    "        return df.sort_values(\"total_time_s\", ascending=False)\n",
    "\n",
    "\n",
    "# Create profiler\n",
    "profiler = LayerProfiler(model, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Power Monitor\n",
    "\n",
    "Simple power monitoring using nvidia-smi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePowerMonitor:\n",
    "    \"\"\"\n",
    "    Simple power monitor using nvidia-smi.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, interval_ms: int = 50, gpu_id: int = 0):\n",
    "        self.interval_ms = interval_ms\n",
    "        self.gpu_id = gpu_id\n",
    "        self.samples = []\n",
    "        self.is_running = False\n",
    "        self._thread = None\n",
    "        self._lock = threading.Lock()\n",
    "    \n",
    "    def _poll(self):\n",
    "        \"\"\"Poll nvidia-smi for power readings.\"\"\"\n",
    "        interval_sec = self.interval_ms / 1000.0\n",
    "        \n",
    "        while self.is_running:\n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [\"nvidia-smi\", \"--query-gpu=power.draw\", \"--format=csv,noheader,nounits\", f\"--id={self.gpu_id}\"],\n",
    "                    capture_output=True, text=True, timeout=2\n",
    "                )\n",
    "                if result.returncode == 0:\n",
    "                    power = float(result.stdout.strip().split('\\n')[0].strip())\n",
    "                    with self._lock:\n",
    "                        self.samples.append((time.perf_counter(), power))\n",
    "            except:\n",
    "                pass\n",
    "            time.sleep(interval_sec)\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"Start monitoring.\"\"\"\n",
    "        with self._lock:\n",
    "            self.samples = []\n",
    "            self.is_running = True\n",
    "        \n",
    "        self._thread = threading.Thread(target=self._poll, daemon=True)\n",
    "        self._thread.start()\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop monitoring.\"\"\"\n",
    "        self.is_running = False\n",
    "        if self._thread:\n",
    "            self._thread.join(timeout=2)\n",
    "    \n",
    "    def get_samples(self) -> List[Tuple[float, float]]:\n",
    "        \"\"\"Get (timestamp, power) samples.\"\"\"\n",
    "        with self._lock:\n",
    "            return self.samples.copy()\n",
    "    \n",
    "    def get_mean_power(self) -> float:\n",
    "        \"\"\"Get mean power in Watts.\"\"\"\n",
    "        samples = self.get_samples()\n",
    "        if len(samples) == 0:\n",
    "            return 0.0\n",
    "        return np.mean([p for _, p in samples])\n",
    "\n",
    "\n",
    "power_monitor = SimplePowerMonitor(interval_ms=50)\n",
    "print(\"‚úì Power monitor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Profiling Inference\n",
    "\n",
    "Run inference with layer profiling and power monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup\n",
    "print(\"Warming up GPU...\")\n",
    "with torch.no_grad():\n",
    "    for i in range(20):\n",
    "        idx = i % input_ids.shape[0]\n",
    "        _ = model(input_ids=input_ids[idx].unsqueeze(0), attention_mask=attention_mask[idx].unsqueeze(0))\n",
    "torch.cuda.synchronize()\n",
    "print(\"‚úì Warmup complete\\n\")\n",
    "\n",
    "# Reset profiler\n",
    "profiler.reset()\n",
    "\n",
    "# Run profiling with power monitoring\n",
    "num_iters = 100\n",
    "print(f\"Running {num_iters} profiling iterations...\")\n",
    "\n",
    "power_monitor.start()\n",
    "time.sleep(0.5)  # Let power monitor stabilize\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_iters):\n",
    "        idx = i % input_ids.shape[0]\n",
    "        _ = model(\n",
    "            input_ids=input_ids[idx].unsqueeze(0),\n",
    "            attention_mask=attention_mask[idx].unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"  Progress: {i+1}/{num_iters}\", end='\\r')\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "time.sleep(0.5)  # Capture trailing power samples\n",
    "power_monitor.stop()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "mean_power = power_monitor.get_mean_power()\n",
    "\n",
    "print(f\"\\n\\n‚úì Profiling complete\")\n",
    "print(f\"  - Total time: {total_time:.3f}s\")\n",
    "print(f\"  - Mean power: {mean_power:.2f}W\")\n",
    "print(f\"  - Total energy: {mean_power * total_time:.3f}J\")\n",
    "print(f\"  - Power samples: {len(power_monitor.get_samples())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Per-Layer Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get layer statistics\n",
    "layer_stats = profiler.get_layer_stats()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PER-LAYER TIMING STATISTICS (Top 20)\")\n",
    "print(\"=\"*80)\n",
    "print(layer_stats.head(20).to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save to CSV\n",
    "output_dir = Path(\"../results\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "layer_stats.to_csv(output_dir / \"per_layer_timing_gpt2_fp32.csv\", index=False)\n",
    "print(f\"\\n‚úì Saved layer timing to: {output_dir / 'per_layer_timing_gpt2_fp32.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compute Per-Layer Energy Consumption\n",
    "\n",
    "Estimate energy per layer using:\n",
    "- Layer execution time (from profiling)\n",
    "- Mean GPU power (from power monitor)\n",
    "\n",
    "**Energy per layer = (Layer time / Total time) √ó Total energy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-layer energy\n",
    "total_energy = mean_power * total_time  # Joules\n",
    "total_measured_time = layer_stats[\"total_time_s\"].sum()\n",
    "\n",
    "layer_stats[\"energy_j\"] = (layer_stats[\"total_time_s\"] / total_measured_time) * total_energy\n",
    "layer_stats[\"energy_mj\"] = layer_stats[\"energy_j\"] * 1000\n",
    "layer_stats[\"energy_per_call_mj\"] = layer_stats[\"energy_mj\"] / layer_stats[\"num_calls\"]\n",
    "\n",
    "# Sort by energy consumption\n",
    "layer_stats = layer_stats.sort_values(\"energy_j\", ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"PER-LAYER ENERGY CONSUMPTION (Top 20)\")\n",
    "print(\"=\"*100)\n",
    "print(layer_stats[[\"layer_name\", \"mean_time_ms\", \"energy_mj\", \"energy_per_call_mj\", \"percent_total\"]].head(20).to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save to CSV\n",
    "layer_stats.to_csv(output_dir / \"per_layer_energy_gpt2_fp32.csv\", index=False)\n",
    "print(f\"\\n‚úì Saved layer energy to: {output_dir / 'per_layer_energy_gpt2_fp32.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Group Energy by Layer Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_stats[\"layer_type\"] = layer_stats[\"layer_name\"].apply(get_layer_type)\n",
    "\n",
    "# Group by layer type\n",
    "type_energy = layer_stats.groupby(\"layer_type\").agg({\n",
    "    \"energy_j\": \"sum\",\n",
    "    \"energy_mj\": \"sum\",\n",
    "    \"total_time_s\": \"sum\",\n",
    "    \"layer_name\": \"count\"\n",
    "}).rename(columns={\"layer_name\": \"num_layers\"})\n",
    "\n",
    "type_energy[\"percent_energy\"] = (type_energy[\"energy_j\"] / type_energy[\"energy_j\"].sum()) * 100\n",
    "type_energy = type_energy.sort_values(\"energy_j\", ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENERGY CONSUMPTION BY LAYER TYPE\")\n",
    "print(\"=\"*80)\n",
    "print(type_energy.to_string())\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save to CSV\n",
    "type_energy.to_csv(output_dir / \"energy_by_layer_type_gpt2_fp32.csv\")\n",
    "print(f\"\\n‚úì Saved grouped energy to: {output_dir / 'energy_by_layer_type_gpt2_fp32.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle(\"GPT-2 Small Per-Layer Energy Analysis (FP32)\", fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Top 15 layers by energy\n",
    "ax = axes[0, 0]\n",
    "top_layers = layer_stats.head(15)\n",
    "y_pos = range(len(top_layers))\n",
    "ax.barh(y_pos, top_layers[\"energy_mj\"], alpha=0.7)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels([name.split('.')[-1] if len(name.split('.')) > 1 else name for name in top_layers[\"layer_name\"]], fontsize=8)\n",
    "ax.set_xlabel(\"Energy (mJ)\", fontsize=10)\n",
    "ax.set_title(\"Top 15 Layers by Energy Consumption\", fontsize=12, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 2. Energy by layer type\n",
    "ax = axes[0, 1]\n",
    "type_energy_sorted = type_energy.sort_values(\"energy_mj\", ascending=True)\n",
    "y_pos = range(len(type_energy_sorted))\n",
    "colors = plt.cm.Set3(range(len(type_energy_sorted)))\n",
    "ax.barh(y_pos, type_energy_sorted[\"energy_mj\"], alpha=0.7, color=colors)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(type_energy_sorted.index, fontsize=10)\n",
    "ax.set_xlabel(\"Energy (mJ)\", fontsize=10)\n",
    "ax.set_title(\"Energy Consumption by Layer Type\", fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 3. Pie chart of energy distribution\n",
    "ax = axes[1, 0]\n",
    "type_energy_pie = type_energy[type_energy[\"percent_energy\"] > 1.0]  # Show only >1%\n",
    "ax.pie(type_energy_pie[\"percent_energy\"], labels=type_energy_pie.index, autopct='%1.1f%%', startangle=90)\n",
    "ax.set_title(\"Energy Distribution by Layer Type\", fontsize=12, fontweight='bold')\n",
    "\n",
    "# 4. Cumulative energy contribution\n",
    "ax = axes[1, 1]\n",
    "cumulative = layer_stats[\"energy_mj\"].cumsum() / layer_stats[\"energy_mj\"].sum() * 100\n",
    "ax.plot(range(len(cumulative)), cumulative, linewidth=2)\n",
    "ax.axhline(y=80, color='r', linestyle='--', alpha=0.7, label='80% threshold')\n",
    "ax.axhline(y=90, color='orange', linestyle='--', alpha=0.7, label='90% threshold')\n",
    "ax.set_xlabel(\"Number of Layers\", fontsize=10)\n",
    "ax.set_ylabel(\"Cumulative Energy (%)\", fontsize=10)\n",
    "ax.set_title(\"Cumulative Energy Contribution\", fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = output_dir / \"per_layer_energy_analysis_gpt2_fp32.png\"\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n‚úì Saved plots to: {plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find how many layers account for 80% of energy\n",
    "cumulative_energy = layer_stats[\"energy_j\"].cumsum() / layer_stats[\"energy_j\"].sum()\n",
    "layers_for_80_pct = (cumulative_energy <= 0.80).sum()\n",
    "layers_for_90_pct = (cumulative_energy <= 0.90).sum()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Energy Distribution:\")\n",
    "print(f\"  - Total layers profiled: {len(layer_stats)}\")\n",
    "print(f\"  - Layers accounting for 80% of energy: {layers_for_80_pct} ({layers_for_80_pct/len(layer_stats)*100:.1f}%)\")\n",
    "print(f\"  - Layers accounting for 90% of energy: {layers_for_90_pct} ({layers_for_90_pct/len(layer_stats)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüî• Top Energy Consumers:\")\n",
    "top_3 = layer_stats.head(3)\n",
    "for i, row in top_3.iterrows():\n",
    "    print(f\"  {row['layer_name']}:\")\n",
    "    print(f\"    - Energy: {row['energy_mj']:.2f} mJ ({row['percent_total']:.1f}% of total)\")\n",
    "    print(f\"    - Avg time: {row['mean_time_ms']:.3f} ms per call\")\n",
    "\n",
    "print(f\"\\nüéØ Quantization Strategy Recommendations:\")\n",
    "print(f\"  1. Prioritize quantizing: {type_energy.head(3).index.tolist()}\")\n",
    "print(f\"  2. These layer types account for {type_energy.head(3)['percent_energy'].sum():.1f}% of total energy\")\n",
    "print(f\"  3. Consider keeping embeddings and LayerNorm in higher precision\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Clean up\n",
    "profiler.remove_hooks()\n",
    "print(\"\\n‚úì Profiling complete and hooks removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Layer Prediction Impact Analysis\n",
    "\n",
    "Measure how much each layer contributes to model predictions using layer ablation.\n",
    "\n",
    "**Method**: For each layer, we'll:\n",
    "1. Get baseline predictions from the full model\n",
    "2. Ablate the layer (zero out its output) and measure prediction change\n",
    "3. Compute impact metrics (KL divergence, logit change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerAblationAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze prediction impact of each layer by ablating (zeroing out) layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, device: str = \"cuda\"):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.hooks = []\n",
    "        self.layer_names = []\n",
    "        \n",
    "        # Get all leaf modules (actual computation layers)\n",
    "        for name, module in model.named_modules():\n",
    "            if len(list(module.children())) == 0:  # Leaf modules only\n",
    "                if isinstance(module, (nn.Linear, nn.LayerNorm, nn.Dropout, nn.GELU, nn.Embedding, nn.Conv1d)):\n",
    "                    self.layer_names.append(name)\n",
    "        \n",
    "        print(f\"‚úì Found {len(self.layer_names)} layers for ablation analysis\")\n",
    "    \n",
    "    def _make_ablation_hook(self, layer_name: str):\n",
    "        \"\"\"Create a hook that zeros out the layer output.\"\"\"\n",
    "        def ablation_hook(module, input, output):\n",
    "            # Zero out the output\n",
    "            if isinstance(output, torch.Tensor):\n",
    "                return torch.zeros_like(output)\n",
    "            elif isinstance(output, tuple):\n",
    "                return tuple(torch.zeros_like(o) if isinstance(o, torch.Tensor) else o for o in output)\n",
    "            return output\n",
    "        return ablation_hook\n",
    "    \n",
    "    def get_baseline_predictions(self, input_ids, attention_mask, labels=None, num_samples: int = 100):\n",
    "        \"\"\"Get baseline predictions from the full model.\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Use a subset of samples for efficiency\n",
    "            sample_indices = torch.randperm(input_ids.shape[0])[:num_samples]\n",
    "            sample_input_ids = input_ids[sample_indices]\n",
    "            sample_attention_mask = attention_mask[sample_indices]\n",
    "            sample_labels = labels[sample_indices] if labels is not None else None\n",
    "            \n",
    "            outputs = self.model(input_ids=sample_input_ids, attention_mask=sample_attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # For language modeling, compute perplexity\n",
    "            if sample_labels is not None:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                # Shift logits and labels for next token prediction\n",
    "                shift_logits = logits[..., :-1, :].contiguous()\n",
    "                shift_labels = sample_labels[..., 1:].contiguous()\n",
    "                baseline_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)).item()\n",
    "                baseline_perplexity = np.exp(baseline_loss)\n",
    "            else:\n",
    "                baseline_loss = None\n",
    "                baseline_perplexity = None\n",
    "            \n",
    "            return {\n",
    "                'logits': logits.cpu(),\n",
    "                'probs': torch.softmax(logits, dim=-1).cpu(),\n",
    "                'loss': baseline_loss,\n",
    "                'perplexity': baseline_perplexity,\n",
    "                'indices': sample_indices.cpu()\n",
    "            }\n",
    "    \n",
    "    def measure_layer_impact(self, input_ids, attention_mask, layer_name: str, baseline: dict, labels=None):\n",
    "        \"\"\"\n",
    "        Measure prediction impact of ablating a specific layer.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with impact metrics\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Register ablation hook\n",
    "        module = dict(self.model.named_modules())[layer_name]\n",
    "        hook = module.register_forward_hook(self._make_ablation_hook(layer_name))\n",
    "        self.hooks.append(hook)\n",
    "        \n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                sample_input_ids = input_ids[baseline['indices']]\n",
    "                sample_attention_mask = attention_mask[baseline['indices']]\n",
    "                sample_labels = labels[baseline['indices']] if labels is not None else None\n",
    "                \n",
    "                outputs = self.model(input_ids=sample_input_ids, attention_mask=sample_attention_mask)\n",
    "                ablated_logits = outputs.logits\n",
    "                ablated_probs = torch.softmax(ablated_logits, dim=-1)\n",
    "            \n",
    "            # Compute impact metrics\n",
    "            baseline_logits = baseline['logits'].to(self.device)\n",
    "            baseline_probs = baseline['probs'].to(self.device)\n",
    "            \n",
    "            # 1. KL Divergence (higher = more impact)\n",
    "            kl_div = torch.nn.functional.kl_div(\n",
    "                torch.log(ablated_probs + 1e-8),\n",
    "                baseline_probs,\n",
    "                reduction='batchmean'\n",
    "            ).item()\n",
    "            \n",
    "            # 2. L2 distance in logit space\n",
    "            logit_l2 = torch.nn.functional.mse_loss(\n",
    "                ablated_logits,\n",
    "                baseline_logits\n",
    "            ).item()\n",
    "            \n",
    "            # 3. Perplexity change\n",
    "            if sample_labels is not None and baseline['loss'] is not None:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                shift_logits = ablated_logits[..., :-1, :].contiguous()\n",
    "                shift_labels = sample_labels[..., 1:].contiguous()\n",
    "                ablated_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)).item()\n",
    "                ablated_perplexity = np.exp(ablated_loss)\n",
    "                perplexity_change = ablated_perplexity - baseline['perplexity']\n",
    "            else:\n",
    "                perplexity_change = None\n",
    "            \n",
    "            # 4. Max logit change\n",
    "            max_logit_change = torch.abs(ablated_logits - baseline_logits).max().item()\n",
    "            \n",
    "            # 5. Mean absolute logit change\n",
    "            mean_logit_change = torch.abs(ablated_logits - baseline_logits).mean().item()\n",
    "            \n",
    "            return {\n",
    "                'kl_divergence': kl_div,\n",
    "                'logit_l2': logit_l2,\n",
    "                'perplexity_change': perplexity_change,\n",
    "                'max_logit_change': max_logit_change,\n",
    "                'mean_logit_change': mean_logit_change,\n",
    "                'impact_score': kl_div + logit_l2  # Combined impact score\n",
    "            }\n",
    "        finally:\n",
    "            # Remove hook\n",
    "            hook.remove()\n",
    "            if hook in self.hooks:\n",
    "                self.hooks.remove(hook)\n",
    "    \n",
    "    def analyze_all_layers(self, input_ids, attention_mask, labels=None, num_samples: int = 100):\n",
    "        \"\"\"\n",
    "        Analyze impact of ablating each layer.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with impact metrics for each layer\n",
    "        \"\"\"\n",
    "        print(f\"\\nüìä Computing baseline predictions ({num_samples} samples)...\")\n",
    "        baseline = self.get_baseline_predictions(input_ids, attention_mask, labels, num_samples)\n",
    "        if baseline['perplexity'] is not None:\n",
    "            print(f\"‚úì Baseline perplexity: {baseline['perplexity']:.2f}\")\n",
    "        else:\n",
    "            print(f\"‚úì Baseline computed\")\n",
    "        \n",
    "        print(f\"\\nüî¨ Analyzing {len(self.layer_names)} layers...\")\n",
    "        results = []\n",
    "        \n",
    "        for i, layer_name in enumerate(self.layer_names):\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  Progress: {i+1}/{len(self.layer_names)}\", end='\\r')\n",
    "            \n",
    "            impact = self.measure_layer_impact(input_ids, attention_mask, layer_name, baseline, labels)\n",
    "            impact['layer_name'] = layer_name\n",
    "            results.append(impact)\n",
    "        \n",
    "        print(f\"\\n‚úì Analysis complete\")\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        df = df.sort_values('impact_score', ascending=False)\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "# Create analyzer\n",
    "print(\"\\nInitializing Layer Ablation Analyzer...\")\n",
    "analyzer = LayerAblationAnalyzer(model, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Run Prediction Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ablation analysis\n",
    "# Use a subset of samples for efficiency (100 samples)\n",
    "impact_df = analyzer.analyze_all_layers(input_ids, attention_mask, labels=labels, num_samples=100)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"PREDICTION IMPACT ANALYSIS (Top 20 Layers)\")\n",
    "print(\"=\"*100)\n",
    "print(impact_df.head(20).to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save to CSV\n",
    "impact_df.to_csv(output_dir / \"per_layer_prediction_impact_gpt2_fp32.csv\", index=False)\n",
    "print(f\"\\n‚úì Saved prediction impact to: {output_dir / 'per_layer_prediction_impact_gpt2_fp32.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Merge Energy and Prediction Impact Data\n",
    "\n",
    "Combine energy consumption and prediction impact data for ALL LAYERS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add layer type to impact dataframe\n",
    "impact_df[\"layer_type\"] = impact_df[\"layer_name\"].apply(get_layer_type)\n",
    "\n",
    "# Merge energy and impact data\n",
    "merged_df = layer_stats[[\"layer_name\", \"energy_j\", \"energy_mj\", \"mean_time_ms\", \"layer_type\"]].merge(\n",
    "    impact_df[[\"layer_name\", \"kl_divergence\", \"logit_l2\", \"perplexity_change\", \"max_logit_change\", \"mean_logit_change\", \"impact_score\"]],\n",
    "    on=\"layer_name\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*100)\n",
    "print(f\"MERGED DATA: ENERGY + PREDICTION IMPACT\")\n",
    "print(f\"=\"*100)\n",
    "print(f\"\\n‚úì Total layers analyzed: {len(merged_df)}\")\n",
    "print(f\"\\nüìä Layer count by type:\")\n",
    "type_counts = merged_df[\"layer_type\"].value_counts()\n",
    "for layer_type, count in type_counts.items():\n",
    "    print(f\"  - {layer_type}: {count}\")\n",
    "\n",
    "print(f\"\\n\\nFirst 10 rows:\")\n",
    "print(merged_df.head(10).to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save merged data\n",
    "merged_df.to_csv(output_dir / \"energy_and_impact_merged_gpt2_fp32.csv\", index=False)\n",
    "print(f\"\\n‚úì Saved merged data to: {output_dir / 'energy_and_impact_merged_gpt2_fp32.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Correlation Analysis: Energy vs Prediction Impact (ALL LAYERS)\n",
    "\n",
    "Create a comprehensive correlation matrix showing the relationship between energy consumption and prediction impact for **ALL LAYERS** in GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for correlation matrix\n",
    "# Normalize energy and impact scores for better visualization\n",
    "merged_df[\"energy_normalized\"] = (merged_df[\"energy_mj\"] - merged_df[\"energy_mj\"].min()) / (merged_df[\"energy_mj\"].max() - merged_df[\"energy_mj\"].min())\n",
    "merged_df[\"impact_normalized\"] = (merged_df[\"impact_score\"] - merged_df[\"impact_score\"].min()) / (merged_df[\"impact_score\"].max() - merged_df[\"impact_score\"].min())\n",
    "\n",
    "# Select columns for correlation\n",
    "corr_columns = [\n",
    "    \"energy_j\", \"energy_mj\", \"mean_time_ms\",\n",
    "    \"kl_divergence\", \"logit_l2\", \"max_logit_change\", \"mean_logit_change\", \"impact_score\",\n",
    "    \"energy_normalized\", \"impact_normalized\"\n",
    "]\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = merged_df[corr_columns].corr()\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"CORRELATION MATRIX: Energy Consumption vs Prediction Impact (ALL {len(merged_df)} LAYERS)\")\n",
    "print(\"=\"*100)\n",
    "print(corr_matrix.to_string())\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Key correlations\n",
    "print(f\"\\nüìä Key Correlations:\")\n",
    "print(f\"  Energy (mJ) vs Impact Score:      {corr_matrix.loc['energy_mj', 'impact_score']:.4f}\")\n",
    "print(f\"  Energy (mJ) vs KL Divergence:     {corr_matrix.loc['energy_mj', 'kl_divergence']:.4f}\")\n",
    "print(f\"  Energy (mJ) vs Logit L2:          {corr_matrix.loc['energy_mj', 'logit_l2']:.4f}\")\n",
    "print(f\"  Energy (mJ) vs Max Logit Change:  {corr_matrix.loc['energy_mj', 'max_logit_change']:.4f}\")\n",
    "print(f\"  Energy (mJ) vs Mean Logit Change: {corr_matrix.loc['energy_mj', 'mean_logit_change']:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "overall_corr = corr_matrix.loc['energy_mj', 'impact_score']\n",
    "if overall_corr > 0.3:\n",
    "    interpretation = \"‚úì Positive correlation: Energy-hungry layers tend to have higher prediction impact\"\n",
    "elif overall_corr < -0.3:\n",
    "    interpretation = \"‚ö†Ô∏è  Negative correlation: Energy-hungry layers tend to have lower prediction impact\"\n",
    "else:\n",
    "    interpretation = \"‚ûñ Weak correlation: No strong relationship between energy and prediction impact\"\n",
    "\n",
    "print(f\"\\nüí° Interpretation: {interpretation}\")\n",
    "\n",
    "# Save correlation matrix\n",
    "corr_matrix.to_csv(output_dir / \"energy_impact_correlation_matrix_gpt2_fp32.csv\")\n",
    "print(f\"\\n‚úì Saved correlation matrix to: {output_dir / 'energy_impact_correlation_matrix_gpt2_fp32.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Visualize Correlation Matrix Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation heatmap\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 12))\n",
    "\n",
    "# Create heatmap\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)  # Mask upper triangle\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.8},\n",
    "    mask=mask,\n",
    "    ax=ax,\n",
    "    vmin=-1,\n",
    "    vmax=1\n",
    ")\n",
    "\n",
    "ax.set_title(f\"Correlation Matrix: Energy vs Prediction Impact\\n(All {len(merged_df)} GPT-2 Layers)\", \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = output_dir / \"energy_impact_correlation_heatmap_gpt2_fp32.png\"\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úì Saved correlation heatmap to: {plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Scatter Plot: Energy vs Prediction Impact (All Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot: Energy vs Impact\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "\n",
    "# Color by layer type\n",
    "layer_types = merged_df[\"layer_type\"].unique()\n",
    "colors_map = dict(zip(layer_types, plt.cm.Set3(range(len(layer_types)))))\n",
    "\n",
    "for layer_type in layer_types:\n",
    "    mask = merged_df[\"layer_type\"] == layer_type\n",
    "    ax.scatter(\n",
    "        merged_df.loc[mask, \"energy_mj\"],\n",
    "        merged_df.loc[mask, \"impact_score\"],\n",
    "        label=layer_type,\n",
    "        alpha=0.6,\n",
    "        s=100,\n",
    "        color=colors_map[layer_type]\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Energy Consumption (mJ)\", fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel(\"Prediction Impact Score\", fontsize=14, fontweight='bold')\n",
    "ax.set_title(f\"Energy Consumption vs Prediction Impact\\n(All {len(merged_df)} GPT-2 Layers)\", \n",
    "             fontsize=16, fontweight='bold')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Add correlation coefficient\n",
    "corr_coef = merged_df[\"energy_mj\"].corr(merged_df[\"impact_score\"])\n",
    "ax.text(0.05, 0.95, f'Pearson Correlation: {corr_coef:.3f}', \n",
    "        transform=ax.transAxes, fontsize=13, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7),\n",
    "        verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = output_dir / \"energy_vs_impact_scatter_gpt2_fp32.png\"\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úì Saved scatter plot to: {plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Summary: Quantization Strategy Based on Energy-Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"FINAL SUMMARY: QUANTIZATION STRATEGY FOR GPT-2 ({len(merged_df)} LAYERS ANALYZED)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Overall statistics\n",
    "print(f\"\\nüìä Overall Statistics:\")\n",
    "print(f\"  Total layers profiled:        {len(merged_df)}\")\n",
    "print(f\"  Total energy (all layers):    {merged_df['energy_j'].sum():.2f} J\")\n",
    "print(f\"  Mean energy per layer:        {merged_df['energy_mj'].mean():.2f} mJ\")\n",
    "print(f\"  Mean impact score per layer:  {merged_df['impact_score'].mean():.4f}\")\n",
    "\n",
    "# Correlation\n",
    "overall_corr = merged_df[\"energy_mj\"].corr(merged_df[\"impact_score\"])\n",
    "print(f\"\\nüîó Energy-Impact Correlation:\")\n",
    "print(f\"  Pearson correlation coefficient: {overall_corr:.4f}\")\n",
    "\n",
    "if overall_corr > 0.3:\n",
    "    corr_interpretation = \"Strong positive: High-energy layers contribute more to predictions\"\n",
    "elif overall_corr > 0.1:\n",
    "    corr_interpretation = \"Moderate positive: Some correlation between energy and prediction impact\"\n",
    "elif overall_corr > -0.1:\n",
    "    corr_interpretation = \"Weak/No correlation: Energy and prediction impact are independent\"\n",
    "else:\n",
    "    corr_interpretation = \"Negative: High-energy layers contribute less to predictions (ideal for quantization!)\"\n",
    "\n",
    "print(f\"  Interpretation: {corr_interpretation}\")\n",
    "\n",
    "# Find layers with high energy but low impact (best quantization candidates)\n",
    "merged_df[\"energy_rank\"] = merged_df[\"energy_mj\"].rank(ascending=False)\n",
    "merged_df[\"impact_rank\"] = merged_df[\"impact_score\"].rank(ascending=False)\n",
    "merged_df[\"rank_diff\"] = merged_df[\"energy_rank\"] - merged_df[\"impact_rank\"]  # Positive = high energy, low impact\n",
    "\n",
    "print(f\"\\nüí° BEST QUANTIZATION CANDIDATES (High Energy, Low Impact):\")\n",
    "print(f\"   (Layers that consume energy but don't contribute much to predictions)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "high_energy_low_impact = merged_df.nlargest(10, \"rank_diff\")[\n",
    "    [\"layer_name\", \"energy_mj\", \"impact_score\", \"energy_rank\", \"impact_rank\", \"rank_diff\", \"layer_type\"]\n",
    "]\n",
    "\n",
    "for idx, row in high_energy_low_impact.iterrows():\n",
    "    print(f\"\\n  {row['layer_name']}\")\n",
    "    print(f\"    Energy:  {row['energy_mj']:.2f} mJ (rank #{int(row['energy_rank'])})\")\n",
    "    print(f\"    Impact:  {row['impact_score']:.4f} (rank #{int(row['impact_rank'])})\")\n",
    "    print(f\"    Rank difference: {int(row['rank_diff'])} (higher = better candidate)\")\n",
    "    print(f\"    Type: {row['layer_type']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"‚ö†Ô∏è  LAYERS TO KEEP IN HIGH PRECISION (High Impact):\")\n",
    "print(f\"   (Layers critical for model predictions)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "high_impact_layers = merged_df.nlargest(5, \"impact_score\")[\n",
    "    [\"layer_name\", \"energy_mj\", \"impact_score\", \"layer_type\"]\n",
    "]\n",
    "\n",
    "for idx, row in high_impact_layers.iterrows():\n",
    "    print(f\"\\n  {row['layer_name']}\")\n",
    "    print(f\"    Impact:  {row['impact_score']:.4f}\")\n",
    "    print(f\"    Energy:  {row['energy_mj']:.2f} mJ\")\n",
    "    print(f\"    Type: {row['layer_type']}\")\n",
    "\n",
    "# Save rankings\n",
    "merged_df.to_csv(output_dir / \"quantization_candidates_ranked_gpt2_fp32.csv\", index=False)\n",
    "print(f\"\\n\\n‚úì Saved ranked quantization candidates to: {output_dir / 'quantization_candidates_ranked_gpt2_fp32.csv'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"‚úì ANALYSIS COMPLETE\")\n",
    "print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
